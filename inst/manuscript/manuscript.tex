\documentclass[
]{jss}

%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

\usepackage[utf8]{inputenc}

\author{
Nikos I. Bosse\\London School of Hygiene \& Tropical Medicine (LSHTM)
\AND Hugo Gruson\\LSHTM \And Anne Cori\\Imperial College London
\AND Edwin van Leeuwen\\UK Health Security Agency,
LSHTM \And Sebastian Funk\\LSHTM \And Sam Abbott\\LSHTM
}
\title{Evaluating Forecasts with \pkg{scoringutils} in \proglang{R}}

\Plainauthor{Nikos I. Bosse, Hugo Gruson, Anne Cori, Edwin van
Leeuwen, Sebastian Funk, Sam Abbott}
\Plaintitle{Evaluating Forecasts with scoringutils in R}
\Shorttitle{Evaluating Forecasts with \pkg{scoringutils} in
\proglang{R}}


\Abstract{
Evaluating forecasts is essential to understand and improve forecasting
and make forecasts useful to decision makers. A variety of \proglang{R}
packages provide a broad variety of scoring rules, visualisations and
diagnostic tools. One particular challenge, which \pkg{scoringutils}
aims to address, is handling the complexity of evaluating and comparing
forecasts from several forecasters across multiple dimensions such as
time, space, and different types of targets. \pkg{scoringutils} extends
the existing landscape by offering a convenient and flexible
\pkg{data.table}-based framework for evaluating and comparing
probabilistic forecasts (forecasts represented by a full predictive
distribution). Notably, \pkg{scoringutils} is the first package to offer
extensive support for probabilistic forecasts in the form of predictive
quantiles, a format that is currently used by several infectious disease
Forecast Hubs. The package is easily extendable, meaning that users can
supply their own scoring rules or extend existing classes to handle new
types of forecasts. \pkg{scoringutils} provides broad functionality to
check the data and diagnose issues, to visualise forecasts and missing
data, to transform data before scoring, to handle missing forecasts, to
aggregate scores, and to visualise the results of the evaluation. The
paper presents the package and its core functionality and illustrates
common workflows using example data of forecasts for COVID-19 cases and
deaths submitted to the European COVID-19 Forecast Hub.
}

\Keywords{forecasting, forecast evaluation, proper scoring
rules, scoring, \proglang{R}}
\Plainkeywords{forecasting, forecast evaluation, proper scoring
rules, scoring, R}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
    Nikos I. Bosse\\
    London School of Hygiene \& Tropical Medicine (LSHTM)\\
    Centre for Mathematical Modelling of Infectious Diseases\\
London School of Hygiene \& Tropical Medicine\\
Keppel Street\\
London WC1E 7HT\\
  E-mail: \email{nikos.bosse@lshtm.ac.uk}\\
  URL: \url{https://lshtm.ac.uk}\\~\\
      Hugo Gruson\\
    LSHTM\\
    Centre for Mathematical Modelling of Infectious Diseases\\
London School of Hygiene \& Tropical Medicine\\
Keppel Street\\
London WC1E 7HT\\
  E-mail: \email{hugo.gruson@lshtm.ac.uk}\\
  
      Anne Cori\\
    Imperial College London\\
    MRC Centre for Global Infectious Disease Analysis, School of Public
Health\\
Imperial College London\\
Norfolk Place\\
London W2 1PG\\
  E-mail: \email{a.cori@imperial.ac.uk}\\
  
      Edwin van Leeuwen\\
    UK Health Security Agency, LSHTM\\
    Statistics, Modelling and Economics Department\\
UK Health Security Agency\\
London NW9 5EQ\\
  E-mail: \email{Edwin.VanLeeuwen@phe.gov.uk}\\
  
      Sebastian Funk\\
    LSHTM\\
    Centre for Mathematical Modelling of Infectious Diseases\\
London School of Hygiene \& Tropical Medicine\\
Keppel Street\\
London WC1E 7HT\\
  E-mail: \email{sebastian.funk@lshtm.ac.uk}\\
  
      Sam Abbott\\
    LSHTM\\
    Centre for Mathematical Modelling of Infectious Diseases\\
London School of Hygiene \& Tropical Medicine\\
Keppel Street\\
London WC1E 7HT\\
  E-mail: \email{sam.abbott@lshtm.ac.uk}\\
  
  }


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\usepackage{amsmath}

\shortcites{reichCollaborativeMultiyearMultimodel2019, kukkonenReviewOperationalRegionalscale2012, funkShorttermForecastsInform2020, cramerEvaluationIndividualEnsemble2021, bracherShorttermForecastingCOVID192021, sherrattPredictivePerformanceMultimodel2022, bracherNationalSubnationalShortterm2022, cramerCOVID19ForecastHub2020}

\usepackage{amssymb} \usepackage{caption}

\captionsetup[table]{skip=10pt}

\newcommand{\class}[1]{`\code{#1}'} \newcommand{\fct}[1]{\code{#1()}} \usepackage{booktabs} \usepackage{multirow} \usepackage{makecell} \usepackage{graphicx}

\begin{document}



\section{Introduction}\label{introduction}

Good forecasts are of great interest to decision makers in various
fields like finance
\citep{timmermannForecastingMethodsFinance2018, elliottForecastingEconomicsFinance2016},
weather predictions
\citep{gneitingWeatherForecastingEnsemble2005, kukkonenReviewOperationalRegionalscale2012}
or infectious disease modeling
\citep{reichCollaborativeMultiyearMultimodel2019, funkShorttermForecastsInform2020, cramerEvaluationIndividualEnsemble2021, bracherNationalSubnationalShortterm2022, sherrattPredictivePerformanceMultimodel2022}.
For decades, researchers, especially in the field of weather
forecasting, have therefore developed and refined an arsenal of
techniques to evaluate predictions (see for example
\cite{goodRationalDecisions1952},
\cite{epsteinScoringSystemProbability1969, murphyNoteRankedProbability1971a, mathesonScoringRulesContinuous1976},
\cite{gneitingProbabilisticForecastsCalibration2007},
\cite{funkAssessingPerformanceRealtime2019},
\cite{gneitingStrictlyProperScoring2007},
\cite{bracherEvaluatingEpidemicForecasts2021}).

Various \proglang{R} \citep{R} packages cover a wide variety of scoring
rules, plots and metrics that are useful in assessing the quality of a
forecast. Existing packages offer functionality that is well suited to
evaluate a variety of predictive tasks, but also come with important
limitations.

Some packages such as \pkg{tscount} \citep{tscount}, \pkg{topmodels}
\citep{topmodels}, \pkg{GLMMadaptive} \citep{GLMMadaptive}, \pkg{cvGEE}
\citep{cvGEE} or \pkg{fabletools} \citep{fabletools} expect that
forecasts were generated in a certain way and require users to supply an
object of a specific class to compute scores. These packages provide
excellent tools for users operating within the specific package
framework but are by their nature not generally applicable to many use
cases practitioners might encounter.

Packages such as \pkg{scoringRules} \citep{scoringRules}, \pkg{Metrics}
\citep{Metrics}, \pkg{MLmetrics} \citep{MLmetrics}, \pkg{verification}
\citep{verification}, \pkg{SpecsVerification} \citep{SpecsVerification},
\pkg{surveillance} \citep{surveillance}, \pkg{predtools}
\citep{predtools}, or \pkg{probably} \citep{probably} provide an
extensive collection of tools, scoring rules and visualisations for
various use cases. However, most scoring functions operate on vectors
and matrices. This is desirable in many applications but can make it
difficult to simultaneously evaluate multiple forecasts across several
dimensions, such as time, space, and different types of targets.

\pkg{scoring} \citep{scoring} operates on a data.frame and uses a
formula interface, making this task easier. However, \pkg{scoring} only
exports a few scoring rules and does not allow users to supply their
own. \pkg{yardstick} \citep{yardstick}, which builds on the
\pkg{tidymodels} \citep{tidymodels} framework, is the most general and
flexible other forecast evaluation package. It allows users to apply
arbitrary scoring rules to a data.frame of forecasts, independently of
how they were created. However, \pkg{yardstick} is primarily focused on
point forecasts and classification tasks. It currently lacks general
support for probabilistic forecasts (forecasts in the form of a full
predictive distribution, represented e.g., by a set of quantiles or
samples from the forecast distribution). Probabilistic forecasts are
desirable, as they allow decision makers to take into account the
uncertainty of a forecast
\citep{gneitingProbabilisticForecastsCalibration2007}, and are widely
used, e.g., in Meteorology or Epidemiology.

\pkg{scoringutils} aims to fill the existing gap in the ecosystem by
providing a flexible general-purpose tool for the evaluation of
probabilistic forecasts. It offers a coherent \code{data.table}-based
framework and workflow that allows users to evaluate and compare
forecasts across multiple dimensions using a wide variety of default and
user-provided scoring rules. Notably, \pkg{scoringutils} is the first
package to offer extensive support for probabilistic forecasts in the
form of predictive quantiles, a format that is currently used by several
infectious disease Forecast Hubs
\citep{reichCollaborativeMultiyearMultimodel2019, cramerCOVID19ForecastHub2020, sherrattPredictivePerformanceMultimodel2022, bracherNationalSubnationalShortterm2022}.
The package provides broad functionality to check the data and diagnose
issues, to visualise forecasts and missing data, to transform data
before scoring \citep[see][]{bosseScoringEpidemiologicalForecasts2023},
to apply various metrics and scoring rules to data, to handle missing
forecasts, to aggregate scores and to visualise the results of the
evaluation. \pkg{scoringutils} makes extensive use of \pkg{data.table}
\citep{data.table} to ensure fast and memory-efficient computations. The
core functionality is designed around S3 classes, allowing users to
expand on the generics and methods implemented in the package.
\pkg{scoringutils} provides extensive documentation and case studies, as
well as sensible defaults for scoring forecasts.

\begin{CodeChunk}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{output/workflow} 

}

\caption{Illustration of the suggested workflow for evaluating forecasts with \pkg{scoringutils}. A: Workflow for working with forecasts in a \code{data.table}-based format. The left side shows the core workflow of the package: 1) validating and processing inputs, 2) scoring forecasts and 3) summarising scores. The right side shows additional functionality that is available at the different stages of the evaluation process. The part in blue is covered by Section \ref{sec:inputs} and includes all functions related to processing and validating inputs as well as obtaining additional information about the forecasts. The part in green is covered by Section \ref{sec:scoring} and includes all functions related to scoring forecasts and obtaining additional information about the scores. The part in red is covered by Section \ref{sec:summarising} and includes all functions related to summarising scores and additional visualisations based on summarised scores. B: An alternative workflow, allowing users to call scoring rules directly with vectors/matrices as inputs.}\label{fig:workflow-scoringutils}
\end{figure}
\end{CodeChunk}

\subsubsection{Paper outline and package
workflow}\label{paper-outline-and-package-workflow}

The structure of this paper follows the suggested package workflow which
consists of 1) validating and processing inputs, 2) scoring forecasts
and 3) summarising scores. This workflow is illustrated in Figure
\ref{fig:workflow-scoringutils}, which displays the core workflow (left
side) as well as additional functionality that is available at different
stages of the evaluation process (right side).

Section \ref{sec:inputs} is centred around validating inputs,
\code{forecast} objects, and the associated functionality. It explains
the expected input formats and how to validate inputs and diagnose
issues. It provides an overview of the types of forecasts supported by
\pkg{scoringutils} and the different S3 classes used to represent these
forecast types. It also provides information on a variety of functions
that can be used to visualise forecasts, transform inputs or obtain
additional information and visualisations.

Section \ref{sec:scoring} is centred around scoring forecasts and the
additional functionality that is available to manipulate and analyse
scores further. It explains how to score forecasts, either in a
\code{data.table}-format or in a format based on matrices and vectors.
It also provides information on additional information that can be
computed from scores, such as correlations between scores or relative
skill scores based on pairwise comparisons. These can be useful to
mitigate the effects of missing forecasts.

Section \ref{sec:summarising} is centred around summarised scores. It
explains how to summarise scores and gives information on additional
visualisations that can be created based on summarised scores.

Section \ref{sec:discussion} discusses the merits and limitations of the
package in its current version as explores avenues for future work.

All functionality will be illustrated using the example data shipped
with the package, which is based on a subset of case and death forecasts
submitted every week between May and September 2021 to the European
COVID-19 Forecast Hub
\citep{sherrattPredictivePerformanceMultimodel2022}. Following the
convention of the different COVID-19 Forecast Hubs, we will restrict
examples to two-week-ahead forecasts.

The code for this package and paper can be found on
\url{https:github.com/epiforecasts/scoringutils}. The full package
documentation as well as an overview of all existing functions can also
be seen on \url{https://epiforecasts.io/scoringutils}.

\section{Inputs, forecast types and input validation} \label{sec:inputs}

\subsection{Input formats and types of
forecasts}\label{input-formats-and-types-of-forecasts}

Forecasts differ in the exact prediction task and in how the forecaster
chooses to represent their prediction. To distinguish different kinds of
forecasts, we use the term ``forecast type'' (which is more a convenient
classification than a formal definition). Currently,
\texttt{scoringutils} distinguishes four different forecast types:
``binary'', ``point'', ``quantile'' and ``sample'' forecasts.

\begin{itemize}
\tightlist
\item
  ``Binary'' denotes a probability forecast for a binary (yes/no)
  outcome variable. This is sometimes also called ``soft binary
  classification''.
\item
  ``Point'' denotes a forecast for a continuous or discrete outcome
  variable that is represented by a single number.
\item
  ``Quantile'' or ``quantile-based'' is used to denote a probabilistic
  forecast for a continuous or discrete outcome variable, with the
  forecast distribution represented by a set of predictive quantiles.
  While a single quantile would already satisfy the requirements for a
  quantile-based forecast, most scoring rules expect a set of quantiles
  which are symmetric around the median (thus forming the lower and
  upper bounds of central ``prediction intervals'') and will return
  \texttt{NA} if this is not the case.
\item
  ``Sample'' or ``sample-based'' is used to denote a probabilistic
  forecast for a continuous or discrete outcome variable, with the
  forecast represented by a finite set of samples drawn from the
  predictive distribution. A single sample technically suffices, but
  would lead to very imprecise results.
\end{itemize}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{
\setlength\tabcolsep{10pt}
\begin{tabular}{@{}lllll@{}} % Define the table with five columns
\toprule
\textbf{Forecast type} & & & \textbf{column} & \textbf{type} \\
\midrule

% All forecast types
\multirow{3}{*}{\makecell[cl]{All forecast\\types}} & & & \texttt{observed}  & \\
                                                    & & & \texttt{predicted} & \\
                                                    & & & \texttt{model}    & \\
\midrule

% Classification
\multirow{2}{*}{Classification} & \multirow{2}{*}{Binary} & Soft classification & \texttt{observed} & factor with 2 levels \\ 
                                &        & {\footnotesize(prediction is probability)}  & \texttt{predicted} & numeric [0,1]       \\
\midrule

% Point forecasts
\multirow{2}{*}{Point forecast} & & & \texttt{observed}  & numeric                \\
                                 & & & \texttt{predicted} & numeric                \\
\midrule

% Probabilistic forecast
\multirow{6}{*}{\makecell[cl]{Probabilistic\\forecast}} & & \multirow{3}{*}{Sample format} & \texttt{observed} & numeric                \\
                                                         &                      &               & \texttt{predicted} & numeric                \\
                                                         &                      &               & \texttt{sample\_id} & numeric              \\
\cmidrule(l){3-5} 
                                                         &                      & \multirow{3}{*}{Quantile format}  & \texttt{observed} & numeric                \\
                                                         &                      &               & \texttt{predicted} & numeric                \\
                                                         &                      &               & \texttt{quantile\_level} & numeric [0,1]  \\
\bottomrule
\end{tabular}
}
\caption{Formatting requirements for data inputs. Regardless of the forecast type, the \texttt{data.frame} (or similar) must have columns called \texttt{observed}, \texttt{predicted}, and \texttt{model}. For binary forecasts, the column \texttt{observed} must be of type factor with two levels and the column \texttt{predicted} must be a numeric between 0 and 1. For all other forecast types, both \texttt{observed} and \texttt{predicted} must be of type numeric. Forecasts in a sample-based format require an additional numeric column \texttt{sample\_id} and forecasts in a quantile-based format require an additional numeric column \texttt{quantile\_level} with values between 0 and 1.}
\label{tab:input-score}
\end{table}

The starting point for working with \pkg{scoringutils} is usually a
\code{data.frame} (or similar) containing both the predictions and the
observed values. In a next step (see Section \ref{sec:validation}) this
data will be validated and transformed into a ``forecast object'' (a
\code{data.table} with a class \texttt{forecast} and an additional class
corresponding to the forecast type). The input data needs to have a
column \texttt{observed} for the observed values, a column
\texttt{predicted} for the predicted values, and a column \texttt{model}
denoting the name of the model/forecaster that generated the forecast.
Additional requirements depend on the forecast type. Table
\ref{tab:input-score} shows the expected input format for each forecast
type.

The package contains example data for each forecast type, which can
serve as an orientation for the correct formats. The example data sets
are exported as \texttt{example\_quantile},
\texttt{example\_sample\_continuous},
\texttt{example\_sample\_discrete}, \texttt{example\_point} and
\texttt{example\_binary}. For illustrative purposes, the example data
also contains some rows with only observations and no corresponding
predictions. Input formats for the scoring rules that can be called
directly follow the same convention, with inputs expected to be vectors
or matrices.

\subsubsection{The unit of a single
forecast}\label{the-unit-of-a-single-forecast}

Apart from the columns \texttt{observed}, \texttt{predicted},
\texttt{model}, and the extra columns required for each forecast type,
it is usually necessary that the input data contains additional columns.
This is because a single probabilistic forecast (apart from binary
predictions) is composed of multiple values. A quantile-based forecast,
for example, is composed of several quantiles, and a sample-based
forecast of multiple samples. However, every row only holds a single
sample/quantile. Several rows in the input data therefore jointly form a
single forecast. Additional columns in the input provide the information
necessary to group rows that belong to the same forecast. The
combination of values in those columns forms the unit of a single
forecast (or ``forecast unit'') and should uniquely identify a single
forecast. For example, consider forecasts made by different models in
various locations at different time points and for different targets. A
single forecast could then be uniquely described by the values in the
columns \texttt{model}, \texttt{location}, \texttt{date}, and
\texttt{target}, and the forecast unit would be
\texttt{forecast\_unit\ =\ c("model",\ "location",\ "date",\ "target")}.

Rows are automatically grouped based on the values in all other columns
present in the data (excluding required columns like \texttt{sample\_id}
or \texttt{quantile\_level} and values computed by \pkg{scoringutils}).
As the forecast unit is determined based on all existing columns, no
column must be present that is unrelated to the forecast unit. As a very
simplistic example, consider an additional row, \texttt{"even"}, that is
one if the row number is even and zero otherwise. The existence of this
column would change results, as \pkg{scoringutils} assumes it was
relevant to grouping the forecasts.

\subsection{Forecast objects and input validation} \label{sec:validation}

The raw input data needs to be processed and validated using the
function \texttt{as\_forecast()}:

\begin{CodeChunk}
\begin{CodeInput}
R> library("scoringutils")
R> forecast_quantile <- example_quantile[horizon == 2] |>
+   as_forecast() 
\end{CodeInput}
\end{CodeChunk}

The function \code{as\_forecast()} recognises the type of the forecast
based on the available columns, transforms the input into a ``forecast''
object and validates it (see Figure \ref{fig:flowchart-validation} for
details). A forecast object is a \texttt{data.table} that has passed
some input validations. It behaves like a \texttt{data.table}, but has
an additional class \texttt{forecast} as well as a class corresponding
to the forecast type (\texttt{forecast\_point},
\texttt{forecast\_binary}, \texttt{forecast\_quantile} or
\texttt{forecast\_sample}).

\code{as\_forecast()} can automatically determine the forecast type and
forecast unit based on the input data. However, it can also take
additional arguments that help facilitate the process of creating a
forecast object:

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_quantile <- example_quantile[horizon == 2] |>
+   as_forecast(
+     forecast_unit = c(
+       "model", "location", "target_end_date", 
+       "forecast_date", "horizon", "location"
+     ),
+     forecast_type = "quantile",
+     observed = "observed", 
+     predicted = "predicted",
+     model = "model",
+     quantile_level = "quantile_level",
+   ) 
\end{CodeInput}
\end{CodeChunk}

The argument \texttt{forecast\_unit} allows the user to manually set the
unit of a single forecast. This is done by dropping all columns that are
not either specified in the \texttt{forecast\_unit} or are ``protected''
columns (such as \texttt{observed}, \texttt{predicted}, \texttt{model},
\texttt{quantile\_level}, or \texttt{sample\_id}). The argument
\texttt{forecast\_type} allows users to manually specify the forecast
type they expect. If the forecast type inferred from the input does not
match the specified forecast type, an error is thrown. The other
arguments can be used to specify the column names of the input data that
correspond to the required columns. \code{as\_forecast()} will rename
the specified columns to the corresponding required columns.

\subsection{Diagnostic helper
functions}\label{diagnostic-helper-functions}

Various helper functions are available to diagnose and fix issues with
the input data. The most important one is \texttt{print()}. Once a
forecast object has successfully been created, diagnostic information
will automatically be added to the output when printing a forecast
object. This information includes the forecast type, the forecast unit,
and additional information in case the object fails validations.

\begin{CodeChunk}
\begin{CodeInput}
R> print(forecast_quantile, 2)
\end{CodeInput}
\begin{CodeOutput}

Key: <location, target_end_date, target_type>
      location target_end_date target_type observed location_name
        <char>          <Date>      <char>    <num>        <char>
   1:       DE      2021-05-15       Cases    64985       Germany
   2:       DE      2021-05-15       Cases    64985       Germany
  ---                                                            
7014:       IT      2021-07-24      Deaths       78         Italy
7015:       IT      2021-07-24      Deaths       78         Italy
      forecast_date quantile_level predicted                 model
             <Date>          <num>     <int>                <char>
   1:    2021-05-03          0.010     63106 EuroCOVIDhub-ensemble
   2:    2021-05-03          0.025     67867 EuroCOVIDhub-ensemble
  ---                                                             
7014:    2021-07-12          0.975       611  epiforecasts-EpiNow2
7015:    2021-07-12          0.990       719  epiforecasts-EpiNow2
      horizon
        <num>
   1:       2
   2:       2
  ---        
7014:       2
7015:       2
\end{CodeOutput}
\end{CodeChunk}

Internally, the print method calls the functions
\code{get\_forecast\_type()}, \code{get\_forecast\_unit()} and
\code{assert\_forecast()}. \code{get\_forecast\_type()} and
\code{get\_forecast\_unit()} work on either an unvalidated
\code{data.frame} (or similar) or on an already validated forecast
object. They return the forecast type and the forecast unit,
respectively, as inferred from the input data. \code{assert\_forecast()}
asserts that an existing forecast object passes all validations and
returns \texttt{invisble(NULL)} if the forecast object is valid (and
otherwise errors). \code{validate\_forecast()} is similar to
\code{assert\_forecast()}, but returns the forecast object in case of
success instead of \texttt{invisble(NULL)}, meaning that it can be used
in a pipe.

One common issue that causes \code{as\_forecast()} to fail are
``duplicates'' in the data. \pkg{scoringutils} strictly requires that
there be only one forecast per forecast unit and only one predicted
value per quantile level or sample id within a single forecast.
Duplicates usually occur if the forecast unit is misspecified. For
example, if we removed the column \texttt{target\_type} from the example
data, we would now have two forecasts (one for cases and one for deaths
of COVID-19) that appear to have the same forecast unit (since the
information that distinguished between case and death forecasts is no
longer there). The function \code{get\_duplicate\_forecasts()} returns
duplicate rows for the user to inspect. To remedy the issue, the user
needs to add additional columns that uniquely identify a single
forecast.

\begin{CodeChunk}
\begin{CodeInput}
R> rbind(example_quantile, example_quantile[1001:1002]) |>
+   get_duplicate_forecasts() 
\end{CodeInput}
\begin{CodeOutput}
   location target_end_date target_type observed location_name
     <char>          <Date>      <char>    <num>        <char>
1:       DE      2021-05-22      Deaths     1285       Germany
2:       DE      2021-05-22      Deaths     1285       Germany
3:       DE      2021-05-22      Deaths     1285       Germany
4:       DE      2021-05-22      Deaths     1285       Germany
   forecast_date quantile_level predicted                model
          <Date>          <num>     <int>               <char>
1:    2021-05-17          0.975      1642 epiforecasts-EpiNow2
2:    2021-05-17          0.975      1642 epiforecasts-EpiNow2
3:    2021-05-17          0.990      1951 epiforecasts-EpiNow2
4:    2021-05-17          0.990      1951 epiforecasts-EpiNow2
   horizon
     <num>
1:       1
2:       1
3:       1
4:       1
\end{CodeOutput}
\end{CodeChunk}

\subsection{Transforming forecasts}\label{transforming-forecasts}

As suggested in \cite{bosseScoringEpidemiologicalForecasts2023}, users
may want to transform forecasts before scoring them. Two commonly used
scoring rules are the continuous ranked probability score (CRPS) and the
weighted interval score (WIS). Both measure the absolute distance
between the forecast and the observation. This may not be desirable, for
example in the context of epidemiological forecasts, where infectious
disease processes are usually modelled to occur on a multiplicative
scale. Taking the logarithm of the forecasts and observations before
scoring them makes it possible to evaluate forecasters based on how well
they predicted the exponential growth rate.

The function \code{transform\_forecasts()} takes a validated forecast
object as input and allows users to apply arbitrary transformations to
forecasts and observations. Users can specify a function via the
argument \code{fun} (as well as supply additional function parameters).
The default function is \code{log_shift()}, which is simply a wrapper
around \code{log()} with an additional argument that allows adding an
offset (i.e., \code{log(x + offset)}) to deal with zeroes in the data.
Users can specify to either append the transformed forecasts to the
existing data by setting \code{append = TRUE} (the default behaviour,
resulting in an additional column \texttt{scale}) or to replace the
existing forecasts in place.

The example data contains negative values which need to be handled
before applying the logarithm. Presumably, negative values for count
data should be dropped altogether, but for illustrative purposes, we
will call \code{transform\_forecasts()} twice to replace them with
zeroes first before appending transformed counts.

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_quantile |> 
+   transform_forecasts(fun = \(x) {pmax(x, 0)}, append = FALSE) |>
+   transform_forecasts(fun = log_shift, offset = 1) |>
+   print(2)
\end{CodeInput}
\begin{CodeOutput}

       location target_end_date target_type     observed
         <char>          <Date>      <char>        <num>
    1:       DE      2021-05-15       Cases 64985.000000
    2:       DE      2021-05-15       Cases 64985.000000
   ---                                                  
14029:       IT      2021-07-24      Deaths     4.369448
14030:       IT      2021-07-24      Deaths     4.369448
       location_name forecast_date quantile_level    predicted
              <char>        <Date>          <num>        <num>
    1:       Germany    2021-05-03          0.010 63106.000000
    2:       Germany    2021-05-03          0.025 67867.000000
   ---                                                        
14029:         Italy    2021-07-12          0.975     6.416732
14030:         Italy    2021-07-12          0.990     6.579251
                       model horizon   scale
                      <char>   <num>  <char>
    1: EuroCOVIDhub-ensemble       2 natural
    2: EuroCOVIDhub-ensemble       2 natural
   ---                                      
14029:  epiforecasts-EpiNow2       2     log
14030:  epiforecasts-EpiNow2       2     log
\end{CodeOutput}
\end{CodeChunk}

\subsection{Additional functionality related to forecast
objects}\label{additional-functionality-related-to-forecast-objects}

\pkg{scoringutils} offers a variety of different functions that allow
users to obtain and visualise additional information about their
forecast. The package also has an extensive Vignette with examples for
further visualisations that are not implemented as functions.

\subsubsection{Displaying the number of forecasts
available}\label{displaying-the-number-of-forecasts-available}

Users can get an overview of how many forecasts there are using
\code{get\_forecast\_counts()}. The function takes a validated forecast
object as input and returns a data.table of forecast counts, which helps
obtain an overview of missing forecasts. This can impact the evaluation,
if missingness correlates with performance. Users can specify the level
of summary through the \texttt{by} argument. For example, to see how
many forecasts there are per \texttt{model}, \texttt{target\_type} and
\texttt{forecast\_date}, we can run

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_counts <- forecast_quantile |>
+   get_forecast_counts(
+     by = c("model", "target_type", "forecast_date")
+   )
\end{CodeInput}
\end{CodeChunk}

We can visualise the results by calling \code{plot\_forecast\_counts()}
on the output (Figure \ref{fig:plot-forecast-counts}).

\begin{CodeChunk}
\begin{CodeInput}
R> library("ggplot2")
R> forecast_counts |>
+   plot_forecast_counts(x = "forecast_date") + 
+   facet_wrap(~ target_type) +
+   labs(y = "Model", x = "Forecast date")
\end{CodeInput}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/plot-forecast-counts-1} 

}

\caption[Visualistion of forecast counts for the example data]{Visualistion of forecast counts for the example data. Numbers (and colour shade) indicate the number of forecasts available for a given model, target type and forecast date.}\label{fig:plot-forecast-counts}
\end{figure}
\end{CodeChunk}

\subsubsection{Probabilistic calibration and PIT
histograms}\label{probabilistic-calibration-and-pit-histograms}

One important quality of good forecasts is calibration. The term
describes a statistical consistency between the forecasts and the
observations, i.e., an absence of systematic deviations between the two.
It is possible to distinguish several forms of calibration which are
discussed in detail by
\cite{gneitingProbabilisticForecastsCalibration2007}. The form of
calibration most commonly focused on is called probabilistic
calibration. Probabilistic calibration means that the forecast
distributions are consistent with the true data-generating distributions
in the sense that on average, \(\tau\)\% of true observations will be
below the corresponding \(\tau\)-\%-quantiles of the cumulative forecast
distributions.

A common way to visualise probabilistic calibration is the probability
integral transform (PIT) histogram
\citep{dawidPresentPositionPotential1984}. Observed values, \(y\), are
transformed using the CDF of the predictive distribution, \(F\), to
create a new variable \(u\) with \(u = F(y)\). \(u\) is therefore simply
the CDF of the predictive distribution evaluated at the observed value.
If forecasts are probabilistically calibrated, then the transformed
values will be uniformly distributed (for a proof see for example
\citet{angusProbabilityIntegralTransform1994}). When plotting a
histogram of PIT values (see Figure \ref{fig:pit-plots}), a systematic
bias usually leads to a triangular shape, a U-shaped histogram
corresponds to forecasts that are underdispersed (too sharp) and a hump
shape appears when forecasts are overdispersed (too wide). There exist
different variations of the PIT to deal with discrete instead of
continuous data (see e.g., \cite{czadoPredictiveModelAssessment2009} and
\cite{funkAssessingPerformanceRealtime2019}). The PIT version
implemented in \texttt{scoringutils} for discrete variables follows
\cite{funkAssessingPerformanceRealtime2019}.

Users can obtain PIT histograms based on validated forecast objects
using the function \code{get\_pit()} and can visualise results using
\code{plot\_pit()}. Once again, the argument \texttt{by} controls the
summary level. The output of the following is shown in Figure
\ref{fig:pit-plots}:

\begin{CodeChunk}
\begin{CodeInput}
R> example_sample_continuous |>
+   as_forecast() |>
+   get_pit(by = c("model", "target_type")) |>
+   plot_pit() + 
+   facet_grid(target_type ~ model)
\end{CodeInput}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/pit-plots-1} 

}

\caption[PIT histograms of all models stratified by forecast target]{PIT histograms of all models stratified by forecast target. Histograms should ideally be uniform. A u-shape usually indicates overconfidence (forecasts are too narrow), a hump-shaped form indicates underconfidence (forecasts are too uncertain) and a triangle-shape indicates bias.}\label{fig:pit-plots}
\end{figure}
\end{CodeChunk}

It is, in theory, possible to conduct a formal test for probabilistic
calibration, for example by employing an Anderson-Darling test on the
uniformity of PIT values. In practice, this can be difficult as
forecasts, and therefore PIT values as well, are often correlated.
Personal experience suggests that the Anderson-Darling test is often too
quick to reject the null hypothesis of uniformity. It is also important
to note that uniformity of the PIT histogram does not guarantee that
forecasts are indeed calibrated.
\cite{gneitingProbabilisticForecastsCalibration2007, hamillInterpretationRankHistograms2001a}
provide examples with different forecasters who are mis-calibrated, but
have uniform PIT histograms.

\subsubsection{Probabilistic calibration and coverage
plots}\label{probabilistic-calibration-and-coverage-plots}

For forecasts in a quantile-based format, there exists a second way to
assess probabilistic calibration: we can easily compare the proportion
of observations that fall below the \(\tau\)-quantiles of all forecasts
(``empirical quantile coverage'') to the nominal quantile coverage
\(\tau\). Similarly, we can compare the empirical coverage of the
central prediction intervals formed by the predictive quantiles to the
nominal interval coverage. For example, the central 50\% prediction
intervals of all forecasts should contain around 50\% of the observed
values, the 90\% central intervals should contain around 90\% of
observations etc. In addition, we can define coverage deviation as the
difference between nominal and empirical coverage.

Interval and quantile coverage can easily be computed by calling
\code{get_coverage()} on a validated forecast object (in a
quantile-based format). The function computes interval coverage,
quantile coverage, interval coverage deviation and quantile coverage
deviation and returns a \texttt{data.table} with corresponding columns.
Coverage values will be summarised according to the level specified in
the \texttt{by} argument and one value per quantile level/interval range
is returned.

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_quantile |>
+   get_coverage(by = "model") |>
+   print(2)
\end{CodeInput}
\end{CodeChunk}

Results can then be visualised using the functions
\texttt{plot\_interval\_coverage()} (see Figure \ref{fig:coverage}A) and
\texttt{plot\_quantile\_coverage()} (see \ref{fig:coverage}B). Both show
nominal against empirical coverage. Ideally, forecasters should lie on
the diagonal line. If the line moves into the green-shaded area, the
forecaster is too conservative, i.e., the predictive distributions are
too wide/overdispersed on average. The white area implies
overconfidence/predictive distributions that are too narrow on average
(see Figure \ref{fig:calibration-plots}) for more details).

\begin{CodeChunk}
\begin{CodeInput}
R> coverage <- get_coverage(forecast_quantile, by = c("model", "target_type")) 
R> 
R> plot_interval_coverage(coverage) + 
+   facet_wrap(~ target_type)
R> 
R> plot_quantile_coverage(coverage) + 
+   facet_wrap(~ target_type)
\end{CodeInput}
\end{CodeChunk}

Note that users can also compute individual coverage values as scores
using \code{score()}. This represents a separate workflow that allows
users to obtain coverage values as a summary measure to be computed
alongside other scores, rather than providing a way to visually assess
calibration.

\begin{CodeChunk}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/coverage-1} 

}

\caption[Interval coverage (A) and quantile coverage (B) plots]{Interval coverage (A) and quantile coverage (B) plots. Areas shaded in green indicate that the forecasts are too wide (i.e., underconfident), while areas in white indicate that the model is overconfident and generates too narrow prediction intervals.}\label{fig:coverage}
\end{figure}
\end{CodeChunk}

\section{Scoring forecasts} \label{sec:scoring}

Metrics and scoring rules can be applied to data in two different ways:
They can be conveniently applied to a data set of observed and predicted
values using \code{score()}, or they be called directly on a set of
vectors and matrices. This section will mostly focus on \code{score()}.

\subsection{score() and working with scoring
rules}\label{score-and-working-with-scoring-rules}

The function \code{score()} is the workhorse of the package and applies
a set of metrics and scoring rules to predicted and observed values. It
is a generic function that dispatches to different methods depending on
the class of the input. The input of \code{score()} is a validated
forecast object and its output is an object of class \texttt{scores},
which is a essentially \texttt{data.table} with an additional attribute
\texttt{metrics} (containing the names of the metrics used for scoring).

\begin{CodeChunk}
\begin{CodeInput}
R> example_point[horizon == 2] |>
+   as_forecast() |>
+   score() |>
+   print(2)
\end{CodeInput}
\begin{CodeOutput}
Key: <location, target_end_date, target_type>
     location target_end_date target_type observed location_name
       <char>          <Date>      <char>    <num>        <char>
  1:       DE      2021-05-15       Cases    64985       Germany
  2:       DE      2021-05-15       Cases    64985       Germany
 ---                                                            
304:       IT      2021-07-24      Deaths       78         Italy
305:       IT      2021-07-24      Deaths       78         Italy
     forecast_date predicted                 model horizon ae_point
            <Date>     <int>                <char>   <num>    <num>
  1:    2021-05-03    110716 EuroCOVIDhub-ensemble       2    45731
  2:    2021-05-03    132607 EuroCOVIDhub-baseline       2    67622
 ---                                                               
304:    2021-07-12       124       UMass-MechBayes       2       46
305:    2021-07-12       186  epiforecasts-EpiNow2       2      108
       se_point       ape
          <num>     <num>
  1: 2091324361 0.7037162
  2: 4572734884 1.0405786
 ---                     
304:       2116 0.5897436
305:      11664 1.3846154
\end{CodeOutput}
\end{CodeChunk}

All \code{score()} methods take an argument \texttt{metrics} with a
named list of functions to apply to the data. These can be metrics
exported by \pkg{scoringutils} or any other custom scoring function. All
metrics scoring rules passed to \code{score()} need to adhere to the
same input format (see Figure \ref{fig:input-scoring-rules}),
corresponding to the type of forecast to be scored. Scoring functions
must accept a vector of observed values as their first argument, a
matrix/vector of predicted values as their second argument and, for
quantile-based forecasts, a vector of quantile levels as their third
argument). However, functions may have arbitrary argument names. Within
\code{score()}, inputs like the observed and predicted values, quantile
levels etc. are passed to the individual scoring rules by position,
rather than by name. The default scoring rules for point forecasts, for
example, comprise functions from the \pkg{Metrics} package, which use
the names \texttt{actual} and \texttt{predicted} for their arguments
instead of \texttt{observed} and \texttt{predicted}. Additional
arguments can be passed down to the scoring functions via the
\texttt{...} arguments in \code{score()}.

\begin{CodeChunk}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{output/input-formats-scoring-rules} 

}

\caption{Overview of the inputs and outputs of the metrics and scoring rules exported by \pkg{scoringutils}. Dots indicate scalar values, while bars indicate vectors (comprised of values that belong together). Several bars (vectors) can be grouped into a matrix with rows representing the individual forecasts. All scoring functions used within \fct{score} must accept the same input formats as the functions here. However, functions used within \fct{score} do not necessarily have to have the same argument names (see Section \ref{sec:scoring}). Input formats directly correspond to the required columns for the different forecast types (see Table \ref{tab:input-score}). The only exception is the forecast type 'sample': Inputs require a column \code{sample\_id} in \fct{score}, but no corresponding argument is necessary when calling scoring rules directly on vectors or matrices.}\label{fig:input-scoring-rules}
\end{figure}
\end{CodeChunk}

\subsubsection{Composing a custom list of metrics and scoring
rules}\label{composing-a-custom-list-of-metrics-and-scoring-rules}

For every forecast type, there exists a default list of scoring rules
that are applied to the data when calling \code{score()}. The default
lists can be accessed by calling the functions \code{metrics\_point()},
\code{metrics\_binary()}, \code{metrics\_sample()} and
\code{metrics\_quantile()}. These functions take additional arguments
\texttt{exclude} and \texttt{select} which can be used to customise
which scoring rules are included. Alternatively, users can call the
function \code{select\_metrics()} on a list of scoring rules, which
achieves the same purposes and allows users to compose custom lists of
metrics and scoring rules.

\begin{CodeChunk}
\begin{CodeInput}
R> custom_metrics <- metrics_quantile() |>
+   select_metrics(select = c("wis", "overprediction"))
R> 
R> score(metrics = custom_metrics)
\end{CodeInput}
\end{CodeChunk}

\subsubsection{Details on metrics exported by scoringutils}

All metrics are named according to the following schema:
\texttt{\{metric\ name\}\_\{forecast\ type\}}. If only a single forecast
type is possible, then \texttt{\_\{forecast\ type\}} is omitted. The
return value is a vector with scores (only in the case of \code{wis()},
which is composed of three components (see \ref{sec:wis}), is there an
optional argument that causes the function to return a list of vectors
for the individual WIS components). The first argument of all metrics
exported by \pkg{scoringutils} is always \texttt{observed}, and the
second one is \texttt{predicted}. Scoring rules for quantile-based
forecasts have an additional argument, \texttt{quantile\_level}, to
denote the quantile levels of the predictive quantiles.

Metrics exported by \pkg{scoringutils} differ in the relationship
between input and output. Some scoring rules have a one-to-one
relationship between predicted values and scores, returning one value
per value in \texttt{predicted}. This is the case for all metrics for
binary and point forecasts. Other scoring rules have a many-to-one
relationship, returning one value per multiple values in
\texttt{predicted}. This is the case for all scoring rules for sample-
and quantile-based forecasts. For sample- and quantile-based forecasts,
\texttt{predicted} is therefore a matrix, with values in each row
jointly forming a single forecast.

Input formats and return values are shown in more detail in Figure
\ref{fig:input-scoring-rules}. The package vignettes provide extensive
documentation for the metrics exported by \pkg{scoringutils} and offer
guidance on which scoring rule to use and how to interpret the scores.

\subsection{Adding relative skill scores based on pairwise
comparisons}\label{pairwisetheory}

Raw scores for different forecasting models are usually not directly
comparable when there are missing forecasts in the data set, as
missingness is often correlated with predictive performance. One way to
mitigate this are relative skill scores based on pairwise comparisons
\citep{cramerEvaluationIndividualEnsemble2021}.

Models enter a `pairwise tournament', where all possible pairs of models
are compared based on the overlapping set of available forecasts common
to both models (omitting comparisons where there is no overlapping set
of forecasts). For every pair, the ratio of the mean scores of both
models is computed. The relative skill score of a model is then the
geometric mean of all mean score ratios which involve that model (see
Figure \ref{fig:pairwise-comparison}. This gives us an indicator of
performance relative to all other models, with the orientation depending
on the score used: if lower values are better for a particular scoring
rule, then the same is true for the relative skill score computed based
on that score.

Two models can of course only be fairly compared if they have
overlapping forecasts. Furthermore, pairwise comparisons between models
for a given score are only possible if all values have the same sign,
i.e., all score values need to be either positive or negative.

\begin{CodeChunk}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{output/pairwise-comparisons} 

}

\caption[Illustration of the computation of relative skill scores through pairwise comparisons of three different forecast models, M1-M3]{Illustration of the computation of relative skill scores through pairwise comparisons of three different forecast models, M1-M3. Score ratios are computed based on the overlapping set of forecasts common to all pairs of two models. The relative skill score of a model is then the geometric mean of all mean score ratios which involve that model. The orientation of the relative skill score depends on the score used: if lower values are better for a particular scoring rule, then the same is true for the relative skill score computed based on that score.}\label{fig:pairwise-comparison}
\end{figure}
\end{CodeChunk}

To compute relative skill scores, users can call
\code{add\_pairwise\_comparison()} on the output of \code{score()}. This
function computes relative skill values with respect to a score
specified in the argument \texttt{metric} and adds them as an additional
column to the input data. Optionally, users can specify a baseline model
to also compute relative skill scores scaled with respect to that
baseline. Scaled relative skill scores are obtained by simply dividing
the relative skill score for every individual model by the relative
skill score of the baseline model. Pairwise comparisons are computed
according to the grouping specified in the argument \code{by}:
internally, the \code{data.table} with all scores gets split into
different \code{data.table}s according to the values specified in
\code{by} (excluding the column `model'). Relative scores are then
computed for every individual group separately. In the example below we
specify \code{by = c("model", "target_type")}, which means that there is
one relative skill score per model, calculated completely separately for
the different forecasting targets.

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_quantile |>
+   score() |>
+   add_relative_skill(by = c("model", "target_type"), 
+                      baseline = "EuroCOVIDhub-baseline")
\end{CodeInput}
\end{CodeChunk}

Pairwise comparisons should usually be made based on raw, unsummarised
scores (meaning that \texttt{add\_relative\_skill()} should be called
before \texttt{summarise\_scores()} (see Section
\ref{sec:summarising})). Summarising scores, for example by computing an
average across several dimensions, can change the set of overlapping
forecasts between two models and distort relative skill scores.

\subsection{Additional functionality related to scores
objects}\label{additional-functionality-related-to-scores-objects}

\subsubsection{Displaying mean score ratios from pairwise
comparisons}\label{displaying-mean-score-ratios-from-pairwise-comparisons}

\pkg{scoringutils} offers a second alternative workflow to conduct
pairwise comparisons between models through the function
\texttt{get\_pairwise\_comparisons()}. The purpose of this workflow is
to obtain and visualise information on the direct comparisons between
every possible pair of models, rather than just computing relative skill
scores for every model. The function
\texttt{get\_pairwise\_comparisons()} accepts the same inputs as
\code{add\_relative\_skill()}, and returns a \code{data.table} with the
results of the pairwise tournament. These include the mean score ratios
for every pair of models, a \(p\)\textasciitilde value for whether
scores for one model are significantly different from scores for another
model, and the relative and scaled relative skill score for every model
(depending on whether a baseline was provided or not).

\texttt{get\_pairwise\_comparisons()} computes
\(p\)\textasciitilde values using either the Wilcoxon rank sum test (the
default, the test is also known as Mann-Whitney-U test)
\citep{mannTestWhetherOne1947} or a permutation test.
\(p\)\textasciitilde values are then adjusted using \texttt{p.adjust}.
In practice, the computation of \(p\)\textasciitilde values is
complicated by the fact that both tests assume independent observations.
In reality, however, forecasts by a model may be correlated across time
or space (e.g., if a forecaster has a bad day, they might perform badly
across different targets for a given forecast date).
\(p\)\textasciitilde values may therefore be too liberal in suggesting
significant differences where there aren't any. We previously suggested
computing relative skill scores based on pairwise comparisons before
summarising scores. One exception is the case where one is interested in
\(p\)\textasciitilde values specifically: One possible way to mitigate
issues from correlated forecasts, is to aggregate observations over a
category where one suspects correlation (provided there are no missing
values within the categories summarised over) to reduce correlation
before making pairwise comparisons. A test that is performed on
aggregate scores will likely be more conservative.

The mean score ratios resulting from \code{pairwise\_comparison()} can
then be visualised using the function
\code{plot\_pairwise\_comparison()}. An example is shown in Figure
\ref{fig:pairwise-plot}.

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_quantile |>
+   score() |>
+   get_pairwise_comparisons(by = c("model", "target_type")) |>
+   plot_pairwise_comparisons() + 
+   facet_wrap(~ target_type)
\end{CodeInput}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/pairwise-plot-1} 

}

\caption[Ratios of mean weighted interval scores based on overlapping forecast sets]{Ratios of mean weighted interval scores based on overlapping forecast sets. When interpreting the plot one should look at the model on the y-axis, and the model on the x-axis is the one it is compared against. If a tile is blue, then the model on the y-axis performed better (assuming that scores are negatively oriented, i.e., that lower scores are better). If it is red, the model on the x-axis performed better in direct comparison. In the example above, the EuroCOVIDhub-ensemble performs best (it only has values smaller than one), while the EuroCOVIDhub-baseline performs worst (and only has values larger than one). For cases, the UMass-MechBayes model is excluded as there are no case forecasts available and therefore the set of overlapping forecasts is empty.}\label{fig:pairwise-plot}
\end{figure}
\end{CodeChunk}

\subsubsection{Correlations between
scores}\label{correlations-between-scores}

Users can examine correlations between scores using the function
\code{correlations()} and plot the result using
\code{plot\_correlations()}. The plot resulting from the following code
is shown in Figure \ref{fig:correlation-plot}.

\begin{CodeChunk}
\begin{CodeInput}
R> correlations <- forecast_quantile |>
+   score() |>
+   summarise_scores() |>
+   get_correlations()
R> 
R> correlations |>
+   plot_correlations(digits = 2)
\end{CodeInput}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/correlation-plot-1} 

}

\caption[Plot of correlations between different scores]{Plot of correlations between different scores. Numbers, as well as the shade of the cells, indicate the correlation between two scores.}\label{fig:correlation-plot}
\end{figure}
\end{CodeChunk}

\section{Summarising results} \label{sec:summarising}

\subsection{Summarising scores}\label{summarising-scores}

Usually, one will not be interested in scores for each individual
forecast, but rather in summarised scores. This can be achieved using
the function \code{summarise\_scores()}. The function takes a
\texttt{scores} object (a data.table with an additional attribute
\texttt{metrics}) as input and applies a summary function to it (by
default the mean), returning a \texttt{data.table} with summarised
scores. Users can set the summary level using the argument \texttt{by}
and will obtain a summarised score for each combination of the value in
the specified columns (e.g., \texttt{by\ =\ c("model",\ "target\_type")}
will return one summarised score per model and target type). To display
scores it is often useful to round the output, for example to two
significant digits, which can be achieved with another call to
\code{summarise\_scores()}.

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_quantile |>
+   score(metrics = list("wis" = wis)) |>
+   summarise_scores(by = c("model", "target_type")) |>
+   summarise_scores(fun = signif, digits = 2)
\end{CodeInput}
\begin{CodeOutput}
                   model   wis
                  <char> <num>
1: EuroCOVIDhub-ensemble 17000
2: EuroCOVIDhub-ensemble    41
3: EuroCOVIDhub-baseline 29000
4: EuroCOVIDhub-baseline   160
5:  epiforecasts-EpiNow2 21000
6:  epiforecasts-EpiNow2    69
7:       UMass-MechBayes    52
\end{CodeOutput}
\end{CodeChunk}

While \code{summarise\_scores()} accepts arbitrary summary functions,
care has to be taken when using something else than \code{mean()}, as
this may create an incentive for dishonest reporting. Many scoring rules
for probabilistic forecasts are `strictly proper scoring rules'
\citep{gneitingStrictlyProperScoring2007}, meaning that they are
constructed such that they cannot be cheated and always incentivise the
forecaster to report her honest belief about the future. Let's assume
that a forecaster's true belief about the future corresponds to a
predictive distribution \(F\). Then, if \(F\) was the true
data-generating process, a scoring rule would be proper if it ensures
that no other forecast distribution \(G\) would yield a better expected
score. If the scoring rule ensures that under \(F\) no other possible
predictive distribution can achieve the same expected score as \(F\),
then it is called strictly proper. From the forecaster's perspective,
any deviation from her true belief \(F\) leads to a worsening of
expected scores. When using summary functions other than the mean,
however, scores may lose their propriety (the property of incentivising
honest reporting) and become cheatable. For example, the median of
several individual scores (individually based on a strictly proper
scoring rule) is usually not proper. A forecaster judged by the median
of several scores may be incentivised to misrepresent their true belief
in a way that is not true for the mean score.

The user must exercise additional caution and should usually avoid
aggregating scores across categories which differ much in the magnitude
of the quantity to forecast, as (depending on the scoring rule used)
forecast errors usually increase with the order of magnitude of the
forecast target. In the given example, looking at one score per model
(i.e., specifying \code{by = c("model")}) is problematic, as overall
aggregate scores would be dominated by case forecasts, while performance
on deaths would have little influence. Similarly, aggregating over
different forecast horizons is often ill-advised as the mean will be
dominated by further ahead forecast horizons. In the previous function
calls, we therefore decided to only analyse forecasts with a forecast
horizon of two weeks.

\subsection{Additional functionality for summarised
scores}\label{additional-functionality-for-summarised-scores}

\subsubsection{Heatmaps}\label{heatmaps}

To detect systematic patterns it may be useful to visualise a single
score across several dimensions. The function \code{plot\_heatmap()} can
be used to create a heatmap that achieves this. The following produces a
heatmap of bias values across different locations and forecast targets
(output shown in Figure \ref{fig:score-heatmap}).

\begin{CodeChunk}
\begin{CodeInput}
R> example_sample_continuous[horizon == 2] |>
+   as_forecast() |>
+   score() |>
+   summarise_scores(by = c("model", "location", "target_type")) |>
+   summarise_scores(
+     by = c("model", "location", "target_type"), 
+     fun = signif, digits = 2) |>
+   plot_heatmap(x = "location", metric = "bias") + 
+     facet_wrap(~ target_type) 
\end{CodeInput}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/score-heatmap-1} 

}

\caption[Heatmap of bias values for different models across different locations and forecast targets]{Heatmap of bias values for different models across different locations and forecast targets. Bias values are bound between -1 (underprediction) and 1 (overprediction) and should be 0 ideally. Red tiles indicate an upwards bias (overprediction), while blue tiles indicate a downwards bias (underprediction)}\label{fig:score-heatmap}
\end{figure}
\end{CodeChunk}

\subsubsection{Weighted interval score
decomposition}\label{weighted-interval-score-decomposition}

For quantile-based forecasts, the weighted interval score
\citep[WIS, ][see Section \ref{sec:wis} in the Appendix]{bracherEvaluatingEpidemicForecasts2021}
is a commonly used strictly proper scoring rule for forecasts in a
quantile-based format. The score is the sum of three components:
overprediction, underprediction and dispersion (width of the forecast).
These can be visualised using the function \code{plot\_wis()}, as shown
in Figure \ref{fig:wis-components}.

\begin{CodeChunk}
\begin{CodeInput}
R> forecast_quantile |>
+   score() |>
+   summarise_scores(by = c("model", "target_type")) |>
+   plot_wis(relative_contributions = FALSE) + 
+   facet_wrap(~ target_type, 
+              scales = "free_x") 
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/wis-components-1} 

}

\caption[Decomposition of the weighted interval score (WIS) into dispersion, overprediction and underprediction]{Decomposition of the weighted interval score (WIS) into dispersion, overprediction and underprediction. A: absolute contributions, B: contributions normalised to 1.}\label{fig:wis-components}
\end{figure}
\end{CodeChunk}

\section{Discussion} \label{sec:discussion}

\subsubsection{Summary}\label{summary}

This paper presented \pkg{scoringutils} an R package for forecast
evaluation. It explained the core workflow, consisting of 1) validating
and processing inputs, 2) scoring forecasts and 3) summarising scores,
as well as additional functionality such as visualisation and diagnostic
tools.

The package specialises in the evaluation of probabilistic forecasts
(the forecast is a full predictive distribution). It provides a
comprehensive framework based on \pkg{data.table} and allows users to
validate, diagnose, visualise, transform and score forecasts using a
wide range of default and custom scoring rules. The package is designed
to be flexible and extensible, and to make it easy to use functionality
from different packages in a single workflow. \pkg{scoringutils}
addresses a gap in the existing ecosystem of forecast evaluation by
creating a \texttt{data.table}-based forecast evaluation framework for
probabilistic forecasts (similarly to what \pkg{yardstick} provides for
point forecasts and classification tasks). Notably, \pkg{scoringutils}
is the first package to provide extensive support for forecasts in a
quantile-based forecasts, which is commonly used for example in
Epidemiology. In addition to providing a coherent forecast evaluation
workflow it offers a wide range of additional functions that
practitioners may find useful when assessing or comparing the quality of
their forecasts.

One important limitation of the package is that it currently does not
support statistical testing of forecast performance as part of its core
workflow. Determining whether a forecaster is significantly better than
another is an important aspect of forecast evaluation that is currently
mostly missing from the package. Another limitation is the fact that the
package currently only supports a small set of possible types of
forecasts. For example, forecasts in a bin-format or forecasts
represented in a closed-form distribution (as can be scored for example
using \pkg{scoringRules} are not supported. While it is in principle
possible to extend the current classes and generic functions, this may
not be very feasible in practice for most users. Some functionality in
\pkg{scoringutils} is necessarily redundant with other packages that
provide functionality to aid with the evaluation of forecasts. The
overall idea of providing a \texttt{data.frame}-based evaluation
framework, for example, is similar to what \pkg{yardstick} offers
(albeit with a focus on point forecasts and classification tasks, rather
than probabilistic forecasts). Having a single package that encompasses
all possible use cases might be preferable. At the moment,
\pkg{scoringutils} falls somewhat short of its aspiration to become a
bridge between different packages in the forecast evaluation ecosystem.
It does not yet offer a wide range of helper functions that allow users
to easily convert between different formats and use functionality from
other packages and many visualisations that are available in other
packages, particularly with respect to model calibration, are missing.

A variety of extensions are planned for \pkg{scoringutils}. The first is
the expansion of the forecast types that are supported. We plan to add
support for evaluating categorical forecasts, as well as multivariate
forecasts that specify a joint distribution across targets. Adding the
possibility to score closed-form distributions might be another useful
extension. A second area of expansion is the integration with other
forecast evaluation and modelling packages. We aim to provide a variety
of helper functions to convert to and from different formats, such as
the one used by \pkg{yardstick} or formats used by modelling packages
such as \pkg{odin}. These functions would make it easy to integrate
\pkg{scoringutils} into existing workflows or use functionality from
other packages that is not available in \pkg{scoringutils}. A third area
of improvement is the addition of case studies and vignettes that make
working with and extending functionality from the package easier.

\pkg{scoringutils} is already used by a variety of public health
institutions such as the US Centers for Disease Control, the European
Centre for Disease Prevention and Control, as well as various academic
institutions. The package is actively maintained and developed and we
hope it will continue to be a valuable resource for researchers and
practitioners working on forecast evaluation.

\section{Acknowledgments}\label{acknowledgments}

Funding statements

NIB received funding from the Health Protection Research Unit (grant
code NIHR200908). HG's work was funded by the Wellcome Trust (grant:
210758/Z/18/Z). AC acknowledges funding by the NIHR, the Sergei Brin
foundation, USAID, and the Academy of Medical Sciences. EvL acknowledges
funding by the National Institute for Health Research (NIHR) Health
Protection Research Unit (HPRU) in Modelling and Health Economics (grant
number NIHR200908) and the European Union's Horizon 2020 research and
innovation programme - project EpiPose (101003688). SF's work was
supported by the Wellcome Trust (grant: 210758/Z/18/Z), and the NIHR
(NIHR200908). SA's work was funded by the Wellcome Trust (grant:
210758/Z/18/Z). This study is partially funded by the National Institute
for Health Research (NIHR) Health Protection Research Unit in Modelling
and Health Economics, a partnership between UK Health Security Agency
and Imperial College London in collaboration with LSHTM (grant code
NIHR200908); and acknowledges funding from the MRC Centre for Global
Infectious Disease Analysis (reference MR/R015600/1), jointly funded by
the UK Medical Research Council (MRC) and the UK Foreign, Commonwealth
\& Development Office (FCDO), under the MRC/FCDO Concordat agreement and
is also part of the EDCTP2 programme supported by the European Union.
Disclaimer: ``The views expressed are those of the author(s) and not
necessarily those of the NIHR, UKHSA or the Department of Health and
Social Care. We thank Community Jameel for Institute and research
funding. This work has also been supported by the US National Institutes
of General Medical Sciences (R35GM119582). The content is solely the
responsibility of the authors and does not necessarily represent the
official views of NIGMS, or the National Institutes of Health.

\bibliography{references.bib, scoringutils-paper.bib}

\clearpage

\appendix
\renewcommand\thefigure{\thesection.\arabic{figure}}

\section{Constructing and validating forecast objects}

The following section gives an overview of how \pkg{scoringutils}
constructs forecast objects. The \texttt{forecast} class comes with a
constructor, \code{new\_forecast()}, a generic validation function,
\code{assert\_forecast()}, and a convenient wrapper function
\code{as\_forecast()}.

\code{new\_forecast()} constructs a \texttt{forecast} object based on a
\texttt{data.frame} or similar. It makes a deep copy of the inptut and
converts it into a \texttt{data.table}, adds a \texttt{model} column
with value ``Unspecified model'' if there isn't one and adds a class
\texttt{forecast\_*}, where \texttt{*} depends on the forecast type to
the object.

\code{assert\_forecast()} is a generic which dispatches to a specialised
validator method depending on the class of the input. It validates the
input and returns it if it is valid. If the input is not valid, it
throws an error with a message that explains what went wrong.

\code{as\_forecast()} (optionally) renames existing columns to conform
with the requirements for forecast objects, (optionally) sets the
forecast unit, determines the forecast type of the input (and optionally
checks for consistency with what the user expects), constructs the class
and validates the input. The process is illustrated in Figure
\ref{fig:flowchart-validation}.

\begin{CodeChunk}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth]{output/flowchart-create-object} 

}

\caption[Illustration of the process of creating a `forecast` object]{Illustration of the process of creating a `forecast` object.}\label{fig:flowchart-validation}
\end{figure}
\end{CodeChunk}

\clearpage

\section{Comparing different calibration plots}

The following Figure gives a more detailed overview of how to interpret
different calibration plots (showing the actual forecasts and
observations that produced the corresponding visualisations).

\begin{CodeChunk}
\begin{figure}[!h]

{\centering \includegraphics[width=1\linewidth,]{output/calibration-diagnostic-examples} 

}

\caption[A]{A: Different forecasting distributions (black) against observations sampled from a standard normal distribution (grey histograms). B: PIT histograms based on the predictive distributions and the sampled observations shown in A. C: Empirical vs. nominal coverage of the central prediction intervals for simulated observations and predictions. Areas shaded in green indicate that the forecasts are too wide (i.e., underconfident), covering more true values than they actually should, while areas in white indicate that the model generates too narrow predictions and fails to cover the desired proportion of true values with its prediction intervals. D: Quantile coverage values, with green areas indicating too wide (i.e., conservative) forecasts. E: Scores for the standard normal predictive distribution and the observations drawn from different data-generating distributions.}\label{fig:calibration-plots}
\end{figure}
\end{CodeChunk}

\clearpage

\section{Details on the weighted interval score (WIS)} \label{sec:wis}

The WIS treats the predictive quantiles as a set of symmetric prediction
intervals and measures the distance between the observation and the
forecast interval. It can be decomposed into a dispersion (uncertainty)
component and penalties for over- and underprediction. For a single
interval, the interval score is computed as
\[IS_\alpha(F,y) = \underbrace{(u-l)}_\text{dispersion} + \underbrace{\frac{2}{\alpha} \cdot (l-y) \cdot \mathbf{1}(y \leq l)}_{\text{overprediction}} + \underbrace{\frac{2}{\alpha} \cdot (y-u) \cdot \mathbf{1}(y \geq u)}_{\text{underprediction}}, \]
where \(\mathbf{1}()\) is the indicator function, \(y\) is the observed
value, and \(l\) and \(u\) are the \(\frac{\alpha}{2}\) and
\(1 - \frac{\alpha}{2}\) quantiles of the predictive distribution \(F\),
i.e., the lower and upper bound of a single prediction interval. For a
set of \(K\) prediction intervals and the median \(m\), the score is
computed as a weighted sum,
\[WIS = \frac{1}{K + 0.5} \cdot \left(w_0 \cdot |y - m| + \sum_{k = 1}^{K} w_k \cdot IS_{\alpha}(F, y)\right),\]
where \(w_k\) is a weight for every interval. Usually,
\(w_k = \frac{\alpha_k}{2}\) and \(w_0 = 0.5\).

\clearpage




\end{document}
