{"type":"system","subtype":"init","cwd":"/Users/nikos/github-synced/scoringutils-pipeline","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","tools":["Task","TaskOutput","Bash","Glob","Grep","ExitPlanMode","Read","Edit","Write","NotebookEdit","WebFetch","TodoWrite","WebSearch","TaskStop","AskUserQuestion","Skill","EnterPlanMode","ToolSearch"],"mcp_servers":[],"model":"claude-opus-4-6","permissionMode":"bypassPermissions","slash_commands":["keybindings-help","debug","compact","context","cost","init","pr-comments","release-notes","review","security-review","insights"],"apiKeySource":"none","claude_code_version":"2.1.39","output_style":"default","agents":["Bash","general-purpose","statusline-setup","Explore","Plan","claude-code-guide"],"skills":["keybindings-help","debug"],"plugins":[],"uuid":"bb4b083e-46a4-4de2-9380-627a07567212","fast_mode_state":"off"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_014dzjkVJZteszc5KUWHmkQ8","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nI'll start by verifying the worktree and branch, then find the next automatable issue to design tests for."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2421,"cache_read_input_tokens":18004,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2421},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"92ad6257-56fa-469e-be71-5adf89b88642"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_014dzjkVJZteszc5KUWHmkQ8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WssXoSUnR74NmYvW4QrsGc","name":"Bash","input":{"command":"git rev-parse --show-toplevel","description":"Check git worktree root"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2421,"cache_read_input_tokens":18004,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2421},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"86ba701a-b51c-4616-9bb8-9aaf86beb224"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_014dzjkVJZteszc5KUWHmkQ8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016kezdnGFxbpDwojTK4wHXb","name":"Bash","input":{"command":"git branch --show-current","description":"Check current branch"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2421,"cache_read_input_tokens":18004,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2421},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"2f4a101a-a7e0-4aea-a072-bd1ebaf6262e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WssXoSUnR74NmYvW4QrsGc","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline","is_error":false}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"6d619b30-4a2e-4046-8a18-e84b224b6345","tool_use_result":{"stdout":"/Users/nikos/github-synced/scoringutils-pipeline","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016kezdnGFxbpDwojTK4wHXb","type":"tool_result","content":"ralph","is_error":false}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"b8c8f679-c8e5-4105-b1c5-06d2010c73c5","tool_use_result":{"stdout":"ralph","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017XjLitY5M7a24tsnfw767p","type":"message","role":"assistant","content":[{"type":"text","text":"Worktree and branch verified. Now let me update from main and read the triage table."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":248,"cache_read_input_tokens":20425,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":248},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"ebc21584-81c7-438b-8209-ec3b5f02f161"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017XjLitY5M7a24tsnfw767p","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XCmGf2c3BHdiwudoL7qHWE","name":"Bash","input":{"command":"git fetch origin main && git merge origin/main --ff-only","description":"Update ralph branch with latest main"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":248,"cache_read_input_tokens":20425,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":248},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8de06434-b048-441a-bb4d-fb84b9322b08"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017XjLitY5M7a24tsnfw767p","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015kryzF5cPLBxP7F6r5Dm2o","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":248,"cache_read_input_tokens":20425,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":248},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"cb0db992-b73f-4061-8060-d4c250af8dd9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XCmGf2c3BHdiwudoL7qHWE","type":"tool_result","content":"Already up to date.\nFrom https://github.com/epiforecasts/scoringutils\n * branch              main       -> FETCH_HEAD","is_error":false}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"66036328-6301-413c-8481-10c0a912cd6a","tool_use_result":{"stdout":"Already up to date.\nFrom https://github.com/epiforecasts/scoringutils\n * branch              main       -> FETCH_HEAD","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015kryzF5cPLBxP7F6r5Dm2o","type":"tool_result","content":"     1→# scoringutils Issue Triage\n     2→\n     3→> Auto-generated by Ralph triage loop. Do not edit manually during the loop.\n     4→\n     5→## Summary\n     6→\n     7→| Metric | Count |\n     8→|--------|-------|\n     9→| Total open issues | 106 |\n    10→| Triaged | 106 |\n    11→| Remaining | 0 |\n    12→\n    13→## Pipeline\n    14→\n    15→- [ ] **Test design** — Test specs written for automatable issues\n    16→- [ ] **Implement** — Fixes implemented, PRs created\n    17→- [ ] **Review** — All PRs independently reviewed\n    18→- [ ] **Summary** — Human summary generated\n    19→\n    20→## Issues\n    21→\n    22→| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n    23→|---|-------|--------|--------|----------|----------|------------|-------------|-------------|--------------|----------------|-----------|-------------|-----|----------|\n    24→| 1065 | Release reminder - February 2026 | | 2026-02-02 | meta | medium | medium | partially | Automated reminder. Dev version (2.1.2.9000) has multivariate feature + bug fix accumulated over 161 days. No blocking bugs. Two open PRs (#1060, #1062) are infrastructure changes. | Maintainers should decide whether to merge #1060/#1062 before releasing. | Decide whether to cut a new release (likely v2.2.0 given multivariate feature). If yes, run `usethis::use_release_issue()` and proceed with CRAN submission. If not, close the issue. | N/A | N/A | — | N/A |\n    25→| 1064 | Pre-ranking for multivariate calibration assessment | | 2026-01-30 | feature | medium | large | no | No pre-ranking or multivariate calibration diagnostics exist in the codebase. The `forecast_sample_multivariate` class and energy score are implemented and stable. Design decision needed: integrate into `score()` (Option A) or standalone `get_multivariate_ranks()` function (Option B). MST method may need `igraph` dependency. | None blocking. Builds on existing `forecast_sample_multivariate` infrastructure. | Implement pre-ranking methods (multivariate rank, optionally MST rank) from Gneiting et al. (2008) as calibration diagnostics for multivariate forecasts, after resolving the API design choice with maintainers. | N/A | N/A | — | N/A |\n    26→| 1057 | Revise README to remove news about the package rewrite | documentation | 2026-01-29 | documentation | low | medium | partially | Open, no assignee. The rewrite messaging and Google Drive paper links appear in 4 locations in README.Rmd. Mechanical edits are straightforward but replacement text requires editorial decisions about where to point users for documentation. | Loosely related to #810 (pkgdown for dev docs) and #759 (future of scoringutils.Rmd vignette). | Edit `README.Rmd` to remove/soften the \"major re-write\" paragraph and replace Google Drive paper links with pointers to package vignettes/docs, then re-render README.md. Confirm preferred documentation hierarchy with maintainers first. | N/A | N/A | — | N/A |\n    27→| 1055 | Model confidence sets | feature request | 2026-01-19 | feature | medium | large | no | Design phase — methodology not yet agreed upon. 6 comments with active debate. Naive pairwise p-value approach acknowledged as incomplete. Three alternatives discussed: Hansen et al. MCS, novel extension for incomplete sets, Bayesian approach. No consensus reached. | Relates to #750 (p-value computation refactor), #769 (pairwise comparison arguments), #37 (Diebold-Mariano test), #1022 (two-model pairwise). | Resolve design consensus on which MCS methodology to implement (Hansen et al. on complete sets, novel incomplete-set extension, or Bayesian), then implement `get_model_confidence_set()` building on existing pairwise comparison infrastructure. | N/A | N/A | — | N/A |\n    28→| 1046 | multivariate scoring in scoring rules vignette | documentation | 2025-11-04 | documentation | low | small | partially | Open, agreed upon by maintainers. The scoring-rules vignette has zero mentions of multivariate scoring. A dedicated multivariate vignette already exists. Only a cross-reference and brief section are missing. | None. All underlying code and the dedicated multivariate vignette already exist. | Add a short section to `vignettes/scoring-rules.Rmd` describing the energy score for multivariate forecasts and cross-linking to the existing `scoring-multivariate-forecasts` vignette. No code changes needed. | N/A | N/A | — | N/A |\n    29→| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |\n    30→| 1043 | Update print method for multivariate forecasts to show `joint_across` | | 2025-09-21 | enhancement | low | small | yes | Still relevant. No `print.forecast_sample_multivariate` method exists; the generic `print.forecast` handles all types. `joint_across` is not stored as an attribute — `set_grouping()` computes `.mv_group_id` but discards the original `joint_across` value. | Builds on multivariate infrastructure from merged PR #998. | Add a `print.forecast_sample_multivariate()` method that displays `joint_across` columns. Requires either storing `joint_across` as an attribute during object construction or computing it as the set difference between forecast unit and `get_grouping()` output. Update test snapshots. | [x] | [ ] | — | [ ] |\n    31→| 1026 | as_forecast_sample.forecast_quantile | enhancement | 2025-07-13 | enhancement | medium | medium | partially | Still relevant. The reverse conversion (`as_forecast_quantile.forecast_sample`) exists, but `as_forecast_sample.forecast_quantile` does not. No distribution-fitting dependencies in DESCRIPTION. Author plans to borrow from hubverse approach. | Related to #557 (sample_to_quantile.numeric). May introduce new dependency (e.g., `distfromq`). | Implement `as_forecast_sample.forecast_quantile()` S3 method that fits a distribution or uses interpolation to approximate the predictive CDF from quantiles and draws samples. Requires design decision on approximation method and potential new package dependency. | N/A | N/A | — | N/A |\n    32→| 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |\n    33→| 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |\n    34→| 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |\n    35→| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n    36→| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [ ] | [ ] | — | [ ] |\n    37→| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n    38→| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [ ] | [ ] | — | [ ] |\n    39→| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n    40→| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n    41→| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n    42→| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n    43→| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n    44→| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n    45→| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n    46→| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n    47→| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n    48→| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [ ] | [ ] | — | [ ] |\n    49→| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n    50→| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n    51→| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n    52→| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [ ] | [ ] | — | [ ] |\n    53→| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n    54→| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n    55→| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n    56→| 880 | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()? | | 2024-07-30 | enhancement | low | small | yes | Still relevant. Function hard-codes `0.5` on two lines (107, 115) in `R/class-forecast-quantile.R`. Design decisions already resolved in PR #876 discussion: parameter name `quantile_level`, default `0.5`, passed via `...`. No work done since issue opened. | None. Self-contained enhancement. | Add a `quantile_level` parameter (default `0.5`) to `as_forecast_point.forecast_quantile()`, replacing the two hard-coded `0.5` references, then add tests and documentation. | [ ] | [ ] | — | [ ] |\n    57→| 872 | Add a reference for CRPS decomposition approach. | | 2024-07-25 | documentation | low | small | partially | Still relevant. `crps_sample()` in `R/metrics-sample.R` has only a `scoringRules` package reference, not one for the decomposition approach. `wis()` already properly cites Bracher et al. 2021. Minor decision needed on which paper(s) to cite: Bracher et al. 2021 and/or Arnold et al. 2023. | Related to #870 (CRPS/WIS decomposition refactor). Could be addressed in same PR. | Add `@references` entries to `crps_sample()` roxygen block citing paper(s) formalizing the overprediction/underprediction/dispersion decomposition (likely Bracher et al. 2021 and/or Arnold et al. 2023). | N/A | N/A | — | N/A |\n    58→| 870 | Refactor CRPS and WIS decomposition | refactor | 2024-07-22 | refactor | low | medium | partially | Partially addressed. PR #985 (Feb 2025) simplified `crps_sample()` decomposition internally, but the structural duplication remains: `crps_sample()`, `wis()`, and `interval_score()` each independently implement `separate_results` branching, and 6 wrapper functions (3 sample + 3 quantile) repeat identical extraction patterns. | Related to #872 (CRPS decomposition reference). Filed during PR #854 review. On `scoringutils-2.x` milestone. | Extract the decomposition logic (`separate_results` branching and wrapper function boilerplate) into a shared internal helper or factory pattern so `crps_sample()`, `wis()`, and `interval_score()` delegate to the same mechanism instead of each implementing it independently. | N/A | N/A | — | N/A |\n    59→| 852 | Design workflow for creating new forecast classes | enhancement | 2024-07-10 | enhancement | high | large | no | Still relevant. The building blocks exist (`new_forecast()` exported, `as_forecast_generic()` internal, pattern visible in 7 class files) but no documentation, vignette, or guide for third-party class creation exists. Three prerequisite S3 refactors (#889, #888, #740) remain open and block clean extensibility because `get_protected_columns()` and `get_duplicate_forecasts()` hard-code column names, preventing new classes from working seamlessly. `as_forecast_generic()`, `apply_metrics()`, and `as_scores()` are still unexported (#885). | #889 (S3 `get_protected_columns`), #888 (S3 `get_duplicate_forecasts`), #740 (`get_forecast_unit` customization), #885 (export decision for helper functions). | Complete prerequisite S3 refactors (#889, #888), decide export list for class-creation helpers (#885), then write a vignette or article documenting the step-by-step workflow for creating a custom `forecast_*` class (define `as_forecast_<type>`, `assert_forecast.<type>`, `score.<type>`, `get_metrics.<type>`, `get_protected_columns.<type>`, `get_duplicate_forecasts.<type>`). | N/A | N/A | — | N/A |\n    60→| 840 | Clean up input checks | | 2024-06-07 | refactor | low | medium | partially | Still relevant. Input checks are applied inconsistently: `assert_forecast.forecast_binary` and `assert_forecast.forecast_point` call `check_input_*` wrappers (double-validating with scoring functions), while sample/quantile/ordinal/nominal types skip this. `check-input-helpers.R` has a comment marking `check_try()` and wrappers for future deletion. Author leans toward pushing all checks to the scoring-function level. No work done. | Related to #939 (remove `check_input_*` functions), #839, #922. On `scoringutils-2.x` milestone. | Decide where input validation should live (scoring-function level vs. `assert_forecast` level vs. both-consistently). Then either remove `check_input_binary`/`check_input_point` calls from `assert_forecast` methods and delete wrappers + `check_try()`, or add missing checks to sample/quantile `assert_forecast` methods for consistency. | N/A | N/A | — | N/A |\n    61→| 839 | Remove `test_columns_not_present` | | 2024-06-07 | cleanup | low | small | yes | Still relevant. `test_columns_not_present()` is a trivial 1-line wrapper around `checkmate::test_names(..., disjunct.from = ...)` with exactly 1 production call site (`R/class-forecast-binary.R:68`) and 1 test block. `checkmate::test_names` is already imported. Straightforward removal. | Related to #922 (remove `check_columns_present`) and #840 (clean up input checks). Can be batched but has no blocking dependency. | Replace the single call in `R/class-forecast-binary.R:68` with `test_names(colnames(forecast), disjunct.from = ...)`, delete the function definition in `R/check-input-helpers.R`, delete the test block, and remove the generated man page. Optionally bundle with #922. | [ ] | [ ] | — | [ ] |\n    62→| 826 | Update `plot_heatmap()` to be a table instead of a plot | | 2024-05-19 | enhancement | low | medium | partially | Still relevant. Accessibility concern from @Bisaloo is valid: current ggplot2 heatmap renders values as image text, inaccessible to screen readers. Conversation stalled — Bisaloo never replied to follow-up about output format (HTML/LaTeX). `kableExtra` is already in `Suggests`. This is a breaking change: function returns `ggplot` object, used with `+ facet_wrap()` in `inst/manuscript/`. Not used in any current vignette. Design decision needed on migration strategy. | None blocking. Thematically adjacent to #713 (visualization vignette). | Resolve stalled design conversation on output format and migration strategy (deprecate + new function vs. breaking change). Implement colored-table version using `kableExtra` (already in Suggests) or `gt` (new dep), update manuscript, replace test/snapshot, and deprecate or replace `plot_heatmap()`. | N/A | N/A | — | N/A |\n    63→| 825 | Package scaffolding | | 2024-05-19 | refactor | low | small | yes | Still relevant. Neither recommendation acted upon: (1) `Metrics` package still imported for 3 trivial functions (`ae`, `se`, `ape`) used only in `R/class-forecast-point.R` — these are one-line base-R replacements. `Metrics` did receive a 2025 CRAN update, reducing urgency. (2) `matrixStats` not adopted; 3 `apply()` calls in `R/metrics-sample.R` could benefit. Suggest splitting into two sub-tasks, prioritizing `Metrics` removal as a quick win. | None blocking. Removing `Metrics` requires updating vignettes (`scoring-rules.Rmd`) and tests (`test-metrics-binary.R`) that reference `Metrics::` functions. | (1) Replace `Metrics::ae/se/ape` with internal one-liners in `R/class-forecast-point.R`, remove from DESCRIPTION/NAMESPACE, update vignettes and tests. (2) Optionally evaluate adding `matrixStats` for 3 `apply()` calls — lower value, separate decision. | [ ] | [ ] | — | [ ] |\n    64→| 823 | Reduce number of `try()`, `suppressMessages()`, `suppressWarnings()` used in the package | | 2024-05-19 | refactor | medium | medium | no | Still relevant. 8 direct `try()` calls + 6 indirect via `check_try()` + 1 `suppressWarnings(suppressMessages(...))` in R/ code. Usage has slightly increased since filing due to new `[.forecast` subsetting operators (PR #884). Each `try()` exists for a reason; removal requires architectural rethinking of validation flow, not simple find-and-replace. | Deeply linked to #935 (print on column change), #840 (input check cleanup), #939 (remove `check_input_*`), #885 (export decisions), #811 (batch warnings). | Address incrementally: (1) Replace blanket `try()` in subsetting operators with targeted `tryCatch()`, (2) add `verbose` params to internal validation paths to eliminate `suppress*()`, (3) replace `check_try()` with proper `check_*()` functions as part of #840. | N/A | N/A | — | N/A |\n    65→| 811 | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? | enhancement | 2024-05-07 | enhancement | low | small | yes | Still relevant. `run_safely()` in `R/score.R:210` uses `try()` + immediate `cli_warn()` per failed metric. No batching/deferring mechanism exists. Failed metrics silently dropped from `scores` object via `as_scores()` filtering. Blog post on deferred errors referenced in issue. Only 1 comment from maintainer (\"I like this idea\"). | None blocking. Tangentially related to #823 (reduce `try()`) and #382 (metric/column name clash warnings). | Modify `apply_metrics()`/`run_safely()` to collect error messages in a list during the `lapply` loop, then emit a single batched `cli_warn()` summarizing all failed metrics at the end. Update test expectations in `test-score.R`. | [ ] | [ ] | — | [ ] |\n    66→| 810 | Clean up readme by using `pkgdown` to render dev version docs | | 2024-05-05 | documentation | low | small | yes | CLOSED (completed 2026-02-09). PR #1060 merged today, replacing R-based `packageVersion()` detection with pkgdown selective HTML (`.pkgdown-devel`/`.pkgdown-release` fenced divs). `_pkgdown.yml` already had `development: mode: auto`. Dev docs deployed at `/dev/` on gh-pages. | None. Fully resolved. | No action needed — issue closed via merged PR #1060. | N/A | N/A | — | N/A |\n    67→| 808 | Run lintr everywhere | | 2024-05-05 | cleanup | low | medium | partially | Active work in progress. PR #1062 (\"enable lintr for tests, inst, and vignettes\") is open but not yet merged. `.lintr` excludes all `tests/`, `inst/`, and `vignettes/*.R` files. CI only lints changed files on PRs. Main scope is `tests/` (~5,400 lines across 35 files). `inst/` has only 2 R files. `vignettes/*.R` not tracked in git (auto-generated tangles). ~90 `nolint` annotations already exist in `R/` source. | #807 (cli nolint cleanup) should be done first to establish pattern. #513 (vignette .R scripts) related to vignette exclusion. PR #1062 is in flight. | Merge or rebase PR #1062 once reviews pass. May need to fix remaining lint violations or add targeted `# nolint` annotations. Address `tests/` directory-wide lint cleanup separately from `inst/` and `vignettes/`. | N/A | N/A | — | N/A |\n    68→| 807 | Clean up linter statements around `cli` conditions | good first issue, implementation-ready | 2024-05-05 | cleanup | low | medium | yes | Still relevant. ~43 `#nolint start: keyword_quote_linter` blocks across R/ wrap `cli_abort()`/`cli_warn()` calls that use `\"!\" =`, `\"i\" =` named vector syntax. Only 12 instances use Hugo's backtick fix (`\\`!\\` =`). The fix is mechanical: replace `\"!\" =` with `` `!` = `` inside cli condition calls, then remove the `nolint` annotations. Additional `object_usage_linter` suppressions (14 blocks) exist for `.data[...]` patterns in data.table code — these need separate handling. No work done on this issue since filing. | Prerequisite for #808 (run lintr everywhere). Related to PR #583 (which introduced the cli pattern). | Replace all `\"!\" =`, `\"i\" =`, `\"x\" =`, `\"v\" =` with backtick-quoted equivalents (`` `!` = ``, etc.) in `cli_abort()`/`cli_warn()` calls across ~20 R/ files, then delete the corresponding `#nolint start`/`#nolint end` blocks. Handle `object_usage_linter` suppressions separately. | [ ] | [ ] | — | [ ] |\n    69→| 793 | How should we document methods that have different arguments than the default method? | | 2024-04-19 | documentation | low | small | yes | Still relevant. Consensus reached (separate pages per @seabbs), but not implemented. Three generics currently lump methods with differing arguments onto one `@rdname` page: `as_forecast_quantile` (`.forecast_sample` has different args), `as_forecast_point` (`.forecast_quantile`), and `get_pit_histogram` (`.forecast_sample` has extra args). Renders a confusing flat argument list. | None. | Remove shared `@rdname` tags for S3 methods whose arguments differ from the generic (`as_forecast_quantile.forecast_sample`, `as_forecast_point.forecast_quantile`, `get_pit_histogram.forecast_sample`) and give each its own documentation page with proper title and description. | [ ] | [ ] | — | [ ] |\n    70→| 789 | Add documentation about how to customise scores | documentation | 2024-04-13 | documentation | medium | medium | partially | Partially addressed. The `score()` roxygen docs now have a \"Customising metrics\" section with `purrr::partial()` examples (PR #874). But the core ask — a vignette walkthrough — remains undone. No vignette covers `get_metrics()` → `select_metrics()` → `purrr::partial()` → `score()` workflow. On `scoringutils-2.x` milestone. | Related to #546 (vignette restructuring — decides where customization docs go), #666 (manuscript update). | Create a vignette section (or standalone vignette) walking users through the full metric customization workflow: `get_metrics()`, `select_metrics()`, `purrr::partial()`, custom metric functions, and passing to `score()`. Coordinate with vignette restructuring in #546. | N/A | N/A | — | N/A |\n    71→| 785 | Small updates to the scoring rules Vignette | documentation | 2024-04-10 | documentation | low | small | yes | Still relevant. None of the 7 suggestions from @seabbs's review of PR #762 have been implemented: author field still present, Google Drive link still used, no citations for proper scoring rules or transformed scores paper, `get_metrics` wording not improved for website readers, no introductory paragraph on different probabilistic forecast types. Nearly 2 years open with no progress on these items. | Loosely related to #666 (manuscript update re: Google Drive link), #758 (scoring rules vignette parent issue), #1046 (multivariate scoring in same vignette). | Implement the 7 small documentation edits in `vignettes/scoring-rules.Rmd`: remove author field, replace Google Drive link with proper reference, add citations for proper scoring rules and transformed scores, improve `get_metrics` wording, clean whitespace, and add intro paragraph on probabilistic forecast types. | [ ] | [ ] | — | [ ] |\n    72→| 780 | Add a `verbose` argument to `as_forecast()` | | 2024-04-05 | enhancement | low | small | yes | Still relevant. `verbose` already exists on `assert_forecast()` and `assert_forecast_generic()` (PR #778) but has not been threaded through to any `as_forecast_<type>()` constructor. ~11 test lines and 1 internal R source line use `suppressMessages`/`suppressWarnings` wrappers around `as_forecast_*` calls, confirming ongoing need. Author acknowledged low urgency but wants it for test cleanup. | None. PR #778 (verbose on `assert_forecast`) already merged; plumbing is in place. | Add `verbose = TRUE` parameter to each `as_forecast_<type>.default()` method, pass it to their `assert_forecast()` calls, guard direct `cli_warn()`/`cli_inform()` calls behind the flag, and replace `suppressMessages`/`suppressWarnings` wrappers in tests and internal code with `verbose = FALSE`. | [ ] | [ ] | — | [ ] |\n    73→| 769 | Rethink the way arguments are passed in `get_pairwise_comparisons()` | package improvement | 2024-04-03 | refactor | low | small | yes | Still relevant. `test_type`, `one_sided`, and `n_permutations` are still passed via `...` through 3 levels (`get_pairwise_comparisons` → `pairwise_comparison_one_group` → `compare_forecasts`). Users must read internal `compare_forecasts` docs to discover options. PR #994 added `test_type = NULL` but didn't address the `...` pattern. #750 (split out p-values) was deferred. | Loosely related to #750 (if p-values split out, this becomes moot — but #750 deferred indefinitely). No hard blockers. | Promote `test_type`, `one_sided`, and `n_permutations` from `...` to explicit named arguments in `get_pairwise_comparisons()` and `add_relative_skill()`, improving API discoverability. Update roxygen `@param` tags accordingly. | [ ] | [ ] | — | [ ] |\n    74→| 763 | We should check for consistency when scoring binary forecasts | feature request, high-priority | 2024-03-30 | enhancement | medium | small | yes | Still relevant. No code warns when 0/1 factor levels are in counterintuitive order (e.g., `levels = c(1, 0)` making `0` the reference level). `assert_input_binary()` only validates factor with 2 levels and predicted in [0,1]. No helper for safe factor conversion exists. @seabbs questioned whether this is must-have for 2.0 or nice-to-have. | Loosely related to #840 (input check cleanup) and #939 (remove `check_input_*` functions). | Add a check in `assert_input_binary()` that warns when observed values are \"0\"/\"1\" and level order is `c(1, 0)`. Optionally create `as_binary_observed()` helper with a `reference_level` argument for safe factor conversion. | [ ] | [ ] | — | [ ] |\n    75→| 759 | Future of the current `scoringutils.Rmd` vignette | documentation, question | 2024-03-27 | documentation | medium | medium | partially | Partially addressed. The outdated `scoringutils.Rmd` was removed in PR #836 (May 2024). However, the promised replacement \"Get started\" vignette was never written — @seabbs volunteered June 2024 but no PR materialized. Package has no introductory vignette. README also has dead links to deleted vignettes (`metric-details`, `scoring-forecasts-directly`). | Related to #546 (simplify getting started). Completing this would likely close #546. Dead README links should be fixed regardless. | Write a new \"Get started\" / quick-start vignette to replace the deleted `scoringutils.Rmd`. Fix dead vignette links in README.Rmd. Consider merging #546 into this issue. | N/A | N/A | — | N/A |\n    76→| 758 | Create a vignette that explains scoring rules | documentation | 2024-03-27 | documentation | medium | large | partially | Partially addressed. Part 1 (explain each scoring rule) is covered by the existing `scoring-rules.Rmd` vignette. Part 2 (practical guidance on choosing a scoring rule — convergence, overconfidence, locality, scale sensitivity) remains unaddressed. The issue body contains a detailed vignette stub. Three illustration PNGs exist in `vignettes/` but are orphaned (not referenced by any vignette). | Parent issue for #929, #930, #933, #934 (illustration issues). Related to #785 (scoring rules vignette updates), #1046 (multivariate content). | Narrow scope to creating a \"Choosing a scoring rule\" vignette covering comparative guidance (convergence, overconfidence, locality, scale sensitivity). Coordinate with #929/#930/#933/#934 to incorporate the orphaned illustrations. | N/A | N/A | — | N/A |\n    77→| 752 | Split up `plot_pit()` into a method for vectors and one for data.frames | | 2024-03-27 | refactor | low | small | yes | Fully resolved. `plot_pit()` was removed entirely in PRs #940 and #949. Replaced by `get_pit_histogram()` (S3 generic with methods for `forecast_sample` and `forecast_quantile`) and `pit_histogram_sample()` for low-level computation. Plotting is now via standard ggplot2 calls. The dual-purpose if/else concern no longer exists. | None. | Close as completed. The issue's concern was superseded by a complete removal of `plot_pit()` in favor of `get_pit_histogram()` with proper S3 dispatch. | N/A | N/A | — | N/A |\n    78→| 750 | Move computation of p-values out of `get_pairwise_comparisons()`? | question | 2024-03-26 | discussion | low | medium | no | Partially addressed. PR #994 added `test_type = NULL` to skip p-value computation, resolving the main pain point (spurious warnings from `add_relative_skill()`). The broader architectural question of separating p-value from score ratio computation remains open but contributor consensus was option 1 (do nothing). | #769 (explicit test_type argument) should be considered together. #1022 (two-model case) tangentially related. | Consider closing as won't-fix or converting to long-term design discussion. The practical problem was solved by PR #994. If kept open, the most impactful remaining change would be defaulting `add_relative_skill()` to `test_type = NULL` since it discards p-values anyway. | N/A | N/A | — | N/A |\n    79→| 740 | Make `get_forecast_unit()` and friends customisable such that they can work with new forecast classes | enhancement | 2024-03-24 | refactor | medium | medium | partially | Still relevant. No implementation work done. `get_protected_columns()` remains a plain function with hard-coded column lists (not S3). `get_duplicate_forecasts()` still hard-codes type-specific columns. Related S3 conversions (e.g., `get_forecast_type`, `get_pit`, `as_forecast`) establish the pattern. On `scoringutils-2.x` milestone. | Near-duplicate of #889 (S3 `get_protected_columns`) and #888 (S3 `get_duplicate_forecasts`). Blocks #852 (custom forecast class workflow). | Convert `get_protected_columns()` into an S3 generic with per-class methods (addressing #889), similarly make `get_duplicate_forecasts()` S3 (#888), so each forecast class declares its own columns. Follow established pattern from completed S3 conversions. | N/A | N/A | — | N/A |\n    80→| 731 | Should `wis()` warn about missing values? | question | 2024-03-23 | discussion | low | small | no | Still relevant. Current behavior is correct — `as_forecast()` emits a vague NA message, `clean_forecast()` strips NA rows before `wis()` sees them. Discussion converged on improving the `as_forecast()` message (#537) as the primary fix, with `wis()`-specific warning being secondary. No consensus reached on implementation. | Related to #537 (high-priority: improve NA-removal message). Related to #542 (debug `score()`). Resolving #537 first would clarify what remains here. | Resolve #537 first (improve NA-removal message in `as_forecast()` to report affected row count). Then decide whether `wis()` needs its own warning for direct (non-`score()` pipeline) usage. May be closeable once #537 is done. | N/A | N/A | — | N/A |\n    81→| 718 | Testing fails locally when done repeatedly | enhancement | 2024-03-20 | bug | low | small | yes | Still relevant and unfixed. Root cause: `cli_warn()` with `.frequency = \"once\"` and `.frequency_id = \"small_interval_range\"` suppresses warnings after first test run in same R session. 4 instances of `.frequency = \"once\"` in codebase. Tests pass on CI (fresh process) and on first local run. | No blocking dependencies. Tangentially related to #935 (surprising output behavior). | Add `rlang::reset_message_verbosity(\"small_interval_range\")` before the `expect_warning()` call in `test-metrics-interval-range.R`. Audit the other 3 `.frequency = \"once\"` sites to ensure their tests are similarly resilient. | [ ] | [ ] | — | [ ] |\n    82→| 713 | Create vignette that showcases different ways to visualise forecasts and scores | documentation | 2024-03-11 | documentation | medium | medium | no | Still relevant. Package has 7 exported plot functions (`plot_wis`, `plot_heatmap`, `plot_interval_coverage`, `plot_quantile_coverage`, `plot_pairwise_comparisons`, `plot_forecast_counts`, `plot_correlations`) plus `theme_scoringutils()` but no dedicated visualization vignette. Only a deprecated-visualisations vignette exists. Issue requests updated `plot_score_table()` and mention of `hubVis`. | Related to #826 (plot_heatmap accessibility), #49 (calibration plotting), #942 (discrimination plots), #356 (interactions vignette). Does not block other issues. | Create a new `vignettes/visualising-scores.Rmd` with worked examples for all exported plot functions using example data. Include mention of `hubVis` as alternative for forecast visualization (per closed #812), and an updated `plot_score_table()` equivalent. | N/A | N/A | — | N/A |\n    83→| 702 | Update docs to conform to tidyverse styleguide | good first issue | 2024-03-08 | documentation | low | small | partially | Still relevant. ~85% of `@param` tags already conform to sentence case. ~21 clearly non-conforming (lowercase start) and ~21 special-character-starting descriptions across ~15 R files. Blocker PR #705 (file reorg) was merged March 2024. Assigned to @jamesmbaazam but no PR submitted. | No blocking dependencies. Standalone cleanup task. PR #705 dependency resolved. | Fix ~21 `@param` descriptions across ~15 R files to start with capital letter and end with full stop per tidyverse style guide. Also audit ~21 special-character-starting descriptions and all `@return`/`@returns` tags for same conformance. | N/A | N/A | — | N/A |\n    84→| 686 | Improve code coverage | | 2024-03-06 | meta | low | small | partially | Largely resolved. Coverage is at 98%. The specific sub-issue (#703, misnamed test file) was fixed in PR #704. Only `theme_scoringutils()` (18 lines) lacks any test coverage. The remaining 2% is edge-case branches. Could be closed as-is. | Sub-issue #703 resolved. No blockers. | Optionally add a test for `theme_scoringutils()` and a `.codecov.yml` with a coverage target to prevent regressions, then close this issue given the excellent 98% coverage level. | N/A | N/A | — | N/A |\n    85→| 685 | Suppress printing in tests | enhancement, implementation-ready | 2024-03-06 | cleanup | medium | medium | partially | Still relevant. 29 `suppressMessages()` and 16 `suppressWarnings()` remain across 13 test files, silently swallowing conditions instead of asserting them. Partially addressed: 80+ `expect_no_condition()` calls already exist. Root causes include `as_forecast_*()` verbose messaging and the #935 print bug. | Partially depends on #935 (unwanted printing on `:=`). Overlaps significantly with #823 (reduce `suppress*()` in package). | Audit all 29 `suppressMessages()` and 16 `suppressWarnings()` in tests: replace with `expect_message()`/`expect_warning()` for intentional conditions, pass `verbose = FALSE` where available, or use `expect_no_condition()` after fixing the source. Also review `setup.R` score calls. | N/A | N/A | — | N/A |\n    86→| 684 | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` | | 2024-03-06 | cleanup | low | small | yes | Still relevant but fully superseded by #939, which covers all 5 `check_input_*` functions. These 3 functions are dead code: never called in production, not exported, only tested. A comment in `check-input-helpers.R` explicitly marks them for future deletion. Should be closed as duplicate of #939. | Superseded by #939. Related to #840 (input check cleanup) and #823 (reduce `try()`). | Close as duplicate of #939. If handled independently, delete the 3 function definitions, their man pages, roxygen cross-references, and corresponding test blocks. | [ ] | [ ] | — | [ ] |\n    87→| 668 | Add argument to control what kind of coverage is computed by `get_coverage()` | feature request | 2024-02-29 | enhancement | low | small | yes | Still relevant and unimplemented. `get_coverage()` always computes all 4 coverage metrics with no user control. The issue body contains a near-complete code proposal (with a minor bug to fix). The function structure already cleanly separates interval and quantile computation paths. On `scoringutils-2.x` milestone. | No blocking dependencies. Standalone enhancement. | Add a `type = c(\"quantile\", \"interval\")` argument to `get_coverage()` that conditionally computes interval coverage, quantile coverage, or both. Update downstream column handling, summarisation, and tests. | [ ] | [ ] | — | [ ] |\n    88→| 667 | Create print method for `scores()` objects | feature request | 2024-02-29 | feature | low | small | partially | Still relevant but effectively blocked by #935. No `print.scores` method exists; scores objects fall through to `print.data.table`. Adding a custom print method would exacerbate the unwanted-printing-on-`:=` bug documented in #935. Both maintainers expressed lukewarm interest, deferring to post-release. | Blocked by #935 (print on column change bug). Related to #975 (link to score definitions in print). | Implement `print.scores()` S3 method displaying computed metric names (from `metrics` attribute) before delegating to `NextMethod()`, following the `print.forecast` pattern. Wait until #935 is resolved first to avoid spurious printing on `:=` operations. | N/A | N/A | — | N/A |\n    89→| 666 | Update the scoringutils paper that accompanies the package | question | 2024-02-29 | documentation | medium | large | no | Still relevant but partially stale. Only 2 of ~15 checklist items done. Manuscript in `inst/manuscript/` still references `magrittr`, CITATION still points to 2022 arXiv preprint. README links to Google Drive PDF. Some checklist items reference functions no longer exported. Should be broken into smaller actionable issues. | Related to #759, #789, #546, #758, #1057. | Audit the to-do list against current API to remove stale items, break remaining unchecked items into individual issues, and decide whether to prioritize the \"make a vignette out of the paper\" item before the next CRAN release. Requires human authorship decisions. | N/A | N/A | — | N/A |\n    90→| 646 | Stop converting errors to warnings in `score()`? | help wanted, question, Breaking change | 2024-02-23 | refactor | medium | medium | partially | Still relevant. `run_safely()` in `R/score.R` still converts all metric errors to warnings via `try()` + `cli_warn()` and returns `NULL`, silently dropping failed metrics. Deferred to `scoringutils-2.x` milestone. No consensus on approach: let errors propagate (breaking), add `on_error` parameter (compatible), or batch warnings (#811). | Related to #811 (batch warnings), #941 (`score()` redesign), #823 (reduce `try()`). | Design decision needed: (a) let errors propagate as errors, (b) add `on_error = c(\"warn\", \"error\")` param to `score()`, or (c) combine with #811's batch-warning approach. Implementation touches `run_safely()` in `R/score.R` and test expectations. | N/A | N/A | — | N/A |\n    91→| 625 | Implement a `get_unique_values()` function | implementation-ready | 2024-02-10 | enhancement | low | small | yes | Still relevant. No implementation work done. Function does not exist. Would compute `length(unique(values))` per forecast-unit column (via `get_forecast_unit()`), optionally grouped by model. Similar diagnostic purpose to `get_forecast_counts()` but different output. | Loosely related to #581 (NA in print), #935 (print bug), #621 (closed, prettier print). No hard blockers for standalone function. | Implement a new exported `get_unique_values()` function that takes a forecast object, calls `get_forecast_unit()` to identify relevant columns, and returns a summary table of unique value counts per column. Optionally integrate into `print.forecast()`. | [ ] | [ ] | — | [ ] |\n    92→| 608 | Define input format for categorical forecasts | implementation-ready | 2024-01-17 | discussion | low | small | no | Largely resolved. Input format fully defined and implemented in `forecast_nominal` (PR #837) and `forecast_ordinal` (PR #977). Column named `predicted_label`. Strict completeness enforced. Only unimplemented item is a helper function to auto-expand incomplete forecasts with zero-probability rows, which has no dedicated tracking issue. Can be closed. | #604 (parent, closed), #607 (naming, closed). No blockers. | Close this issue as completed. Optionally open a new targeted issue for the helper function that auto-expands incomplete forecasts with zero-probability rows if still desired. | N/A | N/A | — | N/A |\n    93→| 581 | Add the number of `NA` values to the planned print method for `forecast_*` objects, rather than throw a message? | feature request | 2024-01-05 | enhancement | medium | medium | partially | Still relevant. None of the three requested features implemented: `print.forecast()` shows no NA info, NA message in `assert_forecast_generic()` is still vague (no row count), no `get_NA()` helper exists. Issue #537 is a near-duplicate and both should be addressed together. | Near-duplicate of #537 (should be resolved together). Tangentially related to #731, #542, #935 (print bug). | Enhance `print.forecast()` to display NA row count, improve the message in `assert_forecast_generic()` to report exact number of NA rows, and consider adding a `get_na_summary()` helper. Consolidate with #537. | N/A | N/A | — | N/A |\n    94→| 557 | sample_to_quantile.numeric method | low-priority | 2024-01-05 | enhancement | low | small | yes | Effectively resolved. `sample_to_quantile()` was replaced by `as_forecast_quantile.forecast_sample()` in PR #876. The motivating code (`interval_coverage_sample()`) was deleted entirely. No current code needs a `.numeric` method. Can be closed. | Tangentially related to #1026 (reverse conversion). No blockers. | Close this issue as resolved. The refactoring in PR #876 and deletion of `interval_coverage_sample()` eliminated the code pattern that motivated this request. | N/A | N/A | — | N/A |\n    95→| 546 | Simplify \"Getting Started\" \\| Split into smaller chunks | documentation | 2024-01-05 | documentation | medium | medium | no | Still relevant. The old `scoringutils.Rmd` was deleted (PR #836) but the promised replacement \"Get started\" vignette was never written. Package currently has no introductory vignette at all. README has a \"Quick start\" section but no pkgdown \"Get started\" page. @seabbs volunteered in #759 but no PR materialized. | Tightly coupled with #759 (future of scoringutils.Rmd) — should be resolved together. Related to #789 (metric customization docs). | Write a new introductory \"Getting Started\" vignette (as `vignettes/scoringutils.Rmd` for pkgdown) providing a concise end-to-end walkthrough. Can close both #546 and #759 simultaneously. Requires editorial decisions on scope and content. | N/A | N/A | — | N/A |\n    96→| 542 | Improve ways to debug `score()` | question | 2024-01-04 | enhancement | medium | medium | partially | Still relevant. Neither action item is done: (1) no vignette documents `debugonce()` for scoring rule debugging, (2) `run_safely()` still only reports the metric name in warnings, not which forecast unit/row caused the failure. The `{cli}` migration (PR #583) improved formatting but not informativeness. | Closely related to #646 (stop converting errors to warnings — open, breaking change). Related to closed #391, #418, #543, #583. | Two tasks remain: (1) add documentation showing `debugonce()` usage for debugging scoring failures, and (2) enhance `run_safely()`/`apply_metrics()` to include forecast-unit context in warning messages. Coordinate error-vs-warning design with #646. | N/A | N/A | — | N/A |\n    97→| 537 | Inform users about the number of rows that have NA values | good first issue, question, implementation-ready, high-priority | 2024-01-03 | enhancement | medium | medium | partially | Still relevant. The message in `assert_forecast_generic()` (line 150 of `R/class-forecast.R`) still says the generic \"Some rows containing NA values may be removed\" without a row count. No NA info in `print.forecast`, no `get_na()` helper. The `anyNA()` check and `na.omit()` call are already in place, so computing the count is trivial. | Near-duplicate of #581 (NA count in print method) — should be resolved together. Related to #731 (wis NA warnings). | Minimal fix: add NA row count to the existing `cli_inform` message in `assert_forecast_generic()`. For a more complete resolution, combine with #581 to also add NA summary to `print.forecast` and potentially create a `get_na()` diagnostic helper. | N/A | N/A | — | N/A |\n    98→| 522 | Create a way to test user-provided scoring functions | feature request | 2023-12-06 | feature | medium | medium | partially | Still relevant. No progress made. `validate_metrics()` only checks that items are functions (via `checkmate::check_function`), not signature compatibility, output shape, or runtime behavior. Custom metric errors are silently converted to warnings by `run_safely()`, making debugging hard. Each of the 6+ forecast types passes different arguments to metrics. | Loosely related to #362 (non-standard metric I/O), #823 (reduce `try()`). | Implement an exported `test_metrics()` function that runs each metric on a small representative subset of a forecast object and reports informative diagnostics: signature compatibility, runtime errors, and output shape validation. Needs design decision on API. | N/A | N/A | — | N/A |\n    99→| 520 | Create documentation/vignette for the naming scheme of our functions | help wanted, question | 2023-11-30 | documentation | low | medium | partially | Still relevant. The issue body already describes the naming scheme in detail but no vignette or formal documentation exists. CONTRIBUTING.md only references tidyverse style guide. Issue #948 (closed) debated `get_` prefix but decided to keep it. 65 exported functions follow conventions consistently. ~2 years old with no implementation. | Related to closed #948, #610, #476, #357. No blockers. | Create a developer-facing vignette or CONTRIBUTING.md section formalizing the naming conventions (scoring rules: `<name>_<type>`, `get_*`, `as_forecast_*`, `is_forecast_*`, `plot_*`, `add_*`, `assert_`/`check_`/`test_`). Update to reflect current classes including `forecast_nominal`, `forecast_ordinal`, `forecast_sample_multivariate`. | N/A | N/A | — | N/A |\n   100→| 513 | Investigate R scripts that get generated based on our vignettes | | 2023-11-28 | cleanup | low | small | yes | Already resolved. The `.R` and `.html` vignette artifacts (generated by `knitr` tangle) are gitignored via `vignettes/.gitignore`, excluded from linting via `.lintr`, and not tracked in git. All three mitigations from the discussion are in place. Issue can be closed. | None. | Close this issue — all requested investigation and mitigations (gitignore, buildignore, lint-ignore for vignette-generated `.R` files) are already implemented. No code changes needed. | N/A | N/A | — | N/A |\n   101→| 492 | Expose `get_range_from_quantile()` to the user | question | 2023-11-22 | enhancement | low | small | yes | Still relevant. Function exists at `R/helper-quantile-interval-range.R:185` as `@keywords internal`, not exported. Maintainer consensus reached: rename to `get_interval_range()` and create `add_interval_range()` wrapper. Neither implemented. Original motivating function `plot_ranges()` was deleted, reducing urgency. 3 internal call sites. | Parent meta-issue #484. Tangentially related to #885 (export decisions). | Rename `get_range_from_quantile()` to `get_interval_range()`, export it, and create a new exported `add_interval_range()` that adds an `interval_range` column to a quantile-format data.table. Update 3 internal call sites, NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n   102→| 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |\n   103→| 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |\n   104→| 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |\n   105→| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |\n   106→| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n   107→| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n   108→| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n   109→| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n   110→| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n   111→| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n   112→| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n   113→| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n   114→| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n   115→| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n   116→| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n   117→| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n   118→| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n   119→| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n   120→| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n   121→| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n   122→| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n   123→| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n   124→| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n   125→| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n   126→| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n   127→| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n   128→| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n   129→| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n   130→\n   131→TRIAGE_COMPLETE"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8862b2a6-a382-4bb5-bd8f-d7927de84c31","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","content":"# scoringutils Issue Triage\n\n> Auto-generated by Ralph triage loop. Do not edit manually during the loop.\n\n## Summary\n\n| Metric | Count |\n|--------|-------|\n| Total open issues | 106 |\n| Triaged | 106 |\n| Remaining | 0 |\n\n## Pipeline\n\n- [ ] **Test design** — Test specs written for automatable issues\n- [ ] **Implement** — Fixes implemented, PRs created\n- [ ] **Review** — All PRs independently reviewed\n- [ ] **Summary** — Human summary generated\n\n## Issues\n\n| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n|---|-------|--------|--------|----------|----------|------------|-------------|-------------|--------------|----------------|-----------|-------------|-----|----------|\n| 1065 | Release reminder - February 2026 | | 2026-02-02 | meta | medium | medium | partially | Automated reminder. Dev version (2.1.2.9000) has multivariate feature + bug fix accumulated over 161 days. No blocking bugs. Two open PRs (#1060, #1062) are infrastructure changes. | Maintainers should decide whether to merge #1060/#1062 before releasing. | Decide whether to cut a new release (likely v2.2.0 given multivariate feature). If yes, run `usethis::use_release_issue()` and proceed with CRAN submission. If not, close the issue. | N/A | N/A | — | N/A |\n| 1064 | Pre-ranking for multivariate calibration assessment | | 2026-01-30 | feature | medium | large | no | No pre-ranking or multivariate calibration diagnostics exist in the codebase. The `forecast_sample_multivariate` class and energy score are implemented and stable. Design decision needed: integrate into `score()` (Option A) or standalone `get_multivariate_ranks()` function (Option B). MST method may need `igraph` dependency. | None blocking. Builds on existing `forecast_sample_multivariate` infrastructure. | Implement pre-ranking methods (multivariate rank, optionally MST rank) from Gneiting et al. (2008) as calibration diagnostics for multivariate forecasts, after resolving the API design choice with maintainers. | N/A | N/A | — | N/A |\n| 1057 | Revise README to remove news about the package rewrite | documentation | 2026-01-29 | documentation | low | medium | partially | Open, no assignee. The rewrite messaging and Google Drive paper links appear in 4 locations in README.Rmd. Mechanical edits are straightforward but replacement text requires editorial decisions about where to point users for documentation. | Loosely related to #810 (pkgdown for dev docs) and #759 (future of scoringutils.Rmd vignette). | Edit `README.Rmd` to remove/soften the \"major re-write\" paragraph and replace Google Drive paper links with pointers to package vignettes/docs, then re-render README.md. Confirm preferred documentation hierarchy with maintainers first. | N/A | N/A | — | N/A |\n| 1055 | Model confidence sets | feature request | 2026-01-19 | feature | medium | large | no | Design phase — methodology not yet agreed upon. 6 comments with active debate. Naive pairwise p-value approach acknowledged as incomplete. Three alternatives discussed: Hansen et al. MCS, novel extension for incomplete sets, Bayesian approach. No consensus reached. | Relates to #750 (p-value computation refactor), #769 (pairwise comparison arguments), #37 (Diebold-Mariano test), #1022 (two-model pairwise). | Resolve design consensus on which MCS methodology to implement (Hansen et al. on complete sets, novel incomplete-set extension, or Bayesian), then implement `get_model_confidence_set()` building on existing pairwise comparison infrastructure. | N/A | N/A | — | N/A |\n| 1046 | multivariate scoring in scoring rules vignette | documentation | 2025-11-04 | documentation | low | small | partially | Open, agreed upon by maintainers. The scoring-rules vignette has zero mentions of multivariate scoring. A dedicated multivariate vignette already exists. Only a cross-reference and brief section are missing. | None. All underlying code and the dedicated multivariate vignette already exist. | Add a short section to `vignettes/scoring-rules.Rmd` describing the energy score for multivariate forecasts and cross-linking to the existing `scoring-multivariate-forecasts` vignette. No code changes needed. | N/A | N/A | — | N/A |\n| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |\n| 1043 | Update print method for multivariate forecasts to show `joint_across` | | 2025-09-21 | enhancement | low | small | yes | Still relevant. No `print.forecast_sample_multivariate` method exists; the generic `print.forecast` handles all types. `joint_across` is not stored as an attribute — `set_grouping()` computes `.mv_group_id` but discards the original `joint_across` value. | Builds on multivariate infrastructure from merged PR #998. | Add a `print.forecast_sample_multivariate()` method that displays `joint_across` columns. Requires either storing `joint_across` as an attribute during object construction or computing it as the set difference between forecast unit and `get_grouping()` output. Update test snapshots. | [x] | [ ] | — | [ ] |\n| 1026 | as_forecast_sample.forecast_quantile | enhancement | 2025-07-13 | enhancement | medium | medium | partially | Still relevant. The reverse conversion (`as_forecast_quantile.forecast_sample`) exists, but `as_forecast_sample.forecast_quantile` does not. No distribution-fitting dependencies in DESCRIPTION. Author plans to borrow from hubverse approach. | Related to #557 (sample_to_quantile.numeric). May introduce new dependency (e.g., `distfromq`). | Implement `as_forecast_sample.forecast_quantile()` S3 method that fits a distribution or uses interpolation to approximate the predictive CDF from quantiles and draws samples. Requires design decision on approximation method and potential new package dependency. | N/A | N/A | — | N/A |\n| 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |\n| 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |\n| 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |\n| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [ ] | [ ] | — | [ ] |\n| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [ ] | [ ] | — | [ ] |\n| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [ ] | [ ] | — | [ ] |\n| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [ ] | [ ] | — | [ ] |\n| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n| 880 | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()? | | 2024-07-30 | enhancement | low | small | yes | Still relevant. Function hard-codes `0.5` on two lines (107, 115) in `R/class-forecast-quantile.R`. Design decisions already resolved in PR #876 discussion: parameter name `quantile_level`, default `0.5`, passed via `...`. No work done since issue opened. | None. Self-contained enhancement. | Add a `quantile_level` parameter (default `0.5`) to `as_forecast_point.forecast_quantile()`, replacing the two hard-coded `0.5` references, then add tests and documentation. | [ ] | [ ] | — | [ ] |\n| 872 | Add a reference for CRPS decomposition approach. | | 2024-07-25 | documentation | low | small | partially | Still relevant. `crps_sample()` in `R/metrics-sample.R` has only a `scoringRules` package reference, not one for the decomposition approach. `wis()` already properly cites Bracher et al. 2021. Minor decision needed on which paper(s) to cite: Bracher et al. 2021 and/or Arnold et al. 2023. | Related to #870 (CRPS/WIS decomposition refactor). Could be addressed in same PR. | Add `@references` entries to `crps_sample()` roxygen block citing paper(s) formalizing the overprediction/underprediction/dispersion decomposition (likely Bracher et al. 2021 and/or Arnold et al. 2023). | N/A | N/A | — | N/A |\n| 870 | Refactor CRPS and WIS decomposition | refactor | 2024-07-22 | refactor | low | medium | partially | Partially addressed. PR #985 (Feb 2025) simplified `crps_sample()` decomposition internally, but the structural duplication remains: `crps_sample()`, `wis()`, and `interval_score()` each independently implement `separate_results` branching, and 6 wrapper functions (3 sample + 3 quantile) repeat identical extraction patterns. | Related to #872 (CRPS decomposition reference). Filed during PR #854 review. On `scoringutils-2.x` milestone. | Extract the decomposition logic (`separate_results` branching and wrapper function boilerplate) into a shared internal helper or factory pattern so `crps_sample()`, `wis()`, and `interval_score()` delegate to the same mechanism instead of each implementing it independently. | N/A | N/A | — | N/A |\n| 852 | Design workflow for creating new forecast classes | enhancement | 2024-07-10 | enhancement | high | large | no | Still relevant. The building blocks exist (`new_forecast()` exported, `as_forecast_generic()` internal, pattern visible in 7 class files) but no documentation, vignette, or guide for third-party class creation exists. Three prerequisite S3 refactors (#889, #888, #740) remain open and block clean extensibility because `get_protected_columns()` and `get_duplicate_forecasts()` hard-code column names, preventing new classes from working seamlessly. `as_forecast_generic()`, `apply_metrics()`, and `as_scores()` are still unexported (#885). | #889 (S3 `get_protected_columns`), #888 (S3 `get_duplicate_forecasts`), #740 (`get_forecast_unit` customization), #885 (export decision for helper functions). | Complete prerequisite S3 refactors (#889, #888), decide export list for class-creation helpers (#885), then write a vignette or article documenting the step-by-step workflow for creating a custom `forecast_*` class (define `as_forecast_<type>`, `assert_forecast.<type>`, `score.<type>`, `get_metrics.<type>`, `get_protected_columns.<type>`, `get_duplicate_forecasts.<type>`). | N/A | N/A | — | N/A |\n| 840 | Clean up input checks | | 2024-06-07 | refactor | low | medium | partially | Still relevant. Input checks are applied inconsistently: `assert_forecast.forecast_binary` and `assert_forecast.forecast_point` call `check_input_*` wrappers (double-validating with scoring functions), while sample/quantile/ordinal/nominal types skip this. `check-input-helpers.R` has a comment marking `check_try()` and wrappers for future deletion. Author leans toward pushing all checks to the scoring-function level. No work done. | Related to #939 (remove `check_input_*` functions), #839, #922. On `scoringutils-2.x` milestone. | Decide where input validation should live (scoring-function level vs. `assert_forecast` level vs. both-consistently). Then either remove `check_input_binary`/`check_input_point` calls from `assert_forecast` methods and delete wrappers + `check_try()`, or add missing checks to sample/quantile `assert_forecast` methods for consistency. | N/A | N/A | — | N/A |\n| 839 | Remove `test_columns_not_present` | | 2024-06-07 | cleanup | low | small | yes | Still relevant. `test_columns_not_present()` is a trivial 1-line wrapper around `checkmate::test_names(..., disjunct.from = ...)` with exactly 1 production call site (`R/class-forecast-binary.R:68`) and 1 test block. `checkmate::test_names` is already imported. Straightforward removal. | Related to #922 (remove `check_columns_present`) and #840 (clean up input checks). Can be batched but has no blocking dependency. | Replace the single call in `R/class-forecast-binary.R:68` with `test_names(colnames(forecast), disjunct.from = ...)`, delete the function definition in `R/check-input-helpers.R`, delete the test block, and remove the generated man page. Optionally bundle with #922. | [ ] | [ ] | — | [ ] |\n| 826 | Update `plot_heatmap()` to be a table instead of a plot | | 2024-05-19 | enhancement | low | medium | partially | Still relevant. Accessibility concern from @Bisaloo is valid: current ggplot2 heatmap renders values as image text, inaccessible to screen readers. Conversation stalled — Bisaloo never replied to follow-up about output format (HTML/LaTeX). `kableExtra` is already in `Suggests`. This is a breaking change: function returns `ggplot` object, used with `+ facet_wrap()` in `inst/manuscript/`. Not used in any current vignette. Design decision needed on migration strategy. | None blocking. Thematically adjacent to #713 (visualization vignette). | Resolve stalled design conversation on output format and migration strategy (deprecate + new function vs. breaking change). Implement colored-table version using `kableExtra` (already in Suggests) or `gt` (new dep), update manuscript, replace test/snapshot, and deprecate or replace `plot_heatmap()`. | N/A | N/A | — | N/A |\n| 825 | Package scaffolding | | 2024-05-19 | refactor | low | small | yes | Still relevant. Neither recommendation acted upon: (1) `Metrics` package still imported for 3 trivial functions (`ae`, `se`, `ape`) used only in `R/class-forecast-point.R` — these are one-line base-R replacements. `Metrics` did receive a 2025 CRAN update, reducing urgency. (2) `matrixStats` not adopted; 3 `apply()` calls in `R/metrics-sample.R` could benefit. Suggest splitting into two sub-tasks, prioritizing `Metrics` removal as a quick win. | None blocking. Removing `Metrics` requires updating vignettes (`scoring-rules.Rmd`) and tests (`test-metrics-binary.R`) that reference `Metrics::` functions. | (1) Replace `Metrics::ae/se/ape` with internal one-liners in `R/class-forecast-point.R`, remove from DESCRIPTION/NAMESPACE, update vignettes and tests. (2) Optionally evaluate adding `matrixStats` for 3 `apply()` calls — lower value, separate decision. | [ ] | [ ] | — | [ ] |\n| 823 | Reduce number of `try()`, `suppressMessages()`, `suppressWarnings()` used in the package | | 2024-05-19 | refactor | medium | medium | no | Still relevant. 8 direct `try()` calls + 6 indirect via `check_try()` + 1 `suppressWarnings(suppressMessages(...))` in R/ code. Usage has slightly increased since filing due to new `[.forecast` subsetting operators (PR #884). Each `try()` exists for a reason; removal requires architectural rethinking of validation flow, not simple find-and-replace. | Deeply linked to #935 (print on column change), #840 (input check cleanup), #939 (remove `check_input_*`), #885 (export decisions), #811 (batch warnings). | Address incrementally: (1) Replace blanket `try()` in subsetting operators with targeted `tryCatch()`, (2) add `verbose` params to internal validation paths to eliminate `suppress*()`, (3) replace `check_try()` with proper `check_*()` functions as part of #840. | N/A | N/A | — | N/A |\n| 811 | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? | enhancement | 2024-05-07 | enhancement | low | small | yes | Still relevant. `run_safely()` in `R/score.R:210` uses `try()` + immediate `cli_warn()` per failed metric. No batching/deferring mechanism exists. Failed metrics silently dropped from `scores` object via `as_scores()` filtering. Blog post on deferred errors referenced in issue. Only 1 comment from maintainer (\"I like this idea\"). | None blocking. Tangentially related to #823 (reduce `try()`) and #382 (metric/column name clash warnings). | Modify `apply_metrics()`/`run_safely()` to collect error messages in a list during the `lapply` loop, then emit a single batched `cli_warn()` summarizing all failed metrics at the end. Update test expectations in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 810 | Clean up readme by using `pkgdown` to render dev version docs | | 2024-05-05 | documentation | low | small | yes | CLOSED (completed 2026-02-09). PR #1060 merged today, replacing R-based `packageVersion()` detection with pkgdown selective HTML (`.pkgdown-devel`/`.pkgdown-release` fenced divs). `_pkgdown.yml` already had `development: mode: auto`. Dev docs deployed at `/dev/` on gh-pages. | None. Fully resolved. | No action needed — issue closed via merged PR #1060. | N/A | N/A | — | N/A |\n| 808 | Run lintr everywhere | | 2024-05-05 | cleanup | low | medium | partially | Active work in progress. PR #1062 (\"enable lintr for tests, inst, and vignettes\") is open but not yet merged. `.lintr` excludes all `tests/`, `inst/`, and `vignettes/*.R` files. CI only lints changed files on PRs. Main scope is `tests/` (~5,400 lines across 35 files). `inst/` has only 2 R files. `vignettes/*.R` not tracked in git (auto-generated tangles). ~90 `nolint` annotations already exist in `R/` source. | #807 (cli nolint cleanup) should be done first to establish pattern. #513 (vignette .R scripts) related to vignette exclusion. PR #1062 is in flight. | Merge or rebase PR #1062 once reviews pass. May need to fix remaining lint violations or add targeted `# nolint` annotations. Address `tests/` directory-wide lint cleanup separately from `inst/` and `vignettes/`. | N/A | N/A | — | N/A |\n| 807 | Clean up linter statements around `cli` conditions | good first issue, implementation-ready | 2024-05-05 | cleanup | low | medium | yes | Still relevant. ~43 `#nolint start: keyword_quote_linter` blocks across R/ wrap `cli_abort()`/`cli_warn()` calls that use `\"!\" =`, `\"i\" =` named vector syntax. Only 12 instances use Hugo's backtick fix (`\\`!\\` =`). The fix is mechanical: replace `\"!\" =` with `` `!` = `` inside cli condition calls, then remove the `nolint` annotations. Additional `object_usage_linter` suppressions (14 blocks) exist for `.data[...]` patterns in data.table code — these need separate handling. No work done on this issue since filing. | Prerequisite for #808 (run lintr everywhere). Related to PR #583 (which introduced the cli pattern). | Replace all `\"!\" =`, `\"i\" =`, `\"x\" =`, `\"v\" =` with backtick-quoted equivalents (`` `!` = ``, etc.) in `cli_abort()`/`cli_warn()` calls across ~20 R/ files, then delete the corresponding `#nolint start`/`#nolint end` blocks. Handle `object_usage_linter` suppressions separately. | [ ] | [ ] | — | [ ] |\n| 793 | How should we document methods that have different arguments than the default method? | | 2024-04-19 | documentation | low | small | yes | Still relevant. Consensus reached (separate pages per @seabbs), but not implemented. Three generics currently lump methods with differing arguments onto one `@rdname` page: `as_forecast_quantile` (`.forecast_sample` has different args), `as_forecast_point` (`.forecast_quantile`), and `get_pit_histogram` (`.forecast_sample` has extra args). Renders a confusing flat argument list. | None. | Remove shared `@rdname` tags for S3 methods whose arguments differ from the generic (`as_forecast_quantile.forecast_sample`, `as_forecast_point.forecast_quantile`, `get_pit_histogram.forecast_sample`) and give each its own documentation page with proper title and description. | [ ] | [ ] | — | [ ] |\n| 789 | Add documentation about how to customise scores | documentation | 2024-04-13 | documentation | medium | medium | partially | Partially addressed. The `score()` roxygen docs now have a \"Customising metrics\" section with `purrr::partial()` examples (PR #874). But the core ask — a vignette walkthrough — remains undone. No vignette covers `get_metrics()` → `select_metrics()` → `purrr::partial()` → `score()` workflow. On `scoringutils-2.x` milestone. | Related to #546 (vignette restructuring — decides where customization docs go), #666 (manuscript update). | Create a vignette section (or standalone vignette) walking users through the full metric customization workflow: `get_metrics()`, `select_metrics()`, `purrr::partial()`, custom metric functions, and passing to `score()`. Coordinate with vignette restructuring in #546. | N/A | N/A | — | N/A |\n| 785 | Small updates to the scoring rules Vignette | documentation | 2024-04-10 | documentation | low | small | yes | Still relevant. None of the 7 suggestions from @seabbs's review of PR #762 have been implemented: author field still present, Google Drive link still used, no citations for proper scoring rules or transformed scores paper, `get_metrics` wording not improved for website readers, no introductory paragraph on different probabilistic forecast types. Nearly 2 years open with no progress on these items. | Loosely related to #666 (manuscript update re: Google Drive link), #758 (scoring rules vignette parent issue), #1046 (multivariate scoring in same vignette). | Implement the 7 small documentation edits in `vignettes/scoring-rules.Rmd`: remove author field, replace Google Drive link with proper reference, add citations for proper scoring rules and transformed scores, improve `get_metrics` wording, clean whitespace, and add intro paragraph on probabilistic forecast types. | [ ] | [ ] | — | [ ] |\n| 780 | Add a `verbose` argument to `as_forecast()` | | 2024-04-05 | enhancement | low | small | yes | Still relevant. `verbose` already exists on `assert_forecast()` and `assert_forecast_generic()` (PR #778) but has not been threaded through to any `as_forecast_<type>()` constructor. ~11 test lines and 1 internal R source line use `suppressMessages`/`suppressWarnings` wrappers around `as_forecast_*` calls, confirming ongoing need. Author acknowledged low urgency but wants it for test cleanup. | None. PR #778 (verbose on `assert_forecast`) already merged; plumbing is in place. | Add `verbose = TRUE` parameter to each `as_forecast_<type>.default()` method, pass it to their `assert_forecast()` calls, guard direct `cli_warn()`/`cli_inform()` calls behind the flag, and replace `suppressMessages`/`suppressWarnings` wrappers in tests and internal code with `verbose = FALSE`. | [ ] | [ ] | — | [ ] |\n| 769 | Rethink the way arguments are passed in `get_pairwise_comparisons()` | package improvement | 2024-04-03 | refactor | low | small | yes | Still relevant. `test_type`, `one_sided`, and `n_permutations` are still passed via `...` through 3 levels (`get_pairwise_comparisons` → `pairwise_comparison_one_group` → `compare_forecasts`). Users must read internal `compare_forecasts` docs to discover options. PR #994 added `test_type = NULL` but didn't address the `...` pattern. #750 (split out p-values) was deferred. | Loosely related to #750 (if p-values split out, this becomes moot — but #750 deferred indefinitely). No hard blockers. | Promote `test_type`, `one_sided`, and `n_permutations` from `...` to explicit named arguments in `get_pairwise_comparisons()` and `add_relative_skill()`, improving API discoverability. Update roxygen `@param` tags accordingly. | [ ] | [ ] | — | [ ] |\n| 763 | We should check for consistency when scoring binary forecasts | feature request, high-priority | 2024-03-30 | enhancement | medium | small | yes | Still relevant. No code warns when 0/1 factor levels are in counterintuitive order (e.g., `levels = c(1, 0)` making `0` the reference level). `assert_input_binary()` only validates factor with 2 levels and predicted in [0,1]. No helper for safe factor conversion exists. @seabbs questioned whether this is must-have for 2.0 or nice-to-have. | Loosely related to #840 (input check cleanup) and #939 (remove `check_input_*` functions). | Add a check in `assert_input_binary()` that warns when observed values are \"0\"/\"1\" and level order is `c(1, 0)`. Optionally create `as_binary_observed()` helper with a `reference_level` argument for safe factor conversion. | [ ] | [ ] | — | [ ] |\n| 759 | Future of the current `scoringutils.Rmd` vignette | documentation, question | 2024-03-27 | documentation | medium | medium | partially | Partially addressed. The outdated `scoringutils.Rmd` was removed in PR #836 (May 2024). However, the promised replacement \"Get started\" vignette was never written — @seabbs volunteered June 2024 but no PR materialized. Package has no introductory vignette. README also has dead links to deleted vignettes (`metric-details`, `scoring-forecasts-directly`). | Related to #546 (simplify getting started). Completing this would likely close #546. Dead README links should be fixed regardless. | Write a new \"Get started\" / quick-start vignette to replace the deleted `scoringutils.Rmd`. Fix dead vignette links in README.Rmd. Consider merging #546 into this issue. | N/A | N/A | — | N/A |\n| 758 | Create a vignette that explains scoring rules | documentation | 2024-03-27 | documentation | medium | large | partially | Partially addressed. Part 1 (explain each scoring rule) is covered by the existing `scoring-rules.Rmd` vignette. Part 2 (practical guidance on choosing a scoring rule — convergence, overconfidence, locality, scale sensitivity) remains unaddressed. The issue body contains a detailed vignette stub. Three illustration PNGs exist in `vignettes/` but are orphaned (not referenced by any vignette). | Parent issue for #929, #930, #933, #934 (illustration issues). Related to #785 (scoring rules vignette updates), #1046 (multivariate content). | Narrow scope to creating a \"Choosing a scoring rule\" vignette covering comparative guidance (convergence, overconfidence, locality, scale sensitivity). Coordinate with #929/#930/#933/#934 to incorporate the orphaned illustrations. | N/A | N/A | — | N/A |\n| 752 | Split up `plot_pit()` into a method for vectors and one for data.frames | | 2024-03-27 | refactor | low | small | yes | Fully resolved. `plot_pit()` was removed entirely in PRs #940 and #949. Replaced by `get_pit_histogram()` (S3 generic with methods for `forecast_sample` and `forecast_quantile`) and `pit_histogram_sample()` for low-level computation. Plotting is now via standard ggplot2 calls. The dual-purpose if/else concern no longer exists. | None. | Close as completed. The issue's concern was superseded by a complete removal of `plot_pit()` in favor of `get_pit_histogram()` with proper S3 dispatch. | N/A | N/A | — | N/A |\n| 750 | Move computation of p-values out of `get_pairwise_comparisons()`? | question | 2024-03-26 | discussion | low | medium | no | Partially addressed. PR #994 added `test_type = NULL` to skip p-value computation, resolving the main pain point (spurious warnings from `add_relative_skill()`). The broader architectural question of separating p-value from score ratio computation remains open but contributor consensus was option 1 (do nothing). | #769 (explicit test_type argument) should be considered together. #1022 (two-model case) tangentially related. | Consider closing as won't-fix or converting to long-term design discussion. The practical problem was solved by PR #994. If kept open, the most impactful remaining change would be defaulting `add_relative_skill()` to `test_type = NULL` since it discards p-values anyway. | N/A | N/A | — | N/A |\n| 740 | Make `get_forecast_unit()` and friends customisable such that they can work with new forecast classes | enhancement | 2024-03-24 | refactor | medium | medium | partially | Still relevant. No implementation work done. `get_protected_columns()` remains a plain function with hard-coded column lists (not S3). `get_duplicate_forecasts()` still hard-codes type-specific columns. Related S3 conversions (e.g., `get_forecast_type`, `get_pit`, `as_forecast`) establish the pattern. On `scoringutils-2.x` milestone. | Near-duplicate of #889 (S3 `get_protected_columns`) and #888 (S3 `get_duplicate_forecasts`). Blocks #852 (custom forecast class workflow). | Convert `get_protected_columns()` into an S3 generic with per-class methods (addressing #889), similarly make `get_duplicate_forecasts()` S3 (#888), so each forecast class declares its own columns. Follow established pattern from completed S3 conversions. | N/A | N/A | — | N/A |\n| 731 | Should `wis()` warn about missing values? | question | 2024-03-23 | discussion | low | small | no | Still relevant. Current behavior is correct — `as_forecast()` emits a vague NA message, `clean_forecast()` strips NA rows before `wis()` sees them. Discussion converged on improving the `as_forecast()` message (#537) as the primary fix, with `wis()`-specific warning being secondary. No consensus reached on implementation. | Related to #537 (high-priority: improve NA-removal message). Related to #542 (debug `score()`). Resolving #537 first would clarify what remains here. | Resolve #537 first (improve NA-removal message in `as_forecast()` to report affected row count). Then decide whether `wis()` needs its own warning for direct (non-`score()` pipeline) usage. May be closeable once #537 is done. | N/A | N/A | — | N/A |\n| 718 | Testing fails locally when done repeatedly | enhancement | 2024-03-20 | bug | low | small | yes | Still relevant and unfixed. Root cause: `cli_warn()` with `.frequency = \"once\"` and `.frequency_id = \"small_interval_range\"` suppresses warnings after first test run in same R session. 4 instances of `.frequency = \"once\"` in codebase. Tests pass on CI (fresh process) and on first local run. | No blocking dependencies. Tangentially related to #935 (surprising output behavior). | Add `rlang::reset_message_verbosity(\"small_interval_range\")` before the `expect_warning()` call in `test-metrics-interval-range.R`. Audit the other 3 `.frequency = \"once\"` sites to ensure their tests are similarly resilient. | [ ] | [ ] | — | [ ] |\n| 713 | Create vignette that showcases different ways to visualise forecasts and scores | documentation | 2024-03-11 | documentation | medium | medium | no | Still relevant. Package has 7 exported plot functions (`plot_wis`, `plot_heatmap`, `plot_interval_coverage`, `plot_quantile_coverage`, `plot_pairwise_comparisons`, `plot_forecast_counts`, `plot_correlations`) plus `theme_scoringutils()` but no dedicated visualization vignette. Only a deprecated-visualisations vignette exists. Issue requests updated `plot_score_table()` and mention of `hubVis`. | Related to #826 (plot_heatmap accessibility), #49 (calibration plotting), #942 (discrimination plots), #356 (interactions vignette). Does not block other issues. | Create a new `vignettes/visualising-scores.Rmd` with worked examples for all exported plot functions using example data. Include mention of `hubVis` as alternative for forecast visualization (per closed #812), and an updated `plot_score_table()` equivalent. | N/A | N/A | — | N/A |\n| 702 | Update docs to conform to tidyverse styleguide | good first issue | 2024-03-08 | documentation | low | small | partially | Still relevant. ~85% of `@param` tags already conform to sentence case. ~21 clearly non-conforming (lowercase start) and ~21 special-character-starting descriptions across ~15 R files. Blocker PR #705 (file reorg) was merged March 2024. Assigned to @jamesmbaazam but no PR submitted. | No blocking dependencies. Standalone cleanup task. PR #705 dependency resolved. | Fix ~21 `@param` descriptions across ~15 R files to start with capital letter and end with full stop per tidyverse style guide. Also audit ~21 special-character-starting descriptions and all `@return`/`@returns` tags for same conformance. | N/A | N/A | — | N/A |\n| 686 | Improve code coverage | | 2024-03-06 | meta | low | small | partially | Largely resolved. Coverage is at 98%. The specific sub-issue (#703, misnamed test file) was fixed in PR #704. Only `theme_scoringutils()` (18 lines) lacks any test coverage. The remaining 2% is edge-case branches. Could be closed as-is. | Sub-issue #703 resolved. No blockers. | Optionally add a test for `theme_scoringutils()` and a `.codecov.yml` with a coverage target to prevent regressions, then close this issue given the excellent 98% coverage level. | N/A | N/A | — | N/A |\n| 685 | Suppress printing in tests | enhancement, implementation-ready | 2024-03-06 | cleanup | medium | medium | partially | Still relevant. 29 `suppressMessages()` and 16 `suppressWarnings()` remain across 13 test files, silently swallowing conditions instead of asserting them. Partially addressed: 80+ `expect_no_condition()` calls already exist. Root causes include `as_forecast_*()` verbose messaging and the #935 print bug. | Partially depends on #935 (unwanted printing on `:=`). Overlaps significantly with #823 (reduce `suppress*()` in package). | Audit all 29 `suppressMessages()` and 16 `suppressWarnings()` in tests: replace with `expect_message()`/`expect_warning()` for intentional conditions, pass `verbose = FALSE` where available, or use `expect_no_condition()` after fixing the source. Also review `setup.R` score calls. | N/A | N/A | — | N/A |\n| 684 | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` | | 2024-03-06 | cleanup | low | small | yes | Still relevant but fully superseded by #939, which covers all 5 `check_input_*` functions. These 3 functions are dead code: never called in production, not exported, only tested. A comment in `check-input-helpers.R` explicitly marks them for future deletion. Should be closed as duplicate of #939. | Superseded by #939. Related to #840 (input check cleanup) and #823 (reduce `try()`). | Close as duplicate of #939. If handled independently, delete the 3 function definitions, their man pages, roxygen cross-references, and corresponding test blocks. | [ ] | [ ] | — | [ ] |\n| 668 | Add argument to control what kind of coverage is computed by `get_coverage()` | feature request | 2024-02-29 | enhancement | low | small | yes | Still relevant and unimplemented. `get_coverage()` always computes all 4 coverage metrics with no user control. The issue body contains a near-complete code proposal (with a minor bug to fix). The function structure already cleanly separates interval and quantile computation paths. On `scoringutils-2.x` milestone. | No blocking dependencies. Standalone enhancement. | Add a `type = c(\"quantile\", \"interval\")` argument to `get_coverage()` that conditionally computes interval coverage, quantile coverage, or both. Update downstream column handling, summarisation, and tests. | [ ] | [ ] | — | [ ] |\n| 667 | Create print method for `scores()` objects | feature request | 2024-02-29 | feature | low | small | partially | Still relevant but effectively blocked by #935. No `print.scores` method exists; scores objects fall through to `print.data.table`. Adding a custom print method would exacerbate the unwanted-printing-on-`:=` bug documented in #935. Both maintainers expressed lukewarm interest, deferring to post-release. | Blocked by #935 (print on column change bug). Related to #975 (link to score definitions in print). | Implement `print.scores()` S3 method displaying computed metric names (from `metrics` attribute) before delegating to `NextMethod()`, following the `print.forecast` pattern. Wait until #935 is resolved first to avoid spurious printing on `:=` operations. | N/A | N/A | — | N/A |\n| 666 | Update the scoringutils paper that accompanies the package | question | 2024-02-29 | documentation | medium | large | no | Still relevant but partially stale. Only 2 of ~15 checklist items done. Manuscript in `inst/manuscript/` still references `magrittr`, CITATION still points to 2022 arXiv preprint. README links to Google Drive PDF. Some checklist items reference functions no longer exported. Should be broken into smaller actionable issues. | Related to #759, #789, #546, #758, #1057. | Audit the to-do list against current API to remove stale items, break remaining unchecked items into individual issues, and decide whether to prioritize the \"make a vignette out of the paper\" item before the next CRAN release. Requires human authorship decisions. | N/A | N/A | — | N/A |\n| 646 | Stop converting errors to warnings in `score()`? | help wanted, question, Breaking change | 2024-02-23 | refactor | medium | medium | partially | Still relevant. `run_safely()` in `R/score.R` still converts all metric errors to warnings via `try()` + `cli_warn()` and returns `NULL`, silently dropping failed metrics. Deferred to `scoringutils-2.x` milestone. No consensus on approach: let errors propagate (breaking), add `on_error` parameter (compatible), or batch warnings (#811). | Related to #811 (batch warnings), #941 (`score()` redesign), #823 (reduce `try()`). | Design decision needed: (a) let errors propagate as errors, (b) add `on_error = c(\"warn\", \"error\")` param to `score()`, or (c) combine with #811's batch-warning approach. Implementation touches `run_safely()` in `R/score.R` and test expectations. | N/A | N/A | — | N/A |\n| 625 | Implement a `get_unique_values()` function | implementation-ready | 2024-02-10 | enhancement | low | small | yes | Still relevant. No implementation work done. Function does not exist. Would compute `length(unique(values))` per forecast-unit column (via `get_forecast_unit()`), optionally grouped by model. Similar diagnostic purpose to `get_forecast_counts()` but different output. | Loosely related to #581 (NA in print), #935 (print bug), #621 (closed, prettier print). No hard blockers for standalone function. | Implement a new exported `get_unique_values()` function that takes a forecast object, calls `get_forecast_unit()` to identify relevant columns, and returns a summary table of unique value counts per column. Optionally integrate into `print.forecast()`. | [ ] | [ ] | — | [ ] |\n| 608 | Define input format for categorical forecasts | implementation-ready | 2024-01-17 | discussion | low | small | no | Largely resolved. Input format fully defined and implemented in `forecast_nominal` (PR #837) and `forecast_ordinal` (PR #977). Column named `predicted_label`. Strict completeness enforced. Only unimplemented item is a helper function to auto-expand incomplete forecasts with zero-probability rows, which has no dedicated tracking issue. Can be closed. | #604 (parent, closed), #607 (naming, closed). No blockers. | Close this issue as completed. Optionally open a new targeted issue for the helper function that auto-expands incomplete forecasts with zero-probability rows if still desired. | N/A | N/A | — | N/A |\n| 581 | Add the number of `NA` values to the planned print method for `forecast_*` objects, rather than throw a message? | feature request | 2024-01-05 | enhancement | medium | medium | partially | Still relevant. None of the three requested features implemented: `print.forecast()` shows no NA info, NA message in `assert_forecast_generic()` is still vague (no row count), no `get_NA()` helper exists. Issue #537 is a near-duplicate and both should be addressed together. | Near-duplicate of #537 (should be resolved together). Tangentially related to #731, #542, #935 (print bug). | Enhance `print.forecast()` to display NA row count, improve the message in `assert_forecast_generic()` to report exact number of NA rows, and consider adding a `get_na_summary()` helper. Consolidate with #537. | N/A | N/A | — | N/A |\n| 557 | sample_to_quantile.numeric method | low-priority | 2024-01-05 | enhancement | low | small | yes | Effectively resolved. `sample_to_quantile()` was replaced by `as_forecast_quantile.forecast_sample()` in PR #876. The motivating code (`interval_coverage_sample()`) was deleted entirely. No current code needs a `.numeric` method. Can be closed. | Tangentially related to #1026 (reverse conversion). No blockers. | Close this issue as resolved. The refactoring in PR #876 and deletion of `interval_coverage_sample()` eliminated the code pattern that motivated this request. | N/A | N/A | — | N/A |\n| 546 | Simplify \"Getting Started\" \\| Split into smaller chunks | documentation | 2024-01-05 | documentation | medium | medium | no | Still relevant. The old `scoringutils.Rmd` was deleted (PR #836) but the promised replacement \"Get started\" vignette was never written. Package currently has no introductory vignette at all. README has a \"Quick start\" section but no pkgdown \"Get started\" page. @seabbs volunteered in #759 but no PR materialized. | Tightly coupled with #759 (future of scoringutils.Rmd) — should be resolved together. Related to #789 (metric customization docs). | Write a new introductory \"Getting Started\" vignette (as `vignettes/scoringutils.Rmd` for pkgdown) providing a concise end-to-end walkthrough. Can close both #546 and #759 simultaneously. Requires editorial decisions on scope and content. | N/A | N/A | — | N/A |\n| 542 | Improve ways to debug `score()` | question | 2024-01-04 | enhancement | medium | medium | partially | Still relevant. Neither action item is done: (1) no vignette documents `debugonce()` for scoring rule debugging, (2) `run_safely()` still only reports the metric name in warnings, not which forecast unit/row caused the failure. The `{cli}` migration (PR #583) improved formatting but not informativeness. | Closely related to #646 (stop converting errors to warnings — open, breaking change). Related to closed #391, #418, #543, #583. | Two tasks remain: (1) add documentation showing `debugonce()` usage for debugging scoring failures, and (2) enhance `run_safely()`/`apply_metrics()` to include forecast-unit context in warning messages. Coordinate error-vs-warning design with #646. | N/A | N/A | — | N/A |\n| 537 | Inform users about the number of rows that have NA values | good first issue, question, implementation-ready, high-priority | 2024-01-03 | enhancement | medium | medium | partially | Still relevant. The message in `assert_forecast_generic()` (line 150 of `R/class-forecast.R`) still says the generic \"Some rows containing NA values may be removed\" without a row count. No NA info in `print.forecast`, no `get_na()` helper. The `anyNA()` check and `na.omit()` call are already in place, so computing the count is trivial. | Near-duplicate of #581 (NA count in print method) — should be resolved together. Related to #731 (wis NA warnings). | Minimal fix: add NA row count to the existing `cli_inform` message in `assert_forecast_generic()`. For a more complete resolution, combine with #581 to also add NA summary to `print.forecast` and potentially create a `get_na()` diagnostic helper. | N/A | N/A | — | N/A |\n| 522 | Create a way to test user-provided scoring functions | feature request | 2023-12-06 | feature | medium | medium | partially | Still relevant. No progress made. `validate_metrics()` only checks that items are functions (via `checkmate::check_function`), not signature compatibility, output shape, or runtime behavior. Custom metric errors are silently converted to warnings by `run_safely()`, making debugging hard. Each of the 6+ forecast types passes different arguments to metrics. | Loosely related to #362 (non-standard metric I/O), #823 (reduce `try()`). | Implement an exported `test_metrics()` function that runs each metric on a small representative subset of a forecast object and reports informative diagnostics: signature compatibility, runtime errors, and output shape validation. Needs design decision on API. | N/A | N/A | — | N/A |\n| 520 | Create documentation/vignette for the naming scheme of our functions | help wanted, question | 2023-11-30 | documentation | low | medium | partially | Still relevant. The issue body already describes the naming scheme in detail but no vignette or formal documentation exists. CONTRIBUTING.md only references tidyverse style guide. Issue #948 (closed) debated `get_` prefix but decided to keep it. 65 exported functions follow conventions consistently. ~2 years old with no implementation. | Related to closed #948, #610, #476, #357. No blockers. | Create a developer-facing vignette or CONTRIBUTING.md section formalizing the naming conventions (scoring rules: `<name>_<type>`, `get_*`, `as_forecast_*`, `is_forecast_*`, `plot_*`, `add_*`, `assert_`/`check_`/`test_`). Update to reflect current classes including `forecast_nominal`, `forecast_ordinal`, `forecast_sample_multivariate`. | N/A | N/A | — | N/A |\n| 513 | Investigate R scripts that get generated based on our vignettes | | 2023-11-28 | cleanup | low | small | yes | Already resolved. The `.R` and `.html` vignette artifacts (generated by `knitr` tangle) are gitignored via `vignettes/.gitignore`, excluded from linting via `.lintr`, and not tracked in git. All three mitigations from the discussion are in place. Issue can be closed. | None. | Close this issue — all requested investigation and mitigations (gitignore, buildignore, lint-ignore for vignette-generated `.R` files) are already implemented. No code changes needed. | N/A | N/A | — | N/A |\n| 492 | Expose `get_range_from_quantile()` to the user | question | 2023-11-22 | enhancement | low | small | yes | Still relevant. Function exists at `R/helper-quantile-interval-range.R:185` as `@keywords internal`, not exported. Maintainer consensus reached: rename to `get_interval_range()` and create `add_interval_range()` wrapper. Neither implemented. Original motivating function `plot_ranges()` was deleted, reducing urgency. 3 internal call sites. | Parent meta-issue #484. Tangentially related to #885 (export decisions). | Rename `get_range_from_quantile()` to `get_interval_range()`, export it, and create a new exported `add_interval_range()` that adds an `interval_range` column to a quantile-format data.table. Update 3 internal call sites, NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n| 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |\n| 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |\n| 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |\n| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |\n| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n\nTRIAGE_COMPLETE","numLines":131,"startLine":1,"totalLines":131}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01BQd8wNzdBt8jqBPv897orU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PpbERiDLreiKGiaVAdGE2A","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"in_progress","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"pending","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":25528,"cache_read_input_tokens":20673,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":25528},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"35f0a1d3-63d2-4e6a-9436-17f5268b6731"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PpbERiDLreiKGiaVAdGE2A","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"7fa679a2-b1b9-4623-8541-ae9a6e2cc98b","tool_use_result":{"oldTodos":[],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"in_progress","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"pending","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_013fRr716ychU84cADTHqJKH","type":"message","role":"assistant","content":[{"type":"text","text":"The first automatable issue with unchecked Has Tests is **#1044 - Remove the commented out vignette code** (row 29). Let me get the full context.\n\n- **Action Summary**: Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere.\n- **Status Note**: Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment.\n\nLet me fetch the full GitHub issue and examine the relevant files."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":46201,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"35eecc29-d51d-4333-8f76-0b992a98c35c"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_013fRr716ychU84cADTHqJKH","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013wCZ7cu7CqcUXR9Z756DKq","name":"Bash","input":{"command":"gh issue view 1044 --comments","description":"View issue #1044 with comments"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":46201,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"54c0e8ec-5713-42dc-9446-a6491a6de8d2"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_013fRr716ychU84cADTHqJKH","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NhBA8UiXD5ezJe1E7hAt8r","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/vignettes/scoring-multivariate-forecasts.Rmd"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":46201,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8a55f201-182e-49b9-8b73-125accfc79ff"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_013fRr716ychU84cADTHqJKH","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019PkMa2hopAEKFxpwJZU3WA","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/vignettes/scoring-rules.Rmd"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":46201,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"3ec0b0a2-32f1-4b8a-a5ca-c37e13f2b71f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013wCZ7cu7CqcUXR9Z756DKq","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"d80609ca-f6af-4e6d-b415-498344b40097","tool_use_result":{"stdout":"","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NhBA8UiXD5ezJe1E7hAt8r","type":"tool_result","content":"     1→---\n     2→title: \"Scoring multivariate forecasts\"\n     3→output: rmarkdown::html_vignette\n     4→vignette: >\n     5→  %\\VignetteIndexEntry{Scoring multivariate forecasts}\n     6→  %\\VignetteEngine{knitr::rmarkdown}\n     7→  %\\VignetteEncoding{UTF-8}\n     8→---\n     9→\n    10→```{r, include = FALSE}\n    11→knitr::opts_chunk$set(\n    12→  collapse = TRUE,\n    13→  comment = \"#>\"\n    14→)\n    15→```\n    16→This Vignette provides an overview about how to score multivariate forecasts.\n    17→\n    18→\n    19→## Univariate forecasts\n    20→\n    21→Let's start with a simple univariate forecast: The number of cases of COVID-19 in Germany on 2021-05-15, forecasted by the EuroCOVIDhub-ensemble model on 2021-05-03. In our example, this forecast is represented by a set of 40 samples from the predictive distribution.\n    22→\n    23→```{r}\n    24→library(scoringutils)\n    25→\n    26→example_univ_single <- example_sample_continuous[\n    27→  target_type == \"Cases\" &\n    28→    location == \"DE\" &\n    29→    forecast_date == \"2021-05-03\" &\n    30→    target_end_date == \"2021-05-15\" &\n    31→    horizon == 2 &\n    32→    model == \"EuroCOVIDhub-ensemble\"\n    33→]\n    34→example_univ_single\n    35→```\n    36→\n    37→We can score this forecast and will receive a single score.\n    38→```{r}\n    39→score(example_univ_single)\n    40→```\n    41→\n    42→Now, of course, we can also score multiple similar forecasts at the same time. Let's say we're not only interested in Germany, but other countries as well.\n    43→\n    44→```{r}\n    45→example_univ_multi <- example_sample_continuous[\n    46→  target_type == \"Cases\" &\n    47→    forecast_date == \"2021-05-03\" &\n    48→    target_end_date == \"2021-05-15\" &\n    49→    horizon == 2 &\n    50→    model == \"EuroCOVIDhub-ensemble\"\n    51→]\n    52→example_univ_multi\n    53→```\n    54→\n    55→Now, we have a set of 4 forecasts for 4 different countries, each of them represented by a set of 40 samples from the predictive distribution.\n    56→\n    57→When we score these forecasts, we will get 4 scores, one for each forecast and observed value.\n    58→```{r}\n    59→score(example_univ_multi)\n    60→```\n    61→\n    62→## Multivariate forecasts\n    63→\n    64→Now, instead of treating the four observations as independent, we could also think of them as a single realisation of a draw from the multivariate distribution of COVID-19 cases across several countries.\n    65→\n    66→The corresponding multivariate forecast would similarly specify a predictive distribution for the number of cases across all 4 countries. The samples are then not draws from four independent distributions, but instead samples from a joint multivariate predictive distribution.\n    67→\n    68→In the following, let's assume that our samples were draws from a multivariate distribution all along (we just treated them as independent for the univariate case).\n    69→\n    70→To tell `scoringutils` that we want to treat these as a multivariate forecast, we need to specify the columns that are pooled together to form a single multivariate forecast. We do this via the `joint_across` argument. For example, if we want to pool forecasts across locations and treat them as a single multivariate forecast, we could set `joint_across = c(\"location\", \"location_name\")` (in our example, the two columns contain essentially the same information - we therefore have to include both in `joint_across` (or could alternatively delete one of them)).\n    71→\n    72→```{r}\n    73→example_multiv <- as_forecast_multivariate_sample(\n    74→  data = example_univ_multi,\n    75→  c(\"location\", \"location_name\")\n    76→)\n    77→example_multiv\n    78→```\n    79→\n    80→The column `.mv_group_id` is created automatically and represents an identifier for each multivariate forecast. `.mv_group_id` is 1 everywhere, because we only have a single multivariate forecast. When scoring this forecast using an appropriate multivariate scoring function, we will get a single score, even though we have 4 observations, one for each country. (Note that for the purposes of scoring, it doesn't matter that sample ids are still 1-40, repeated 4 times, instead of 1-160. `scoringutils` handles this appropriately.)\n    81→\n    82→```{r}\n    83→score(example_multiv)\n    84→```\n    85→\n    86→If, at any point, you want to score the same forecast using different groupings, you'd have create a new separate forecast object with a different grouping and score that new forecast object.\n    87→\n    88→<!-- ## Details on the grouping - WIP\n    89→\n    90→As mentioned above, you can set the grouping with the `set_grouping` function. This will return the original data with an additional column `.mv_group_id` that indicates the group each observation belongs to.\n    91→\n    92→Alternatively, you can set the grouping right when creating the forecast.\n    93→\n    94→\n    95→forecast <- as_forecast_sample(\n    96→  example_sample_continuous,\n    97→  grouping = c(\"target_end_date\", \"target_type\", \"forecast_date\", \"model\", \"horizon\")\n    98→)\n    99→\n   100→forecast -->\n   101→\n   102→\n   103→\n   104→## Univariate and multivariate scoring for matrices\n   105→\n   106→Note: this section may only be relevant to you if you're planning to score forecasts in matrix format.\n   107→\n   108→Let's construct a simple multivariate forecast:\n   109→\n   110→```{r}\n   111→# parameters for multivariate normal example\n   112→set.seed(123)\n   113→d <- 10 # number of dimensions\n   114→m <- 50 # number of samples from multivariate forecast distribution\n   115→\n   116→mu0 <- rep(0, d)\n   117→mu <- rep(1, d)\n   118→\n   119→S0 <- S <- diag(d)\n   120→S0[S0 == 0] <- 0.2\n   121→S[S == 0] <- 0.1\n   122→\n   123→# generate samples from multivariate normal distributions\n   124→obs <- drop(mu0 + rnorm(d) %*% chol(S0))\n   125→fc_sample <- replicate(m, drop(mu + rnorm(d) %*% chol(S)))\n   126→\n   127→obs2 <- drop(mu0 + rnorm(d) %*% chol(S0))\n   128→fc_sample2 <- replicate(m, drop(mu + rnorm(d) %*% chol(S)))\n   129→```\n   130→\n   131→Now, we can compute the Energy Score. Let's compare the `scoringutils` implementation with that of the `scoringRules` package, on which the `scoringutils` implementation is based. The only difference is that `scoringRules` always expects a single multivariate `forecast`, while the `scoringutils` implementation can handle multiple multivariate forecasts together, identified via a grouping vector (assuming they all have the same dimension).\n   132→\n   133→```{r}\n   134→scoringRules::es_sample(y = obs, dat = fc_sample)\n   135→# in the univariate case, Energy Score and CRPS are the same\n   136→# illustration: Evaluate forecast sample for the first variable\n   137→es_sr1 <- scoringRules::es_sample(y = obs, dat = fc_sample)\n   138→es_sr2 <- scoringRules::es_sample(y = obs2, dat = fc_sample2)\n   139→es_sr <- c(es_sr1, es_sr2)\n   140→\n   141→es_su <- energy_score_multivariate(\n   142→  observed = c(obs, obs2),\n   143→  predicted = rbind(fc_sample, fc_sample2),\n   144→  mv_group_id = c(rep(1, d), rep(2, d))\n   145→)\n   146→all.equal(es_sr, es_su, tolerance = 1e-6, check.attributes = FALSE)\n   147→```\n   148→\n   149→You can provide observation weights when computing the Energy Score.\n   150→\n   151→```{r}\n   152→# illustration of observation weights for Energy Score\n   153→# example: equal weights for first half of draws; zero weights for other draws\n   154→w <- rep(c(1, 0), each = 0.5 * m) / (0.5 * m)\n   155→\n   156→es_sr1 <- scoringRules::es_sample(y = obs, dat = fc_sample, w = w)\n   157→es_sr2 <- scoringRules::es_sample(y = obs2, dat = fc_sample2, w = w)\n   158→es_sr <- c(es_sr1, es_sr2)\n   159→\n   160→es_su <- energy_score_multivariate(\n   161→  observed = c(obs, obs2),\n   162→  predicted = rbind(fc_sample, fc_sample2),\n   163→  mv_group_id = c(rep(1, d), rep(2, d)),\n   164→  w = w\n   165→)\n   166→\n   167→all.equal(es_sr, es_su, tolerance = 1e-6, check.attributes = FALSE)\n   168→```\n   169→"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"ea715080-0f67-4c34-9756-bee777c827a9","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/vignettes/scoring-multivariate-forecasts.Rmd","content":"---\ntitle: \"Scoring multivariate forecasts\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{Scoring multivariate forecasts}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\"\n)\n```\nThis Vignette provides an overview about how to score multivariate forecasts.\n\n\n## Univariate forecasts\n\nLet's start with a simple univariate forecast: The number of cases of COVID-19 in Germany on 2021-05-15, forecasted by the EuroCOVIDhub-ensemble model on 2021-05-03. In our example, this forecast is represented by a set of 40 samples from the predictive distribution.\n\n```{r}\nlibrary(scoringutils)\n\nexample_univ_single <- example_sample_continuous[\n  target_type == \"Cases\" &\n    location == \"DE\" &\n    forecast_date == \"2021-05-03\" &\n    target_end_date == \"2021-05-15\" &\n    horizon == 2 &\n    model == \"EuroCOVIDhub-ensemble\"\n]\nexample_univ_single\n```\n\nWe can score this forecast and will receive a single score.\n```{r}\nscore(example_univ_single)\n```\n\nNow, of course, we can also score multiple similar forecasts at the same time. Let's say we're not only interested in Germany, but other countries as well.\n\n```{r}\nexample_univ_multi <- example_sample_continuous[\n  target_type == \"Cases\" &\n    forecast_date == \"2021-05-03\" &\n    target_end_date == \"2021-05-15\" &\n    horizon == 2 &\n    model == \"EuroCOVIDhub-ensemble\"\n]\nexample_univ_multi\n```\n\nNow, we have a set of 4 forecasts for 4 different countries, each of them represented by a set of 40 samples from the predictive distribution.\n\nWhen we score these forecasts, we will get 4 scores, one for each forecast and observed value.\n```{r}\nscore(example_univ_multi)\n```\n\n## Multivariate forecasts\n\nNow, instead of treating the four observations as independent, we could also think of them as a single realisation of a draw from the multivariate distribution of COVID-19 cases across several countries.\n\nThe corresponding multivariate forecast would similarly specify a predictive distribution for the number of cases across all 4 countries. The samples are then not draws from four independent distributions, but instead samples from a joint multivariate predictive distribution.\n\nIn the following, let's assume that our samples were draws from a multivariate distribution all along (we just treated them as independent for the univariate case).\n\nTo tell `scoringutils` that we want to treat these as a multivariate forecast, we need to specify the columns that are pooled together to form a single multivariate forecast. We do this via the `joint_across` argument. For example, if we want to pool forecasts across locations and treat them as a single multivariate forecast, we could set `joint_across = c(\"location\", \"location_name\")` (in our example, the two columns contain essentially the same information - we therefore have to include both in `joint_across` (or could alternatively delete one of them)).\n\n```{r}\nexample_multiv <- as_forecast_multivariate_sample(\n  data = example_univ_multi,\n  c(\"location\", \"location_name\")\n)\nexample_multiv\n```\n\nThe column `.mv_group_id` is created automatically and represents an identifier for each multivariate forecast. `.mv_group_id` is 1 everywhere, because we only have a single multivariate forecast. When scoring this forecast using an appropriate multivariate scoring function, we will get a single score, even though we have 4 observations, one for each country. (Note that for the purposes of scoring, it doesn't matter that sample ids are still 1-40, repeated 4 times, instead of 1-160. `scoringutils` handles this appropriately.)\n\n```{r}\nscore(example_multiv)\n```\n\nIf, at any point, you want to score the same forecast using different groupings, you'd have create a new separate forecast object with a different grouping and score that new forecast object.\n\n<!-- ## Details on the grouping - WIP\n\nAs mentioned above, you can set the grouping with the `set_grouping` function. This will return the original data with an additional column `.mv_group_id` that indicates the group each observation belongs to.\n\nAlternatively, you can set the grouping right when creating the forecast.\n\n\nforecast <- as_forecast_sample(\n  example_sample_continuous,\n  grouping = c(\"target_end_date\", \"target_type\", \"forecast_date\", \"model\", \"horizon\")\n)\n\nforecast -->\n\n\n\n## Univariate and multivariate scoring for matrices\n\nNote: this section may only be relevant to you if you're planning to score forecasts in matrix format.\n\nLet's construct a simple multivariate forecast:\n\n```{r}\n# parameters for multivariate normal example\nset.seed(123)\nd <- 10 # number of dimensions\nm <- 50 # number of samples from multivariate forecast distribution\n\nmu0 <- rep(0, d)\nmu <- rep(1, d)\n\nS0 <- S <- diag(d)\nS0[S0 == 0] <- 0.2\nS[S == 0] <- 0.1\n\n# generate samples from multivariate normal distributions\nobs <- drop(mu0 + rnorm(d) %*% chol(S0))\nfc_sample <- replicate(m, drop(mu + rnorm(d) %*% chol(S)))\n\nobs2 <- drop(mu0 + rnorm(d) %*% chol(S0))\nfc_sample2 <- replicate(m, drop(mu + rnorm(d) %*% chol(S)))\n```\n\nNow, we can compute the Energy Score. Let's compare the `scoringutils` implementation with that of the `scoringRules` package, on which the `scoringutils` implementation is based. The only difference is that `scoringRules` always expects a single multivariate `forecast`, while the `scoringutils` implementation can handle multiple multivariate forecasts together, identified via a grouping vector (assuming they all have the same dimension).\n\n```{r}\nscoringRules::es_sample(y = obs, dat = fc_sample)\n# in the univariate case, Energy Score and CRPS are the same\n# illustration: Evaluate forecast sample for the first variable\nes_sr1 <- scoringRules::es_sample(y = obs, dat = fc_sample)\nes_sr2 <- scoringRules::es_sample(y = obs2, dat = fc_sample2)\nes_sr <- c(es_sr1, es_sr2)\n\nes_su <- energy_score_multivariate(\n  observed = c(obs, obs2),\n  predicted = rbind(fc_sample, fc_sample2),\n  mv_group_id = c(rep(1, d), rep(2, d))\n)\nall.equal(es_sr, es_su, tolerance = 1e-6, check.attributes = FALSE)\n```\n\nYou can provide observation weights when computing the Energy Score.\n\n```{r}\n# illustration of observation weights for Energy Score\n# example: equal weights for first half of draws; zero weights for other draws\nw <- rep(c(1, 0), each = 0.5 * m) / (0.5 * m)\n\nes_sr1 <- scoringRules::es_sample(y = obs, dat = fc_sample, w = w)\nes_sr2 <- scoringRules::es_sample(y = obs2, dat = fc_sample2, w = w)\nes_sr <- c(es_sr1, es_sr2)\n\nes_su <- energy_score_multivariate(\n  observed = c(obs, obs2),\n  predicted = rbind(fc_sample, fc_sample2),\n  mv_group_id = c(rep(1, d), rep(2, d)),\n  w = w\n)\n\nall.equal(es_sr, es_su, tolerance = 1e-6, check.attributes = FALSE)\n```\n","numLines":169,"startLine":1,"totalLines":169}}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019PkMa2hopAEKFxpwJZU3WA","type":"tool_result","content":"     1→---\n     2→title: \"Scoring rules in `scoringutils`\"\n     3→author: \"Nikos Bosse\"\n     4→date: \"`r Sys.Date()`\"\n     5→output: rmarkdown::html_vignette\n     6→vignette: >\n     7→  %\\VignetteIndexEntry{Scoring rules in `scoringutils`}\n     8→  %\\VignetteEngine{knitr::rmarkdown}\n     9→  %\\VignetteEncoding{UTF-8}\n    10→bibliography: \"scoring-rules/scoringutils-package.bib\"\n    11→---\n    12→\n    13→```{r, include = FALSE}\n    14→knitr::opts_chunk$set(\n    15→  fig.width = 7,\n    16→  collapse = TRUE,\n    17→  comment = \"#>\",\n    18→  cache = FALSE\n    19→)\n    20→library(scoringutils)\n    21→library(data.table)\n    22→```\n    23→\n    24→# Introduction\n    25→\n    26→This vignette gives an overview of the default scoring rules made available through the `scoringutils` package. You can, of course, also use your own scoring rules, provided they follow the same format. If you want to obtain more detailed information about how the package works, have a look at the [revised version](https://drive.google.com/file/d/1URaMsXmHJ1twpLpMl1sl2HW4lPuUycoj/view?usp=drive_link) of our `scoringutils` paper.\n    27→\n    28→We can distinguish two types of forecasts: point forecasts and probabilistic forecasts. A point forecast is a single number representing a single outcome. A probabilistic forecast is a full predictive probability distribution over multiple possible outcomes. In contrast to point forecasts, probabilistic forecasts incorporate uncertainty about different possible outcomes.\n    29→\n    30→Scoring rules are functions that take a forecast and an observation as input and return a single numeric value. For point forecasts, they take the form $S(\\hat{y}, y)$, where $\\hat{y}$ is the forecast and $y$ is the observation. For probabilistic forecasts, they usually take the form $S(F, y)$, where $F$ is the cumulative density function (CDF) of the predictive distribution and $y$ is the observation. By convention, scoring rules are usually negatively oriented, meaning that smaller values are better (the best possible score is usually zero). In that sense, the score can be understood as a penalty.\n    31→\n    32→Many scoring rules for probabilistic forecasts are so-called (strictly) proper scoring rules. Essentially, this means that they cannot be \"cheated\": A forecaster evaluated by a strictly proper scoring rule is always incentivised to report her honest best belief about the future and cannot, in expectation, improve her score by reporting something else. A more formal definition is the following: Let $G$ be the true, unobserved data-generating distribution. A scoring rule is said to be proper, if under $G$ and for an ideal forecast $F = G$, there is no forecast $F' \\neq F$ that in expectation receives a better score than $F$. A scoring rule is considered strictly proper if, under $G$, no other forecast $F'$ in expectation receives a score that is better than or the same as that of $F$.\n    33→\n    34→---\n    35→\n    36→# Metrics for point forecasts\n    37→\n    38→See a list of the default metrics for point forecasts by calling `get_metrics(example_point)`.\n    39→\n    40→This is an overview of the input and output formats for point forecasts:\n    41→\n    42→```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for point.\"}\n    43→knitr::include_graphics(file.path(\"scoring-rules\", \"input-point.png\"))\n    44→```\n    45→\n    46→\n    47→## A note of caution\n    48→\n    49→Scoring point forecasts can be tricky business. Depending on the choice of the scoring rule, a forecaster who is clearly worse than another, might consistently receive better scores (see @gneitingMakingEvaluatingPoint2011 for an illustrative example).\n    50→\n    51→Every scoring rule for a point forecast is implicitly minimised by a specific aspect of the predictive distribution. The mean squared error, for example, is only a meaningful scoring rule if the forecaster actually reported the mean of their predictive distribution as a point forecast. If the forecaster reported the median, then the mean absolute error would be the appropriate scoring rule. If the scoring rule and the predictive task do not align, misleading results ensue. Consider the following example:\n    52→\n    53→```{r}\n    54→set.seed(123)\n    55→n <- 1000\n    56→observed <- rnorm(n, 5, 4)^2\n    57→\n    58→predicted_mu <- mean(observed)\n    59→predicted_not_mu <- predicted_mu - rnorm(n, 10, 2)\n    60→\n    61→mean(Metrics::ae(observed, predicted_mu))\n    62→mean(Metrics::ae(observed, predicted_not_mu))\n    63→\n    64→mean(Metrics::se(observed, predicted_mu))\n    65→mean(Metrics::se(observed, predicted_not_mu))\n    66→```\n    67→\n    68→\n    69→\n    70→## Absolute error\n    71→\n    72→**Observation**: $y$, a real number\n    73→\n    74→**Forecast**: $\\hat{y}$, a real number, the median of the forecaster's predictive distribution.\n    75→\n    76→The absolute error is the absolute difference between the predicted and the observed values. See `?Metrics::ae`.\n    77→\n    78→$$\\text{ae} = |y - \\hat{y}|$$\n    79→\n    80→The absolute error is only an appropriate rule if $\\hat{y}$ corresponds to the median of the forecaster's predictive distribution. Otherwise, results will be misleading (see @gneitingMakingEvaluatingPoint2011).\n    81→\n    82→## Squared error\n    83→\n    84→**Observation**: $y$, a real number\n    85→\n    86→**Forecast**: $\\hat{y}$, a real number, the mean of the forecaster's predictive distribution.\n    87→\n    88→The squared error is the squared difference between the predicted and the observed values. See `?Metrics::se`.\n    89→\n    90→$$\\text{se} = (y - \\hat{y})^2$$\n    91→The squared error is only an appropriate rule if $\\hat{y}$ corresponds to the mean of the forecaster's predictive distribution. Otherwise, results will be misleading (see @gneitingMakingEvaluatingPoint2011).\n    92→\n    93→## Absolute percentage error\n    94→\n    95→**Observation**: $y$, a real number\n    96→\n    97→**Forecast**: $\\hat{y}$, a real number\n    98→\n    99→The absolute percentage error is the absolute percent difference between the predicted and the observed values. See `?Metrics::ape`.\n   100→\n   101→$$\\text{ape} = \\frac{|y - \\hat{y}|}{|y|}$$\n   102→\n   103→The absolute percentage error is only an appropriate rule if $\\hat{y}$ corresponds to the $\\beta$-median of the forecaster's predictive distribution with $\\beta = -1$. The $\\beta$-median, $\\text{med}^{(\\beta)}(F)$, is the median of a random variable whose density is proportional to $y^\\beta f(y)$. The specific $\\beta$-median that corresponds to the absolute percentage error is $\\text{med}^{(-1)}(F)$. Otherwise, results will be misleading (see @gneitingMakingEvaluatingPoint2011).\n   104→\n   105→---\n   106→\n   107→# Binary forecasts\n   108→\n   109→See a list of the default metrics for point forecasts by calling `?get_metrics(example_binary)`.\n   110→\n   111→This is an overview of the input and output formats for point forecasts:\n   112→\n   113→```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for binary forecasts.\"}\n   114→knitr::include_graphics(file.path(\"scoring-rules\", \"input-binary.png\"))\n   115→```\n   116→\n   117→\n   118→## Brier score\n   119→\n   120→**Observation**: $y$, either 0 or 1\n   121→\n   122→**Forecast**: $p$, a probability that the observed outcome will be 1.\n   123→\n   124→The Brier score is a strictly proper scoring rule. It is computed as the mean squared error between the probabilistic prediction and the observed outcome.\n   125→\n   126→\\begin{equation}\n   127→    \\text{BS}(p, y) = (p - y)^2 =\n   128→    \\begin{cases}\n   129→        p^2,       & \\text{if } y = 1\\\\\n   130→        (1 - p)^2,   & \\text{if } y = 0\n   131→    \\end{cases}\n   132→\\end{equation}\n   133→\n   134→The Brier score and the logarithmic score (see below) differ in how they penalise over- and underconfidence (see @macheteContrastingProbabilisticScoring2012). The Brier score penalises overconfidence and underconfidence in probability space the same. Consider the following example:\n   135→\n   136→```{r}\n   137→n <- 1e6\n   138→p_true <- 0.7\n   139→observed <- factor(rbinom(n = n, size = 1, prob = p_true), levels = c(0, 1))\n   140→\n   141→p_over <- p_true + 0.15\n   142→p_under <- p_true - 0.15\n   143→\n   144→abs(mean(brier_score(observed, p_true)) - mean(brier_score(observed, p_over)))\n   145→abs(mean(brier_score(observed, p_true)) - mean(brier_score(observed, p_under)))\n   146→```\n   147→\n   148→See `?brier_score()` for more information.\n   149→\n   150→\n   151→## Logarithmic score\n   152→\n   153→**Observation**: $y$, either 0 or 1\n   154→\n   155→**Forecast**: $p$, a probability that the observed outcome will be 1.\n   156→\n   157→The logarithmic score (or log score) is a strictly proper scoring rule. It is computed as the negative logarithm of the probability assigned to the observed outcome.\n   158→\n   159→\\begin{equation}\n   160→    \\text{Log score}(p, y) = - \\log(1 - |y - p|) =\n   161→    \\begin{cases}\n   162→        -\\log (p),       & \\text{if } y = 1\\\\\n   163→        -\\log (1 - p),   & \\text{if } y = 0\n   164→    \\end{cases}\n   165→\\end{equation}\n   166→\n   167→The log score penalises overconfidence more strongly than underconfidence (in probability space). Consider the following example:\n   168→\n   169→```{r}\n   170→abs(mean(logs_binary(observed, p_true)) - mean(logs_binary(observed, p_over)))\n   171→abs(mean(logs_binary(observed, p_true)) - mean(logs_binary(observed, p_under)))\n   172→```\n   173→\n   174→See `?logs_binary()` for more information.\n   175→\n   176→\n   177→---\n   178→\n   179→# Sample-based forecasts\n   180→\n   181→See a list of the default metrics for sample-based forecasts by calling `get_metrics(example_sample_continuous)`.\n   182→\n   183→This is an overview of the input and output formats for quantile forecasts:\n   184→\n   185→```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for sample-based forecasts.\"}\n   186→knitr::include_graphics(file.path(\"scoring-rules\", \"input-sample.png\"))\n   187→```\n   188→\n   189→\n   190→## CRPS\n   191→\n   192→**Observation**: $y$, a real number (or a discrete number).\n   193→\n   194→**Forecast**: A continuous ($F$) or discrete ($P$) forecast.\n   195→\n   196→The continuous ranked probability score (CRPS) is popular in fields such as meteorology and epidemiology. The CRPS is defined as\n   197→$$\\text{CRPS}(F, y) = \\int_{-\\infty}^\\infty \\left( F(x) - 1(x \\geq y) \\right)^2 dx,$$\n   198→where $y$ is the observed value and $F$ the CDF of predictive distribution.\n   199→\n   200→For discrete forecasts, for example count data, the ranked probability score (RPS) can be used instead and is commonly defined as:\n   201→$$ \\text{RPS}(P, y) = \\sum_{x = 0}^\\infty (P(x) - 1(x \\geq y))^2, $$\n   202→where $P$ is the cumulative probability mass function (PMF) of the predictive distribution.\n   203→\n   204→\n   205→The CRPS can be understood as a generalisation of the absolute error to predictive distributions [@gneitingStrictlyProperScoring2007]. It can also be understood as the integral over the Brier score for the binary probability forecasts implied by the CDF for all possible observed values. The CRPS is also related to the Cramér-distance between two distributions and equals the special case where one of the distributions is concentrated in a single point (see e.g. @zielEnergyDistanceEnsemble2021). The CRPS is a global scoring rule, meaning that the entire predictive distribution is taken into account when determining the quality of the forecast.\n   206→\n   207→`scoringutils` re-exports the `crps_sample()` function from the `scoringRules` package, which assumes that the forecast is represented by a set of samples from the predictive distribution. See `?crps_sample()` for more information.\n   208→\n   209→\n   210→### Overprediction, underprediction and dispersion\n   211→\n   212→The CRPS can be interpreted as a sum of a dispersion, an overprediction and an underprediction component.\n   213→If $m$ is the median forecast then the dispersion component is\n   214→$$\\text{CRPS}(F, m),$$\n   215→the overprediction component is\n   216→$$\n   217→\\begin{cases}\n   218→m > y & CRPS(F, y) - CRPS(F, m)\\\\\n   219→m \\leq y & 0\\\\\n   220→\\end{cases}\n   221→$$\n   222→and the underprediction component is\n   223→$$\n   224→\\begin{cases}\n   225→m < y & CRPS(F, y) - CRPS(F, m)\\\\\n   226→m \\geq y & 0\\\\\n   227→\\end{cases}\n   228→$$\n   229→\n   230→These can be accessed via the `dispersion_sample()`, `overprediction_sample()` and `underprediction_sample()` functions, respectively.\n   231→\n   232→## Log score\n   233→\n   234→**Observation**: $y$, a real number (or a discrete number).\n   235→\n   236→**Forecast**: A continuous ($F$) or discrete ($P$) forecast.\n   237→\n   238→The logarithmic scoring rule is simply the negative logarithm of the density of the the predictive distribution evaluated at the observed value:\n   239→\n   240→$$ \\text{log score}(F, y) = -\\log f(y), $$\n   241→\n   242→where $f$ is the predictive probability density function (PDF) corresponding to the Forecast $F$ and $y$ is the observed value.\n   243→\n   244→For discrete forecasts, the log score can be computed as\n   245→\n   246→$$ \\text{log score}(F, y) = -\\log p_y, $$\n   247→where $p_y$ is the probability assigned to the observed outcome $y$ by the forecast $F$.\n   248→\n   249→The logarithmic scoring rule can produce large penalties when the observed value takes on values for which $f(y)$ (or $p_y$) is close to zero. It is therefore considered to be sensitive to outlier forecasts. This may be desirable in some applications, but it also means that scores can easily be dominated by a few extreme values. The logarithmic scoring rule is a local scoring rule, meaning that the score only depends on the probability that was assigned to the actual outcome. This is often regarded as a desirable property for example in the context of Bayesian inference \\citep{winklerScoringRulesEvaluation1996}. It implies for example, that the ranking between forecasters would be invariant under monotone transformations of the predictive distribution and the target.\n   250→\n   251→`scoringutils` re-exports the `logs_sample()` function from the `scoringRules` package, which assumes that the forecast is represented by a set of samples from the predictive distribution. One implications of this is that it is currently not advisable to use the log score for discrete forecasts. The reason for this is that `scoringRules::logs_sample()` estimates a predictive density from the samples, which can be problematic for discrete forecasts.\n   252→\n   253→See `?logs_sample()` for more information.\n   254→\n   255→\n   256→## Dawid-Sebastiani score\n   257→\n   258→**Observation**: $y$, a real number (or a discrete number).\n   259→\n   260→**Forecast**: $F$. The predictive distribution with mean $\\mu$ and standard deviation $\\sigma$.\n   261→\n   262→The Dawid-Sebastiani score is a proper scoring rule that only relies on the first moments of the predictive distribution and is therefore easy to compute. It is given as\n   263→\n   264→$$\\text{dss}(F, y) = \\left( \\frac{y - \\mu}{\\sigma} \\right)^2 + 2 \\cdot \\log \\sigma.$$\n   265→\n   266→`scoringutils` re-exports the implementation of the DSS from the `scoringRules` package. It assumes that the forecast is represented by a set of samples drawn from the predictive distribution. See `?dss_sample()` for more information.\n   267→\n   268→## Dispersion - Median Absolute Deviation (MAD)\n   269→\n   270→**Observation**: Not required.\n   271→\n   272→**Forecast**: $F$, the predictive distribution.\n   273→\n   274→Dispersion (also called sharpness) is the ability to produce narrow forecasts. It is a feature of the forecasts only and does not depend on the observations. Dispersion is therefore only of interest conditional on calibration: a very precise forecast is not useful if it is clearly wrong.\n   275→\n   276→One way to measure sharpness (as suggested by @funkAssessingPerformanceRealtime2019) is the normalised median absolute deviation about the median (MADN) ). It is computed as\n   277→  $$ S(F) = \\frac{1}{0.675} \\cdot \\text{median}(|F - \\text{median(F)}|). $$\n   278→If the forecast $F$ follows a normal distribution, then sharpness will equal the standard deviation of $F$.For more details, see `?mad_sample()`.\n   279→\n   280→## Bias\n   281→\n   282→**Observation**: $y$, a real number (or a discrete number).\n   283→\n   284→**Forecast**: A continuous ($F$) or discrete ($P$) forecast.\n   285→\n   286→\n   287→Bias is a measure of the tendency of a forecaster to over- or underpredict. For *continuous* forecasts, the `scoringutils` implementation calculates bias as\n   288→$$B(F, y) = 1 - 2 \\cdot F (y), $$\n   289→where $F(y)$ is the empirical cumulative distribution function of the forecast evaluated at the observed value $y$. To handle ties appropriately (which can occur when predictions equal observations for example due to rounding), the implementation uses mid-ranks: $F(y)$ is computed as the proportion of predictions strictly less than $y$ plus half the proportion of predictions equal to $y$.\n   290→\n   291→\n   292→For *discrete* forecasts, we calculate bias as\n   293→$$B(P, y) = 1 - (P(y) + P(y + 1)). $$\n   294→where $P(y)$ is the cumulative probability assigned to all outcomes smaller or equal to $y$, i.e. the cumulative probability mass function.\n   295→\n   296→Bias is bound between -1 and 1 and represents the tendency of forecasts to be biased rather than the absolute amount of over- and underprediction (which is e.g. the case for the weighted interval score (WIS, see below).\n   297→\n   298→## Absolute error of the median\n   299→\n   300→**Observation**: $y$, a real number (or a discrete number).\n   301→\n   302→**Forecast**: A forecast $F$\n   303→\n   304→$$\\text{ae}_{\\text{median}}(F, y) = |\\text{median} (F) - y|.$$\n   305→See section [A note of caution] or @gneitingMakingEvaluatingPoint2011 for a discussion on the correspondence between the absolute error and the median.\n   306→\n   307→## Squared error of the mean\n   308→\n   309→**Observation**: $y$, a real number (or a discrete number).\n   310→\n   311→**Forecast**: A forecast $F$\n   312→\n   313→$$\\text{se}_{\\text{medn}}(F, y) = (\\text{mean} (F) - y)^2.$$\n   314→See section [A note of caution] or @gneitingMakingEvaluatingPoint2011 for a discussion on the correspondence between the squared error and the mean.\n   315→\n   316→---\n   317→\n   318→# Quantile-based forecasts\n   319→\n   320→See a list of the default metrics for quantile-based forecasts by calling `get_metrics(example_quantile)`.\n   321→\n   322→This is an overview of the input and output formats for quantile forecasts:\n   323→\n   324→```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for quantile-based forecasts.\"}\n   325→knitr::include_graphics(file.path(\"scoring-rules\", \"input-quantile.png\"))\n   326→```\n   327→\n   328→## Weighted interval score (WIS)\n   329→\n   330→**Observation**: $y$, a real number\n   331→\n   332→**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles. These quantiles form the lower ($l$) and upper ($u$) bounds of central prediction intervals.\n   333→\n   334→The weighted interval score (WIS) is a strictly proper scoring rule and can be understood as an approximation of the CRPS for forecasts in a quantile format (which in turn represents a generalisation of the absolute error). Quantiles are assumed to be the lower and upper bounds of prediction intervals symmetric around the median. For a single interval, the interval score is\n   335→\n   336→$$IS_\\alpha(F,y) = \\underbrace{(u-l)}_\\text{dispersion} + \\underbrace{\\frac{2}{\\alpha} \\cdot (l-y) \\cdot \\mathbf{1}(y \\leq l)}_{\\text{overprediction}} + \\underbrace{\\frac{2}{\\alpha} \\cdot (y-u) \\cdot \\mathbf{1}(y \\geq u)}_{\\text{underprediction}}, $$\n   337→\n   338→where $\\mathbf{1}()$ is the indicator function, and $l$ and $u$ are the $\\frac{\\alpha}{2}$ and $1 - \\frac{\\alpha}{2}$ quantiles of the predictive distribution $F$. $l$ and $u$ together form the prediction interval. The interval score can be understood as the sum of three components: dispersion, overprediction and underprediction.\n   339→\n   340→For a set of $K$ prediction intervals and the median $m$, the score is given as a weighted sum of individual interval scores, i.e.\n   341→\n   342→$$WIS = \\frac{1}{K + 0.5} \\cdot \\left(w_0 \\cdot |y - m| + \\sum_{k = 1}^{K} w_k \\cdot IS_{\\alpha_{k}}(F, y)\\right),$$\n   343→where $m$ is the median forecast and $w_k$ is a weight assigned to every interval. When the weights are set to $w_k = \\frac{\\alpha_k}{2}$ and $w_0 = 0.5$, then the WIS converges to the CRPS for an increasing number of equally spaced quantiles.\n   344→\n   345→See `?wis()` for more information.\n   346→\n   347→### Overprediction, underprediction and dispersion\n   348→\n   349→These are the individual components of the WIS. See `?overprediction_quantile()`, `?underprediction_quantile()` and `?dispersion_quantile()` for more information.\n   350→\n   351→## Bias\n   352→\n   353→**Observation**: $y$, a real number\n   354→\n   355→**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles, $Q$.\n   356→\n   357→Bias can be measured as\n   358→\n   359→\\begin{equation}\n   360→    \\text{B}(F, y) =\n   361→    \\begin{cases}\n   362→        (1 - 2 \\cdot \\max \\{\\alpha | q_\\alpha \\in Q \\land q_\\alpha \\leq y\\}),       & \\text{if } y < q_{0.5} \\quad \\text{(overprediction)}\\\\\n   363→        (1 - 2 \\cdot \\min \\{\\alpha | q_\\alpha \\in Q_t \\land q_\\alpha \\geq y\\},   & \\text{if } y > q_{0.5} \\quad \\text{(underprediction)}\\\\\n   364→        0,   & \\text{if } y = q_{0.5}, \\\\\n   365→    \\end{cases}\n   366→\\end{equation}\n   367→\n   368→where $q_\\alpha$ is the $\\alpha$-quantile of the predictive distribution. For consistency, we define $Q$ (the set of quantiles that form the predictive distribution $F$) such that it always includes the element $q_0 = -\\infty$ and $q_1 = \\infty$. In clearer terms, bias is:\n   369→\n   370→- $1 - (2 \\times$ the maximum percentile rank for which the corresponding quantile is still below the observed value), *if the observed value is smaller than the median of the predictive distribution.*\n   371→- $1 - (2 \\times$ the minimum percentile rank for which the corresponding quantile is still larger than the observed value) *if the observed value is larger than the median of the predictive distribution.*.\n   372→- $0$ *if the observed value is exactly the median*.\n   373→\n   374→Bias can assume values between -1 (underprediction) and 1 (overprediction) and is 0 ideally (i.e. unbiased).\n   375→\n   376→For an increasing number of quantiles, the percentile rank will equal the proportion of predictive samples below the observed value, and the bias metric coincides with the one for continuous forecasts (see above).\n   377→\n   378→See `?bias_quantile()` for more information.\n   379→\n   380→## Interval coverage\n   381→\n   382→**Observation**: $y$, a real number\n   383→\n   384→**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles. These quantiles form central prediction intervals.\n   385→\n   386→Interval coverage for a given interval range is defined as the proportion of observations that fall within the corresponding central prediction intervals. Central prediction intervals are symmetric around the median and formed by two quantiles that denote the lower and upper bound. For example, the 50% central prediction interval is the interval between the 0.25 and 0.75 quantiles of the predictive distribution.\n   387→\n   388→### Interval coverage deviation\n   389→\n   390→The interval coverage deviation is the difference between the observed interval coverage and the nominal interval coverage. For example, if the observed interval coverage for the 50% central prediction interval is 0.6, then the interval coverage deviation is $0.6 - 0.5 = 0.1.$\n   391→\n   392→$$\\text{interval coverage deviation} = \\text{observed interval coverage} - \\text{nominal interval coverage}$$\n   393→\n   394→## Absolute error of the median\n   395→\n   396→**Observation**: $y$, a real number\n   397→\n   398→**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles.\n   399→\n   400→The absolute error of the median is the absolute difference between the median of the predictive distribution and the observed value.\n   401→\n   402→$$\\text{ae}_\\text{median} = |\\text{median}(F) - y|$$\n   403→See section [A note of caution] or @gneitingMakingEvaluatingPoint2011 for a discussion on the correspondence between the absolute error and the median.\n   404→\n   405→\n   406→## Quantile score\n   407→\n   408→**Observation**: $y$, a real number\n   409→\n   410→**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles.\n   411→\n   412→The quantile score, also called pinball loss, for a single quantile level $\\tau$ is defined as\n   413→\n   414→\\begin{equation}\n   415→    \\text{QS}_\\tau(F, y) = 2 \\cdot \\{ \\mathbf{1}(y \\leq q_\\tau) - \\tau\\} \\cdot (q_\\tau - y) =\n   416→    \\begin{cases}\n   417→        2 \\cdot (1 - \\tau) * q_\\tau - y,       & \\text{if } y \\leq q_\\tau\\\\\n   418→        2 \\cdot \\tau * |q_\\tau - y|,           & \\text{if } y > q_\\tau,\n   419→    \\end{cases}\n   420→\\end{equation}\n   421→\n   422→with $q_\\tau$ being the $\\tau$-quantile of the predictive distribution $F$, and $\\mathbf{1}(\\cdot)$ the indicator function.\n   423→\n   424→The (unweighted) interval score (see above) for a $1 - \\alpha$ prediction interval can be computed from the quantile scores at levels $\\alpha/2$ and $1 - \\alpha/2$ as\n   425→\n   426→$$\\text{IS}_\\alpha(F, y) = \\frac{\\text{QS}_{\\alpha/2}(F, y) + \\text{QS}_{1 - \\alpha/2}(F, y)}{\\alpha}$$.\n   427→\n   428→The weighted interval score can be obtained as a simple average of the quantile scores:\n   429→\n   430→$$\\text{WIS}_\\alpha(F, y) = \\frac{\\text{QS}_{\\alpha/2}(F, y) + \\text{QS}_{1 - \\alpha/2}(F, y)}{2}$$.\n   431→\n   432→See `?quantile_score` and @bracherEvaluatingEpidemicForecasts2021 for more details.\n   433→\n   434→\n   435→---\n   436→\n   437→# Additional metrics\n   438→\n   439→\n   440→## Quantile coverage\n   441→\n   442→Quantile coverage for a given quantile level is defined as the proportion of observed values that are smaller than the corresponding predictive quantiles. For example, the 0.5 quantile coverage is the proportion of observed values that are smaller than the 0.5-quantiles of the predictive distribution.\n   443→\n   444→<!-- ## Probability integral transform (PIT) -->\n   445→\n   446→\n   447→\n   448→\n   449→<!-- # Calibration -->\n   450→\n   451→<!-- Calibration or reliability of forecasts is the ability of a model to -->\n   452→<!-- correctly identify its own uncertainty in making predictions. In a model -->\n   453→<!-- with perfect calibration, the observed data at each time point look as if -->\n   454→<!-- they came from the predictive probability distribution at that time. -->\n   455→\n   456→<!-- Equivalently, one can inspect the probability integral transform of the -->\n   457→<!-- predictive distribution at time t, -->\n   458→\n   459→<!-- $$u_t = F_t (x_t)$$ -->\n   460→\n   461→<!-- where $x_t$ is the observed data point at time $t \\text{ in } t_1, …, t_n$, -->\n   462→<!-- n being the number of forecasts, and $F_t$ is the (continuous) predictive -->\n   463→<!-- cumulative probability distribution at time t. If the true probability -->\n   464→<!-- distribution of outcomes at time t is $G_t$ then the forecasts $F_t$ are -->\n   465→<!-- said to be ideal if $F_t = G_t$ at all times $t$. In that case, the -->\n   466→<!-- probabilities ut are distributed uniformly. -->\n   467→\n   468→<!-- In the case of discrete outcomes such as incidence counts, -->\n   469→<!-- the PIT is no longer uniform even when forecasts are ideal. -->\n   470→<!-- In that case a randomised PIT can be used instead: -->\n   471→\n   472→<!-- $$u_t = P_t(k_t) + v \\cdot (P_t(k_t) - P_t(k_t - 1) )$$ -->\n   473→\n   474→<!-- where $k_t$ is the observed count, $P_t(x)$ is the predictive -->\n   475→<!-- cumulative probability of observing incidence $k$ at time $t$, -->\n   476→<!-- $P_t (-1) = 0$ by definition and $v$ is standard uniform and independent -->\n   477→<!-- of $k$. If $P_t$ is the true cumulative -->\n   478→<!-- probability distribution, then $u_t$ is standard uniform. -->\n   479→\n   480→<!-- The function checks whether integer or continuous forecasts were provided. -->\n   481→<!-- It then applies the (randomised) probability integral and tests -->\n   482→<!-- the values $u_t$ for uniformity using the -->\n   483→<!-- Anderson-Darling test. -->\n   484→\n   485→<!-- As a rule of thumb, there is no evidence to suggest a forecasting model is -->\n   486→<!-- miscalibrated if the p-value found was greater than a threshold of $p >= 0.1$, -->\n   487→<!-- some evidence that it was miscalibrated if $0.01 < p < 0.1$, and good -->\n   488→<!-- evidence that it was miscalibrated if $p <= 0.01$. -->\n   489→<!-- In this context it should be noted, though, that uniformity of the -->\n   490→<!-- PIT is a necessary but not sufficient condition of calibration. It should -->\n   491→<!-- also be noted that the test only works given sufficient samples, otherwise the  -->\n   492→<!-- Null hypothesis will often be rejected outright.  -->\n   493→"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"1d65f213-9631-4068-be84-a87f49d4dd89","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/vignettes/scoring-rules.Rmd","content":"---\ntitle: \"Scoring rules in `scoringutils`\"\nauthor: \"Nikos Bosse\"\ndate: \"`r Sys.Date()`\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{Scoring rules in `scoringutils`}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\nbibliography: \"scoring-rules/scoringutils-package.bib\"\n---\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  fig.width = 7,\n  collapse = TRUE,\n  comment = \"#>\",\n  cache = FALSE\n)\nlibrary(scoringutils)\nlibrary(data.table)\n```\n\n# Introduction\n\nThis vignette gives an overview of the default scoring rules made available through the `scoringutils` package. You can, of course, also use your own scoring rules, provided they follow the same format. If you want to obtain more detailed information about how the package works, have a look at the [revised version](https://drive.google.com/file/d/1URaMsXmHJ1twpLpMl1sl2HW4lPuUycoj/view?usp=drive_link) of our `scoringutils` paper.\n\nWe can distinguish two types of forecasts: point forecasts and probabilistic forecasts. A point forecast is a single number representing a single outcome. A probabilistic forecast is a full predictive probability distribution over multiple possible outcomes. In contrast to point forecasts, probabilistic forecasts incorporate uncertainty about different possible outcomes.\n\nScoring rules are functions that take a forecast and an observation as input and return a single numeric value. For point forecasts, they take the form $S(\\hat{y}, y)$, where $\\hat{y}$ is the forecast and $y$ is the observation. For probabilistic forecasts, they usually take the form $S(F, y)$, where $F$ is the cumulative density function (CDF) of the predictive distribution and $y$ is the observation. By convention, scoring rules are usually negatively oriented, meaning that smaller values are better (the best possible score is usually zero). In that sense, the score can be understood as a penalty.\n\nMany scoring rules for probabilistic forecasts are so-called (strictly) proper scoring rules. Essentially, this means that they cannot be \"cheated\": A forecaster evaluated by a strictly proper scoring rule is always incentivised to report her honest best belief about the future and cannot, in expectation, improve her score by reporting something else. A more formal definition is the following: Let $G$ be the true, unobserved data-generating distribution. A scoring rule is said to be proper, if under $G$ and for an ideal forecast $F = G$, there is no forecast $F' \\neq F$ that in expectation receives a better score than $F$. A scoring rule is considered strictly proper if, under $G$, no other forecast $F'$ in expectation receives a score that is better than or the same as that of $F$.\n\n---\n\n# Metrics for point forecasts\n\nSee a list of the default metrics for point forecasts by calling `get_metrics(example_point)`.\n\nThis is an overview of the input and output formats for point forecasts:\n\n```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for point.\"}\nknitr::include_graphics(file.path(\"scoring-rules\", \"input-point.png\"))\n```\n\n\n## A note of caution\n\nScoring point forecasts can be tricky business. Depending on the choice of the scoring rule, a forecaster who is clearly worse than another, might consistently receive better scores (see @gneitingMakingEvaluatingPoint2011 for an illustrative example).\n\nEvery scoring rule for a point forecast is implicitly minimised by a specific aspect of the predictive distribution. The mean squared error, for example, is only a meaningful scoring rule if the forecaster actually reported the mean of their predictive distribution as a point forecast. If the forecaster reported the median, then the mean absolute error would be the appropriate scoring rule. If the scoring rule and the predictive task do not align, misleading results ensue. Consider the following example:\n\n```{r}\nset.seed(123)\nn <- 1000\nobserved <- rnorm(n, 5, 4)^2\n\npredicted_mu <- mean(observed)\npredicted_not_mu <- predicted_mu - rnorm(n, 10, 2)\n\nmean(Metrics::ae(observed, predicted_mu))\nmean(Metrics::ae(observed, predicted_not_mu))\n\nmean(Metrics::se(observed, predicted_mu))\nmean(Metrics::se(observed, predicted_not_mu))\n```\n\n\n\n## Absolute error\n\n**Observation**: $y$, a real number\n\n**Forecast**: $\\hat{y}$, a real number, the median of the forecaster's predictive distribution.\n\nThe absolute error is the absolute difference between the predicted and the observed values. See `?Metrics::ae`.\n\n$$\\text{ae} = |y - \\hat{y}|$$\n\nThe absolute error is only an appropriate rule if $\\hat{y}$ corresponds to the median of the forecaster's predictive distribution. Otherwise, results will be misleading (see @gneitingMakingEvaluatingPoint2011).\n\n## Squared error\n\n**Observation**: $y$, a real number\n\n**Forecast**: $\\hat{y}$, a real number, the mean of the forecaster's predictive distribution.\n\nThe squared error is the squared difference between the predicted and the observed values. See `?Metrics::se`.\n\n$$\\text{se} = (y - \\hat{y})^2$$\nThe squared error is only an appropriate rule if $\\hat{y}$ corresponds to the mean of the forecaster's predictive distribution. Otherwise, results will be misleading (see @gneitingMakingEvaluatingPoint2011).\n\n## Absolute percentage error\n\n**Observation**: $y$, a real number\n\n**Forecast**: $\\hat{y}$, a real number\n\nThe absolute percentage error is the absolute percent difference between the predicted and the observed values. See `?Metrics::ape`.\n\n$$\\text{ape} = \\frac{|y - \\hat{y}|}{|y|}$$\n\nThe absolute percentage error is only an appropriate rule if $\\hat{y}$ corresponds to the $\\beta$-median of the forecaster's predictive distribution with $\\beta = -1$. The $\\beta$-median, $\\text{med}^{(\\beta)}(F)$, is the median of a random variable whose density is proportional to $y^\\beta f(y)$. The specific $\\beta$-median that corresponds to the absolute percentage error is $\\text{med}^{(-1)}(F)$. Otherwise, results will be misleading (see @gneitingMakingEvaluatingPoint2011).\n\n---\n\n# Binary forecasts\n\nSee a list of the default metrics for point forecasts by calling `?get_metrics(example_binary)`.\n\nThis is an overview of the input and output formats for point forecasts:\n\n```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for binary forecasts.\"}\nknitr::include_graphics(file.path(\"scoring-rules\", \"input-binary.png\"))\n```\n\n\n## Brier score\n\n**Observation**: $y$, either 0 or 1\n\n**Forecast**: $p$, a probability that the observed outcome will be 1.\n\nThe Brier score is a strictly proper scoring rule. It is computed as the mean squared error between the probabilistic prediction and the observed outcome.\n\n\\begin{equation}\n    \\text{BS}(p, y) = (p - y)^2 =\n    \\begin{cases}\n        p^2,       & \\text{if } y = 1\\\\\n        (1 - p)^2,   & \\text{if } y = 0\n    \\end{cases}\n\\end{equation}\n\nThe Brier score and the logarithmic score (see below) differ in how they penalise over- and underconfidence (see @macheteContrastingProbabilisticScoring2012). The Brier score penalises overconfidence and underconfidence in probability space the same. Consider the following example:\n\n```{r}\nn <- 1e6\np_true <- 0.7\nobserved <- factor(rbinom(n = n, size = 1, prob = p_true), levels = c(0, 1))\n\np_over <- p_true + 0.15\np_under <- p_true - 0.15\n\nabs(mean(brier_score(observed, p_true)) - mean(brier_score(observed, p_over)))\nabs(mean(brier_score(observed, p_true)) - mean(brier_score(observed, p_under)))\n```\n\nSee `?brier_score()` for more information.\n\n\n## Logarithmic score\n\n**Observation**: $y$, either 0 or 1\n\n**Forecast**: $p$, a probability that the observed outcome will be 1.\n\nThe logarithmic score (or log score) is a strictly proper scoring rule. It is computed as the negative logarithm of the probability assigned to the observed outcome.\n\n\\begin{equation}\n    \\text{Log score}(p, y) = - \\log(1 - |y - p|) =\n    \\begin{cases}\n        -\\log (p),       & \\text{if } y = 1\\\\\n        -\\log (1 - p),   & \\text{if } y = 0\n    \\end{cases}\n\\end{equation}\n\nThe log score penalises overconfidence more strongly than underconfidence (in probability space). Consider the following example:\n\n```{r}\nabs(mean(logs_binary(observed, p_true)) - mean(logs_binary(observed, p_over)))\nabs(mean(logs_binary(observed, p_true)) - mean(logs_binary(observed, p_under)))\n```\n\nSee `?logs_binary()` for more information.\n\n\n---\n\n# Sample-based forecasts\n\nSee a list of the default metrics for sample-based forecasts by calling `get_metrics(example_sample_continuous)`.\n\nThis is an overview of the input and output formats for quantile forecasts:\n\n```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for sample-based forecasts.\"}\nknitr::include_graphics(file.path(\"scoring-rules\", \"input-sample.png\"))\n```\n\n\n## CRPS\n\n**Observation**: $y$, a real number (or a discrete number).\n\n**Forecast**: A continuous ($F$) or discrete ($P$) forecast.\n\nThe continuous ranked probability score (CRPS) is popular in fields such as meteorology and epidemiology. The CRPS is defined as\n$$\\text{CRPS}(F, y) = \\int_{-\\infty}^\\infty \\left( F(x) - 1(x \\geq y) \\right)^2 dx,$$\nwhere $y$ is the observed value and $F$ the CDF of predictive distribution.\n\nFor discrete forecasts, for example count data, the ranked probability score (RPS) can be used instead and is commonly defined as:\n$$ \\text{RPS}(P, y) = \\sum_{x = 0}^\\infty (P(x) - 1(x \\geq y))^2, $$\nwhere $P$ is the cumulative probability mass function (PMF) of the predictive distribution.\n\n\nThe CRPS can be understood as a generalisation of the absolute error to predictive distributions [@gneitingStrictlyProperScoring2007]. It can also be understood as the integral over the Brier score for the binary probability forecasts implied by the CDF for all possible observed values. The CRPS is also related to the Cramér-distance between two distributions and equals the special case where one of the distributions is concentrated in a single point (see e.g. @zielEnergyDistanceEnsemble2021). The CRPS is a global scoring rule, meaning that the entire predictive distribution is taken into account when determining the quality of the forecast.\n\n`scoringutils` re-exports the `crps_sample()` function from the `scoringRules` package, which assumes that the forecast is represented by a set of samples from the predictive distribution. See `?crps_sample()` for more information.\n\n\n### Overprediction, underprediction and dispersion\n\nThe CRPS can be interpreted as a sum of a dispersion, an overprediction and an underprediction component.\nIf $m$ is the median forecast then the dispersion component is\n$$\\text{CRPS}(F, m),$$\nthe overprediction component is\n$$\n\\begin{cases}\nm > y & CRPS(F, y) - CRPS(F, m)\\\\\nm \\leq y & 0\\\\\n\\end{cases}\n$$\nand the underprediction component is\n$$\n\\begin{cases}\nm < y & CRPS(F, y) - CRPS(F, m)\\\\\nm \\geq y & 0\\\\\n\\end{cases}\n$$\n\nThese can be accessed via the `dispersion_sample()`, `overprediction_sample()` and `underprediction_sample()` functions, respectively.\n\n## Log score\n\n**Observation**: $y$, a real number (or a discrete number).\n\n**Forecast**: A continuous ($F$) or discrete ($P$) forecast.\n\nThe logarithmic scoring rule is simply the negative logarithm of the density of the the predictive distribution evaluated at the observed value:\n\n$$ \\text{log score}(F, y) = -\\log f(y), $$\n\nwhere $f$ is the predictive probability density function (PDF) corresponding to the Forecast $F$ and $y$ is the observed value.\n\nFor discrete forecasts, the log score can be computed as\n\n$$ \\text{log score}(F, y) = -\\log p_y, $$\nwhere $p_y$ is the probability assigned to the observed outcome $y$ by the forecast $F$.\n\nThe logarithmic scoring rule can produce large penalties when the observed value takes on values for which $f(y)$ (or $p_y$) is close to zero. It is therefore considered to be sensitive to outlier forecasts. This may be desirable in some applications, but it also means that scores can easily be dominated by a few extreme values. The logarithmic scoring rule is a local scoring rule, meaning that the score only depends on the probability that was assigned to the actual outcome. This is often regarded as a desirable property for example in the context of Bayesian inference \\citep{winklerScoringRulesEvaluation1996}. It implies for example, that the ranking between forecasters would be invariant under monotone transformations of the predictive distribution and the target.\n\n`scoringutils` re-exports the `logs_sample()` function from the `scoringRules` package, which assumes that the forecast is represented by a set of samples from the predictive distribution. One implications of this is that it is currently not advisable to use the log score for discrete forecasts. The reason for this is that `scoringRules::logs_sample()` estimates a predictive density from the samples, which can be problematic for discrete forecasts.\n\nSee `?logs_sample()` for more information.\n\n\n## Dawid-Sebastiani score\n\n**Observation**: $y$, a real number (or a discrete number).\n\n**Forecast**: $F$. The predictive distribution with mean $\\mu$ and standard deviation $\\sigma$.\n\nThe Dawid-Sebastiani score is a proper scoring rule that only relies on the first moments of the predictive distribution and is therefore easy to compute. It is given as\n\n$$\\text{dss}(F, y) = \\left( \\frac{y - \\mu}{\\sigma} \\right)^2 + 2 \\cdot \\log \\sigma.$$\n\n`scoringutils` re-exports the implementation of the DSS from the `scoringRules` package. It assumes that the forecast is represented by a set of samples drawn from the predictive distribution. See `?dss_sample()` for more information.\n\n## Dispersion - Median Absolute Deviation (MAD)\n\n**Observation**: Not required.\n\n**Forecast**: $F$, the predictive distribution.\n\nDispersion (also called sharpness) is the ability to produce narrow forecasts. It is a feature of the forecasts only and does not depend on the observations. Dispersion is therefore only of interest conditional on calibration: a very precise forecast is not useful if it is clearly wrong.\n\nOne way to measure sharpness (as suggested by @funkAssessingPerformanceRealtime2019) is the normalised median absolute deviation about the median (MADN) ). It is computed as\n  $$ S(F) = \\frac{1}{0.675} \\cdot \\text{median}(|F - \\text{median(F)}|). $$\nIf the forecast $F$ follows a normal distribution, then sharpness will equal the standard deviation of $F$.For more details, see `?mad_sample()`.\n\n## Bias\n\n**Observation**: $y$, a real number (or a discrete number).\n\n**Forecast**: A continuous ($F$) or discrete ($P$) forecast.\n\n\nBias is a measure of the tendency of a forecaster to over- or underpredict. For *continuous* forecasts, the `scoringutils` implementation calculates bias as\n$$B(F, y) = 1 - 2 \\cdot F (y), $$\nwhere $F(y)$ is the empirical cumulative distribution function of the forecast evaluated at the observed value $y$. To handle ties appropriately (which can occur when predictions equal observations for example due to rounding), the implementation uses mid-ranks: $F(y)$ is computed as the proportion of predictions strictly less than $y$ plus half the proportion of predictions equal to $y$.\n\n\nFor *discrete* forecasts, we calculate bias as\n$$B(P, y) = 1 - (P(y) + P(y + 1)). $$\nwhere $P(y)$ is the cumulative probability assigned to all outcomes smaller or equal to $y$, i.e. the cumulative probability mass function.\n\nBias is bound between -1 and 1 and represents the tendency of forecasts to be biased rather than the absolute amount of over- and underprediction (which is e.g. the case for the weighted interval score (WIS, see below).\n\n## Absolute error of the median\n\n**Observation**: $y$, a real number (or a discrete number).\n\n**Forecast**: A forecast $F$\n\n$$\\text{ae}_{\\text{median}}(F, y) = |\\text{median} (F) - y|.$$\nSee section [A note of caution] or @gneitingMakingEvaluatingPoint2011 for a discussion on the correspondence between the absolute error and the median.\n\n## Squared error of the mean\n\n**Observation**: $y$, a real number (or a discrete number).\n\n**Forecast**: A forecast $F$\n\n$$\\text{se}_{\\text{medn}}(F, y) = (\\text{mean} (F) - y)^2.$$\nSee section [A note of caution] or @gneitingMakingEvaluatingPoint2011 for a discussion on the correspondence between the squared error and the mean.\n\n---\n\n# Quantile-based forecasts\n\nSee a list of the default metrics for quantile-based forecasts by calling `get_metrics(example_quantile)`.\n\nThis is an overview of the input and output formats for quantile forecasts:\n\n```{r, echo=FALSE, out.width=\"100%\", fig.cap=\"Input and output formats: metrics for quantile-based forecasts.\"}\nknitr::include_graphics(file.path(\"scoring-rules\", \"input-quantile.png\"))\n```\n\n## Weighted interval score (WIS)\n\n**Observation**: $y$, a real number\n\n**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles. These quantiles form the lower ($l$) and upper ($u$) bounds of central prediction intervals.\n\nThe weighted interval score (WIS) is a strictly proper scoring rule and can be understood as an approximation of the CRPS for forecasts in a quantile format (which in turn represents a generalisation of the absolute error). Quantiles are assumed to be the lower and upper bounds of prediction intervals symmetric around the median. For a single interval, the interval score is\n\n$$IS_\\alpha(F,y) = \\underbrace{(u-l)}_\\text{dispersion} + \\underbrace{\\frac{2}{\\alpha} \\cdot (l-y) \\cdot \\mathbf{1}(y \\leq l)}_{\\text{overprediction}} + \\underbrace{\\frac{2}{\\alpha} \\cdot (y-u) \\cdot \\mathbf{1}(y \\geq u)}_{\\text{underprediction}}, $$\n\nwhere $\\mathbf{1}()$ is the indicator function, and $l$ and $u$ are the $\\frac{\\alpha}{2}$ and $1 - \\frac{\\alpha}{2}$ quantiles of the predictive distribution $F$. $l$ and $u$ together form the prediction interval. The interval score can be understood as the sum of three components: dispersion, overprediction and underprediction.\n\nFor a set of $K$ prediction intervals and the median $m$, the score is given as a weighted sum of individual interval scores, i.e.\n\n$$WIS = \\frac{1}{K + 0.5} \\cdot \\left(w_0 \\cdot |y - m| + \\sum_{k = 1}^{K} w_k \\cdot IS_{\\alpha_{k}}(F, y)\\right),$$\nwhere $m$ is the median forecast and $w_k$ is a weight assigned to every interval. When the weights are set to $w_k = \\frac{\\alpha_k}{2}$ and $w_0 = 0.5$, then the WIS converges to the CRPS for an increasing number of equally spaced quantiles.\n\nSee `?wis()` for more information.\n\n### Overprediction, underprediction and dispersion\n\nThese are the individual components of the WIS. See `?overprediction_quantile()`, `?underprediction_quantile()` and `?dispersion_quantile()` for more information.\n\n## Bias\n\n**Observation**: $y$, a real number\n\n**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles, $Q$.\n\nBias can be measured as\n\n\\begin{equation}\n    \\text{B}(F, y) =\n    \\begin{cases}\n        (1 - 2 \\cdot \\max \\{\\alpha | q_\\alpha \\in Q \\land q_\\alpha \\leq y\\}),       & \\text{if } y < q_{0.5} \\quad \\text{(overprediction)}\\\\\n        (1 - 2 \\cdot \\min \\{\\alpha | q_\\alpha \\in Q_t \\land q_\\alpha \\geq y\\},   & \\text{if } y > q_{0.5} \\quad \\text{(underprediction)}\\\\\n        0,   & \\text{if } y = q_{0.5}, \\\\\n    \\end{cases}\n\\end{equation}\n\nwhere $q_\\alpha$ is the $\\alpha$-quantile of the predictive distribution. For consistency, we define $Q$ (the set of quantiles that form the predictive distribution $F$) such that it always includes the element $q_0 = -\\infty$ and $q_1 = \\infty$. In clearer terms, bias is:\n\n- $1 - (2 \\times$ the maximum percentile rank for which the corresponding quantile is still below the observed value), *if the observed value is smaller than the median of the predictive distribution.*\n- $1 - (2 \\times$ the minimum percentile rank for which the corresponding quantile is still larger than the observed value) *if the observed value is larger than the median of the predictive distribution.*.\n- $0$ *if the observed value is exactly the median*.\n\nBias can assume values between -1 (underprediction) and 1 (overprediction) and is 0 ideally (i.e. unbiased).\n\nFor an increasing number of quantiles, the percentile rank will equal the proportion of predictive samples below the observed value, and the bias metric coincides with the one for continuous forecasts (see above).\n\nSee `?bias_quantile()` for more information.\n\n## Interval coverage\n\n**Observation**: $y$, a real number\n\n**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles. These quantiles form central prediction intervals.\n\nInterval coverage for a given interval range is defined as the proportion of observations that fall within the corresponding central prediction intervals. Central prediction intervals are symmetric around the median and formed by two quantiles that denote the lower and upper bound. For example, the 50% central prediction interval is the interval between the 0.25 and 0.75 quantiles of the predictive distribution.\n\n### Interval coverage deviation\n\nThe interval coverage deviation is the difference between the observed interval coverage and the nominal interval coverage. For example, if the observed interval coverage for the 50% central prediction interval is 0.6, then the interval coverage deviation is $0.6 - 0.5 = 0.1.$\n\n$$\\text{interval coverage deviation} = \\text{observed interval coverage} - \\text{nominal interval coverage}$$\n\n## Absolute error of the median\n\n**Observation**: $y$, a real number\n\n**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles.\n\nThe absolute error of the median is the absolute difference between the median of the predictive distribution and the observed value.\n\n$$\\text{ae}_\\text{median} = |\\text{median}(F) - y|$$\nSee section [A note of caution] or @gneitingMakingEvaluatingPoint2011 for a discussion on the correspondence between the absolute error and the median.\n\n\n## Quantile score\n\n**Observation**: $y$, a real number\n\n**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles.\n\nThe quantile score, also called pinball loss, for a single quantile level $\\tau$ is defined as\n\n\\begin{equation}\n    \\text{QS}_\\tau(F, y) = 2 \\cdot \\{ \\mathbf{1}(y \\leq q_\\tau) - \\tau\\} \\cdot (q_\\tau - y) =\n    \\begin{cases}\n        2 \\cdot (1 - \\tau) * q_\\tau - y,       & \\text{if } y \\leq q_\\tau\\\\\n        2 \\cdot \\tau * |q_\\tau - y|,           & \\text{if } y > q_\\tau,\n    \\end{cases}\n\\end{equation}\n\nwith $q_\\tau$ being the $\\tau$-quantile of the predictive distribution $F$, and $\\mathbf{1}(\\cdot)$ the indicator function.\n\nThe (unweighted) interval score (see above) for a $1 - \\alpha$ prediction interval can be computed from the quantile scores at levels $\\alpha/2$ and $1 - \\alpha/2$ as\n\n$$\\text{IS}_\\alpha(F, y) = \\frac{\\text{QS}_{\\alpha/2}(F, y) + \\text{QS}_{1 - \\alpha/2}(F, y)}{\\alpha}$$.\n\nThe weighted interval score can be obtained as a simple average of the quantile scores:\n\n$$\\text{WIS}_\\alpha(F, y) = \\frac{\\text{QS}_{\\alpha/2}(F, y) + \\text{QS}_{1 - \\alpha/2}(F, y)}{2}$$.\n\nSee `?quantile_score` and @bracherEvaluatingEpidemicForecasts2021 for more details.\n\n\n---\n\n# Additional metrics\n\n\n## Quantile coverage\n\nQuantile coverage for a given quantile level is defined as the proportion of observed values that are smaller than the corresponding predictive quantiles. For example, the 0.5 quantile coverage is the proportion of observed values that are smaller than the 0.5-quantiles of the predictive distribution.\n\n<!-- ## Probability integral transform (PIT) -->\n\n\n\n\n<!-- # Calibration -->\n\n<!-- Calibration or reliability of forecasts is the ability of a model to -->\n<!-- correctly identify its own uncertainty in making predictions. In a model -->\n<!-- with perfect calibration, the observed data at each time point look as if -->\n<!-- they came from the predictive probability distribution at that time. -->\n\n<!-- Equivalently, one can inspect the probability integral transform of the -->\n<!-- predictive distribution at time t, -->\n\n<!-- $$u_t = F_t (x_t)$$ -->\n\n<!-- where $x_t$ is the observed data point at time $t \\text{ in } t_1, …, t_n$, -->\n<!-- n being the number of forecasts, and $F_t$ is the (continuous) predictive -->\n<!-- cumulative probability distribution at time t. If the true probability -->\n<!-- distribution of outcomes at time t is $G_t$ then the forecasts $F_t$ are -->\n<!-- said to be ideal if $F_t = G_t$ at all times $t$. In that case, the -->\n<!-- probabilities ut are distributed uniformly. -->\n\n<!-- In the case of discrete outcomes such as incidence counts, -->\n<!-- the PIT is no longer uniform even when forecasts are ideal. -->\n<!-- In that case a randomised PIT can be used instead: -->\n\n<!-- $$u_t = P_t(k_t) + v \\cdot (P_t(k_t) - P_t(k_t - 1) )$$ -->\n\n<!-- where $k_t$ is the observed count, $P_t(x)$ is the predictive -->\n<!-- cumulative probability of observing incidence $k$ at time $t$, -->\n<!-- $P_t (-1) = 0$ by definition and $v$ is standard uniform and independent -->\n<!-- of $k$. If $P_t$ is the true cumulative -->\n<!-- probability distribution, then $u_t$ is standard uniform. -->\n\n<!-- The function checks whether integer or continuous forecasts were provided. -->\n<!-- It then applies the (randomised) probability integral and tests -->\n<!-- the values $u_t$ for uniformity using the -->\n<!-- Anderson-Darling test. -->\n\n<!-- As a rule of thumb, there is no evidence to suggest a forecasting model is -->\n<!-- miscalibrated if the p-value found was greater than a threshold of $p >= 0.1$, -->\n<!-- some evidence that it was miscalibrated if $0.01 < p < 0.1$, and good -->\n<!-- evidence that it was miscalibrated if $p <= 0.01$. -->\n<!-- In this context it should be noted, though, that uniformity of the -->\n<!-- PIT is a necessary but not sufficient condition of calibration. It should -->\n<!-- also be noted that the test only works given sufficient samples, otherwise the  -->\n<!-- Null hypothesis will often be rejected outright.  -->\n","numLines":493,"startLine":1,"totalLines":493}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01ADh2B4jMDeutR8m6xvvM79","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014N7zrwAeDQ3dCpPAsDKU5u","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"completed","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"in_progress","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":12342,"cache_read_input_tokens":46481,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":12342},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"27470d7a-4ddc-4e20-9df6-bed1638c5406"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014N7zrwAeDQ3dCpPAsDKU5u","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"914b5caa-bc6f-4894-9e30-ce71b3a68ab3","tool_use_result":{"oldTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"in_progress","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"pending","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"completed","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"in_progress","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01DaaNfJjyhBjToVcg5tHnQy","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me understand the test landscape — this is a vignette cleanup issue, so I need to understand how vignette-related changes are tested and what tests exist."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":58823,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8d4e50e6-6871-4d58-8489-9e1f3b7c1e9c"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01DaaNfJjyhBjToVcg5tHnQy","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Jx9jrfgZSTWyZDJUbekchG","name":"Glob","input":{"pattern":"tests/testthat/test-*.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":58823,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"d67bfa5a-e135-4b9c-a145-3f3afe351049"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Jx9jrfgZSTWyZDJUbekchG","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-check-input-helpers.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-binary.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-multivariate-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-nominal.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-ordinal.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-point.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-quantile.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-scores.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-forecast-unit.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-correlations.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-duplicate-forecasts.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-forecast-counts.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-forecast-type.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-protected-columns.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-helper-quantile-interval-range.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-inputs-scoring-functions.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-binary.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-interval-range.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-nominal.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-ordinal.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-point.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-pairwise_comparison.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-plot_heatmap.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-plot_wis.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-score.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-summarise_scores.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-transform-forecasts.R"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"155e319d-f1b8-490a-8e45-06231c84bb2f","tool_use_result":{"filenames":["/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-check-input-helpers.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-binary.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-multivariate-sample.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-nominal.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-ordinal.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-point.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-quantile.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-sample.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-scores.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-forecast-unit.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-correlations.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-duplicate-forecasts.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-forecast-counts.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-forecast-type.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-protected-columns.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-helper-quantile-interval-range.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-inputs-scoring-functions.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-binary.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-interval-range.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-nominal.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-ordinal.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-point.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-sample.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-pairwise_comparison.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-plot_heatmap.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-plot_wis.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-score.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-summarise_scores.R","/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-transform-forecasts.R"],"durationMs":308,"numFiles":33,"truncated":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01DaaNfJjyhBjToVcg5tHnQy","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017vhRATsgniXGdjCcw82PNu","name":"Glob","input":{"pattern":"tests/testthat/helper-*.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":58823,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"56374ab4-502f-41a6-a314-882c7eb6bd59"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017vhRATsgniXGdjCcw82PNu","type":"tool_result","content":"No files found"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"513b2536-727b-4ddb-8b58-eff4848b246c","tool_use_result":{"filenames":[],"durationMs":275,"numFiles":0,"truncated":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01DaaNfJjyhBjToVcg5tHnQy","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FJto1DVN3MnUAjp3xhpqXV","name":"Glob","input":{"pattern":"tests/testthat/setup.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":280,"cache_read_input_tokens":58823,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":280},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"6c08ef1f-5b6d-4924-b639-e27c56baa610"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FJto1DVN3MnUAjp3xhpqXV","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"076b4a88-4e88-4b88-b3e7-6d748fafa122","tool_use_result":{"filenames":["/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R"],"durationMs":267,"numFiles":1,"truncated":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01AAgDQYFxr2sqLDt7mdn9ES","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01F2j9ybS8cwMxvF5J1Fpnb7","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1365,"cache_read_input_tokens":59103,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1365},"output_tokens":18,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"db31600b-8d85-47e9-b3cc-2ab73cc4cc5e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01F2j9ybS8cwMxvF5J1Fpnb7","type":"tool_result","content":"     1→data.table::setDTthreads(2) # restricts number of cores used on CRAN\n     2→\n     3→metrics_no_cov <- get_metrics(\n     4→  example_quantile,\n     5→  exclude = c(\"interval_coverage_50\", \"interval_coverage_90\")\n     6→)\n     7→metrics_no_cov_no_ae <- get_metrics(\n     8→  example_quantile,\n     9→  exclude = c(\n    10→    \"interval_coverage_50\", \"interval_coverage_90\",\n    11→    \"ae_median\"\n    12→  )\n    13→)\n    14→\n    15→example_quantile_df <- as.data.frame(na.omit(example_quantile))\n    16→checkmate::assert_number(length(class(example_quantile_df)))\n    17→\n    18→\n    19→# pre-computed scores\n    20→scores_quantile <- score(example_quantile)\n    21→scores_sample_continuous <- score(example_sample_continuous)\n    22→scores_sample_discrete <- score(example_sample_discrete)\n    23→scores_point <- score(example_point)\n    24→scores_binary <- score(example_binary)\n    25→scores_nominal <- score(example_nominal)\n    26→"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"ddf5397d-2daf-45c3-ab51-8ca1ef654c23","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R","content":"data.table::setDTthreads(2) # restricts number of cores used on CRAN\n\nmetrics_no_cov <- get_metrics(\n  example_quantile,\n  exclude = c(\"interval_coverage_50\", \"interval_coverage_90\")\n)\nmetrics_no_cov_no_ae <- get_metrics(\n  example_quantile,\n  exclude = c(\n    \"interval_coverage_50\", \"interval_coverage_90\",\n    \"ae_median\"\n  )\n)\n\nexample_quantile_df <- as.data.frame(na.omit(example_quantile))\ncheckmate::assert_number(length(class(example_quantile_df)))\n\n\n# pre-computed scores\nscores_quantile <- score(example_quantile)\nscores_sample_continuous <- score(example_sample_continuous)\nscores_sample_discrete <- score(example_sample_discrete)\nscores_point <- score(example_point)\nscores_binary <- score(example_binary)\nscores_nominal <- score(example_nominal)\n","numLines":26,"startLine":1,"totalLines":26}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Pcs4obXTfVfbX7mNQLcSZQ","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have the full picture. Let me analyze this issue carefully:\n\n**Issue #1044** is about deleting two HTML-commented-out code blocks from vignettes. This is a pure file content deletion task — removing commented-out code from `.Rmd` files. \n\nThe key question is: **what tests can meaningfully be written for this?** The fix is:\n1. Delete lines 88-100 from `scoring-multivariate-forecasts.Rmd` (WIP \"Details on the grouping\" HTML comment)\n2. Delete lines 444-493 from `scoring-rules.Rmd` (PIT/Calibration HTML comment block)\n\nSince this is a vignette cleanup (removing HTML comments from `.Rmd` files), the meaningful tests are:\n- Verify the commented blocks are gone (no HTML comments remain)\n- Verify the vignettes still render successfully after deletion\n- Verify surrounding content is not disturbed\n\nLet me check if there are any existing vignette-related tests."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":419,"cache_read_input_tokens":60468,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":419},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"7bee3d99-6a91-4186-b9c5-eb5d65b4a59f"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Pcs4obXTfVfbX7mNQLcSZQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UtGDWn33Xywmw19pYGg9AN","name":"Grep","input":{"pattern":"vignette|\\.Rmd","path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":419,"cache_read_input_tokens":60468,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":419},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"0eb85dc2-85bc-4ccb-94ae-a48844d3c14c"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UtGDWn33Xywmw19pYGg9AN","type":"tool_result","content":"No matches found"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"955a280e-b169-4290-9287-fc8eda41013b","tool_use_result":{"mode":"content","numFiles":0,"filenames":[],"content":"","numLines":0}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_018aQk64pu7eFjxBjxp6dv8k","type":"message","role":"assistant","content":[{"type":"text","text":"No vignette tests exist. This is a pure documentation cleanup issue — deleting HTML comments from two vignette files. There's no R code being changed, no functions affected, and no functional behavior to test.\n\nLet me think about this carefully. The triage table says `Has Tests: [ ]` for this issue, but the action is to delete HTML comment blocks from vignettes. There's nothing testable in the testthat sense — no R functions change, no behavior changes. The only \"test\" would be to verify the vignettes render, but that's not a unit test pattern used in this project.\n\nLet me update the triage and issue file accordingly."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":364,"cache_read_input_tokens":60887,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":364},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"619c787f-33b5-4bbc-9925-adfc67bd6338"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_018aQk64pu7eFjxBjxp6dv8k","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WCkwH6b8sJMu2gFfe3o8tM","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"completed","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"in_progress","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":364,"cache_read_input_tokens":60887,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":364},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8bd57070-2cda-4f56-b90e-def88ddc92f8"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WCkwH6b8sJMu2gFfe3o8tM","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"78eb92aa-3dbd-4c90-b035-fa32644cf87a","tool_use_result":{"oldTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"completed","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"in_progress","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"completed","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"in_progress","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_016gQ48VRymng9V1j3HNz8LJ","type":"message","role":"assistant","content":[{"type":"text","text":"Wait — I should re-examine the triage table. The Has Tests column for #1044 is `N/A` would be more appropriate since this is a documentation-only change with no testable R code. But the instructions say it's `[ ]`. Let me re-read the row carefully.\n\nLooking at row 29 for issue #1044:\n- **Automatable**: `yes`\n- **Has Tests**: `N/A`\n\nWait, let me re-read more carefully..."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":425,"cache_read_input_tokens":61251,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":425},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"6c3de539-aae4-47bc-99bb-db36596c86d1"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_016gQ48VRymng9V1j3HNz8LJ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FbyHcYWnsK92WByx8Pz4hQ","name":"Grep","input":{"pattern":"1044\"","path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":425,"cache_read_input_tokens":61251,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":425},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"46abbfa5-61ad-4673-a2d4-3cd272525842"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FbyHcYWnsK92WByx8Pz4hQ","type":"tool_result","content":"No matches found"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"0eb7a6eb-b182-483b-8256-fe7b6171330a","tool_use_result":{"mode":"content","numFiles":0,"filenames":[],"content":"","numLines":0}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017eVmgfiMR1G8u19GiWBboi","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GWZ7DKx72HUHrLpoxHUeRv","name":"Grep","input":{"pattern":"1044","path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":242,"cache_read_input_tokens":61676,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":242},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"19ac89cc-415e-4b00-94bf-0abba9da99e1"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GWZ7DKx72HUHrLpoxHUeRv","type":"tool_result","content":"29:[Omitted long matching line]"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"9bb143c1-f363-41f6-b020-1768346007bc","tool_use_result":{"mode":"content","numFiles":0,"filenames":[],"content":"29:[Omitted long matching line]","numLines":1}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_016fd1cNcGapdxNiwdoupwZc","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01K8nNFEyBnxF9FebkQE4xzU","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","offset":29,"limit":1}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":136,"cache_read_input_tokens":61918,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":136},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"89af3274-5ae2-4944-8d36-d0102797e6a2"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01K8nNFEyBnxF9FebkQE4xzU","type":"tool_result","content":"    29→| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"0ca14cd4-26d6-4bcb-9afb-8c1d99d015e2","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","content":"| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |","numLines":1,"startLine":29,"totalLines":131}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01TjKnx9oXBiCpWo66W2G7ny","type":"message","role":"assistant","content":[{"type":"text","text":"The **Has Tests** column for #1044 is `N/A`. This issue doesn't need tests — it's a documentation-only cleanup. So I need to skip this issue and find the next one.\n\nLet me re-scan for the first row where **Automatable** is `yes` AND **Has Tests** is `[ ]` (unchecked, not `N/A`).\n\nLooking through the table:\n- #1044: Automatable=yes, Has Tests=**N/A** → skip\n- #1043: Automatable=yes, Has Tests=**[x]** → already done\n- #1022: Automatable=yes, Has Tests=**[x]** → already done\n- #1002: Automatable=yes, Has Tests=**[x]** → already done\n- #983: Automatable=yes, Has Tests=**[ ]** → **this is the one!**\n\nIssue **#983: Create `.data.frame` methods for `as_forecast<type>`**\n\nLet me get the full context."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":326,"cache_read_input_tokens":62054,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":326},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"73e1cf2b-a8e3-494b-83ed-6a100fbca703"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01TjKnx9oXBiCpWo66W2G7ny","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DwqDBP8R8arPYcHXx48yVr","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"in_progress","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"pending","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":326,"cache_read_input_tokens":62054,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":326},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"c8219c11-8e73-4ea2-9764-79344517dbc7"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DwqDBP8R8arPYcHXx48yVr","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"ba620e26-1043-41e2-b82f-35ab02db02ac","tool_use_result":{"oldTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #1044 (remove commented vignette code)","status":"completed","activeForm":"Reading issue #1044 context"},{"content":"Understand the test landscape","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #1044","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"in_progress","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"in_progress","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"pending","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01M4z1x21EYn9VzYSoU8ucbt","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EmeoqrDqdhzpQXBRQqLcMx","name":"Bash","input":{"command":"gh issue view 983 --comments","description":"View issue #983 with comments"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":523,"cache_read_input_tokens":62380,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":523},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"a167b24a-2cd7-4962-9982-b7c13e5ecfae"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EmeoqrDqdhzpQXBRQqLcMx","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"77a5023a-9aa0-43be-88f7-6067413bd08a","tool_use_result":{"stdout":"","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_0114xZcTZWpykHBnZQiPAFu3","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Gojch92PdwkDgJLx9ShAJQ","name":"Bash","input":{"command":"gh issue view 983 --comments 2>&1 || gh issue view 983 2>&1","description":"View issue #983"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":108,"cache_read_input_tokens":62903,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":108},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"d8e73564-853b-4e23-b2be-f7379c7fdc1f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Gojch92PdwkDgJLx9ShAJQ","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"bb09c1ad-8898-4e5b-8631-c3240c46e2f3","tool_use_result":{"stdout":"","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_016a7TgDCpCYRGDDjxWiSMjJ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017w86PGCmWE1X9TExeqhnaM","name":"Bash","input":{"command":"gh issue view 983 2>&1; echo \"EXIT: $?\"","description":"View issue #983 with exit code"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":120,"cache_read_input_tokens":63011,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":120},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"7ab1249d-9d7a-4c82-af98-498446bc343d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017w86PGCmWE1X9TExeqhnaM","type":"tool_result","content":"title:\tCreate `.data.frame` methods for `as_forecast<type>`\nstate:\tOPEN\nauthor:\tnikosbosse\nlabels:\t\ncomments:\t0\nassignees:\t\nprojects:\t\nmilestone:\t\nnumber:\t983\n--\nWe only have `default` methods, but it would probably be cleaner to use `.data.frame` in addition to that\nEXIT: 0","is_error":false}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"c6c17cce-955f-4b1a-a3ea-2e6d07c08675","tool_use_result":{"stdout":"title:\tCreate `.data.frame` methods for `as_forecast<type>`\nstate:\tOPEN\nauthor:\tnikosbosse\nlabels:\t\ncomments:\t0\nassignees:\t\nprojects:\t\nmilestone:\t\nnumber:\t983\n--\nWe only have `default` methods, but it would probably be cleaner to use `.data.frame` in addition to that\nEXIT: 0","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01H33bSbfdzNy6H1wkEGCRuE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FFiVTxcUbkx3mmyfbGU3wD","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"in_progress","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":196,"cache_read_input_tokens":63131,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":196},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"1e0d0fac-1f9c-4e29-a855-88d80771a5d9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FFiVTxcUbkx3mmyfbGU3wD","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"42ef74a7-13ed-4b43-9fc5-a8e7414f43d7","tool_use_result":{"oldTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"in_progress","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"pending","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"in_progress","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01WNZBFZ45tvbrnV3V4w4GdN","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me understand the current code structure — the `as_forecast_*` functions, their `.default` methods, and the existing tests."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":282,"cache_read_input_tokens":63327,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":282},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"ef651e07-d0d0-4299-b7e6-7fba2ed51979"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01WNZBFZ45tvbrnV3V4w4GdN","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","name":"Task","input":{"description":"Explore as_forecast S3 methods","prompt":"I need to understand the as_forecast_* S3 methods in this R package for issue #983 which asks to create `.data.frame` methods for `as_forecast<type>`.\n\nPlease do the following research:\n\n1. Find all `as_forecast_*` generic definitions and their `.default` methods in the R/ directory. List each generic and its default method's file + line number.\n\n2. For each of the 7 forecast types, identify:\n   - The generic definition (e.g., `as_forecast_binary <- function(...)`)\n   - The `.default` method (e.g., `as_forecast_binary.default <- function(...)`)\n   - What the `.default` method does - does it assume data.frame-like input? Does it call `as.data.table()`?\n\n3. Look at the existing test files for these classes:\n   - `tests/testthat/test-class-forecast-binary.R`\n   - `tests/testthat/test-class-forecast-point.R`  \n   - `tests/testthat/test-class-forecast-quantile.R`\n   - `tests/testthat/test-class-forecast-sample.R`\n   - `tests/testthat/test-class-forecast-nominal.R`\n   - `tests/testthat/test-class-forecast-ordinal.R`\n   - `tests/testthat/test-class-forecast-multivariate-sample.R`\n   \n   For each, find any tests that call `as_forecast_*()` and note what inputs are used (data.table, data.frame, tibble, etc.) and what the tests verify.\n\n4. Check if `as_forecast_generic()` exists (the triage mentions it's internal) and understand its role.\n\n5. Look at the NAMESPACE file to see what's exported.\n\nReturn comprehensive findings.","subagent_type":"Explore"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":282,"cache_read_input_tokens":63327,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":282},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"46cc6861-bbaf-4f98-bd65-9a932ebc8035"}
{"type":"user","message":{"role":"user","content":[{"type":"text","text":"I need to understand the as_forecast_* S3 methods in this R package for issue #983 which asks to create `.data.frame` methods for `as_forecast<type>`.\n\nPlease do the following research:\n\n1. Find all `as_forecast_*` generic definitions and their `.default` methods in the R/ directory. List each generic and its default method's file + line number.\n\n2. For each of the 7 forecast types, identify:\n   - The generic definition (e.g., `as_forecast_binary <- function(...)`)\n   - The `.default` method (e.g., `as_forecast_binary.default <- function(...)`)\n   - What the `.default` method does - does it assume data.frame-like input? Does it call `as.data.table()`?\n\n3. Look at the existing test files for these classes:\n   - `tests/testthat/test-class-forecast-binary.R`\n   - `tests/testthat/test-class-forecast-point.R`  \n   - `tests/testthat/test-class-forecast-quantile.R`\n   - `tests/testthat/test-class-forecast-sample.R`\n   - `tests/testthat/test-class-forecast-nominal.R`\n   - `tests/testthat/test-class-forecast-ordinal.R`\n   - `tests/testthat/test-class-forecast-multivariate-sample.R`\n   \n   For each, find any tests that call `as_forecast_*()` and note what inputs are used (data.table, data.frame, tibble, etc.) and what the tests verify.\n\n4. Check if `as_forecast_generic()` exists (the triage mentions it's internal) and understand its role.\n\n5. Look at the NAMESPACE file to see what's exported.\n\nReturn comprehensive findings."}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8db1751f-9807-4468-a71d-46f2ba7ca9a9"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_016ZNLZM7iJSfD8mu8MWVBie","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01G3magzTQCEFd3nmfiCzUup","name":"Bash","input":{"command":"find /Users/nikos/github-synced/scoringutils-pipeline/R -type f -name \"*.R\" | head -20"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1842,"cache_read_input_tokens":9234,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1842},"output_tokens":6,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"f2ee15a9-8fb3-4bba-97eb-aff6789dbdde"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01G3magzTQCEFd3nmfiCzUup","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/R/documentation-templates.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-ordinal.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/plot-heatmap.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-forecast-type.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-duplicate-forecasts.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/score.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-correlations.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/summarise_scores.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-binary.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-point.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-interval-range.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/transform-forecasts.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-binary.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-nominal.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/forecast-unit.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-quantile.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-pit-histogram.R","is_error":false}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"2cb93532-e8db-464a-aa85-7080f841aece"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013VsHXQdn5PgrrxY4z1z7Ta","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011adKeBsHMQ6WPuDV6k8ckn","name":"Grep","input":{"pattern":"^as_forecast_\\w+\\s*(?:<<-|-|=)\\s*function","path":"/Users/nikos/github-synced/scoringutils-pipeline/R","output_mode":"files_with_matches"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":681,"cache_read_input_tokens":11076,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":681},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"2383a44f-32c3-4707-8f9d-a99029811133"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011adKeBsHMQ6WPuDV6k8ckn","type":"tool_result","content":"No files found"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"00f43146-31e0-453f-86cc-4c6da9d5d98a"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013VsHXQdn5PgrrxY4z1z7Ta","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013uUyRbNui8y6ZZzjG6awrt","name":"Bash","input":{"command":"find /Users/nikos/github-synced/scoringutils-pipeline/R -name \"class-forecast*.R\" -type f"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":681,"cache_read_input_tokens":11076,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":681},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"7dcc0db8-f5cb-444a-a768-2e308280bebc"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013uUyRbNui8y6ZZzjG6awrt","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-binary.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-quantile.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-multivariate-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-ordinal.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-point.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-nominal.R","is_error":false}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"64878edd-2cdb-4e16-bd04-a38f8bf3d645"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014w33BhJEaAWg7UfYZtUs8s","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-binary.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"c6099833-6cdb-4517-add1-180d2ac82425"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014w33BhJEaAWg7UfYZtUs8s","type":"tool_result","content":"     1→#' @title Create a `forecast` object for binary forecasts\n     2→#' @inherit as_forecast_doc_template params description\n     3→#' @details\n     4→#' # Target format\n     5→#'\n     6→#' The input for all further scoring needs to be a data.frame or similar with\n     7→#' the following columns:\n     8→#' - `observed`: `factor` with exactly two levels representing the observed\n     9→#'   values. The highest factor level is assumed to be the reference level.\n    10→#'   This means that corresponding value in `predicted` represent the\n    11→#'   probability that the observed value is equal to the highest factor level.\n    12→#' - `predicted`: `numeric` with predicted probabilities, representing\n    13→#'   the probability that the corresponding value in `observed` is equal to\n    14→#'   the highest available factor level.\n    15→#'\n    16→#' For convenience, we recommend an additional column `model` holding the name\n    17→#' of the forecaster or model that produced a prediction, but this is not\n    18→#' strictly necessary.\n    19→#'\n    20→#' See the [example_binary] data set for an example.\n    21→#' @inheritSection forecast_types Forecast unit\n    22→#' @param ... Unused\n    23→#' @returns A `forecast` object of class `forecast_binary`\n    24→#' @family functions to create forecast objects\n    25→#' @export\n    26→#' @keywords as_forecast transform\n    27→#' @examples\n    28→#' as_forecast_binary(\n    29→#'   example_binary,\n    30→#'   predicted = \"predicted\",\n    31→#'   forecast_unit = c(\"model\", \"target_type\", \"target_end_date\",\n    32→#'                     \"horizon\", \"location\")\n    33→#' )\n    34→as_forecast_binary <- function(data, ...) {\n    35→  UseMethod(\"as_forecast_binary\")\n    36→}\n    37→\n    38→#' @rdname as_forecast_binary\n    39→#' @export\n    40→#' @method as_forecast_binary default\n    41→#' @importFrom cli cli_warn\n    42→as_forecast_binary.default <- function(data,\n    43→                                       forecast_unit = NULL,\n    44→                                       observed = NULL,\n    45→                                       predicted = NULL,\n    46→                                       ...) {\n    47→  data <- as_forecast_generic(\n    48→    data,\n    49→    forecast_unit,\n    50→    observed = observed,\n    51→    predicted = predicted\n    52→  )\n    53→  data <- new_forecast(data, \"forecast_binary\")\n    54→  assert_forecast(data)\n    55→  return(data)\n    56→}\n    57→\n    58→#' @export\n    59→#' @rdname assert_forecast\n    60→#' @importFrom cli cli_abort\n    61→#' @keywords validate-forecast-object\n    62→assert_forecast.forecast_binary <- function(\n    63→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    64→) {\n    65→  forecast <- assert_forecast_generic(forecast, verbose)\n    66→  assert_forecast_type(forecast, actual = \"binary\", desired = forecast_type)\n    67→\n    68→  columns_correct <- test_columns_not_present(\n    69→    forecast, c(\"sample_id\", \"quantile_level\")\n    70→  )\n    71→  if (!columns_correct) {\n    72→    #nolint start: keyword_quote_linter\n    73→    cli_abort(\n    74→      c(\n    75→        \"!\" = \"Checking `forecast`: Input looks like a binary forecast, but an\n    76→         additional column called `sample_id` or `quantile` was found.\",\n    77→        \"i\" = \"Please remove the column.\"\n    78→      )\n    79→    )\n    80→  }\n    81→  input_check <- check_input_binary(forecast$observed, forecast$predicted)\n    82→  if (!isTRUE(input_check)) {\n    83→    cli_abort(\n    84→      c(\n    85→        \"!\" = \"Checking `forecast`: Input looks like a binary forecast, but\n    86→             found the following issue: {input_check}\"\n    87→      )\n    88→    )\n    89→    #nolint end\n    90→  }\n    91→  return(invisible(NULL))\n    92→}\n    93→\n    94→\n    95→#' @export\n    96→#' @rdname is_forecast\n    97→is_forecast_binary <- function(x) {\n    98→  inherits(x, \"forecast_binary\") && inherits(x, \"forecast\")\n    99→}\n   100→\n   101→\n   102→#' @importFrom stats na.omit\n   103→#' @importFrom data.table setattr copy\n   104→#' @rdname score\n   105→#' @export\n   106→score.forecast_binary <- function(forecast, metrics = get_metrics(forecast), ...) {\n   107→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   108→  metrics <- validate_metrics(metrics)\n   109→  forecast <- as.data.table(forecast)\n   110→\n   111→  scores <- apply_metrics(\n   112→    forecast, metrics,\n   113→    forecast$observed, forecast$predicted\n   114→  )\n   115→  scores[, `:=`(predicted = NULL, observed = NULL)]\n   116→\n   117→  scores <- as_scores(scores, metrics = names(metrics))\n   118→  return(scores[])\n   119→}\n   120→\n   121→\n   122→#' Get default metrics for binary forecasts\n   123→#'\n   124→#' @description\n   125→#' For binary forecasts, the default scoring rules are:\n   126→#' - \"brier_score\" = [brier_score()]\n   127→#' - \"log_score\" = [logs_binary()]\n   128→#' @inheritSection illustration-input-metric-binary-point Input format\n   129→#' @param x A forecast object (a validated data.table with predicted and\n   130→#'   observed values, see [as_forecast_binary()]).\n   131→#' @param select A character vector of scoring rules to select from the list. If\n   132→#'   `select` is `NULL` (the default), all possible scoring rules are returned.\n   133→#' @param exclude A character vector of scoring rules to exclude from the list.\n   134→#'   If `select` is not `NULL`, this argument is ignored.\n   135→#' @param ... unused\n   136→#' @returns A list of scoring functions.\n   137→#' @export\n   138→#' @family get_metrics functions\n   139→#' @keywords handle-metrics\n   140→#' @examples\n   141→#' get_metrics(example_binary)\n   142→#' get_metrics(example_binary, select = \"brier_score\")\n   143→#' get_metrics(example_binary, exclude = \"log_score\")\n   144→get_metrics.forecast_binary <- function(x, select = NULL, exclude = NULL, ...) {\n   145→  all <- list(\n   146→    brier_score = brier_score,\n   147→    log_score = logs_binary\n   148→  )\n   149→  select_metrics(all, select, exclude)\n   150→}\n   151→\n   152→\n   153→#' Binary forecast example data\n   154→#'\n   155→#' A data set with binary predictions for COVID-19 cases and deaths constructed\n   156→#' from data submitted to the European Forecast Hub.\n   157→#'\n   158→#' Predictions in the data set were constructed based on the continuous example\n   159→#' data by looking at the number of samples below the mean prediction.\n   160→#' The outcome was constructed as whether or not the actually\n   161→#' observed value was below or above that mean prediction.\n   162→#' This should not be understood as sound statistical practice, but rather\n   163→#' as a practical way to create an example data set.\n   164→#'\n   165→#' The data was created using the script create-example-data.R in the inst/\n   166→#' folder (or the top level folder in a compiled package).\n   167→#'\n   168→#' @format An object of class `forecast_binary` (see [as_forecast_binary()])\n   169→#' with the following columns:\n   170→#' \\describe{\n   171→#'   \\item{location}{the country for which a prediction was made}\n   172→#'   \\item{location_name}{name of the country for which a prediction was made}\n   173→#'   \\item{target_end_date}{the date for which a prediction was made}\n   174→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   175→#'   \\item{observed}{A factor with observed values}\n   176→#'   \\item{forecast_date}{the date on which a prediction was made}\n   177→#'   \\item{model}{name of the model that generated the forecasts}\n   178→#'   \\item{horizon}{forecast horizon in weeks}\n   179→#'   \\item{predicted}{predicted value}\n   180→#' }\n   181→# nolint start\n   182→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   183→# nolint end\n   184→\"example_binary\"\n   185→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"24836637-2286-4dba-bc99-7baf21bb1b0a"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01P6DRxzZqb8jh4uenBxZWsX","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-point.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"1b430c2d-14e1-419a-8ee9-7bf0a051fce4"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01P6DRxzZqb8jh4uenBxZWsX","type":"tool_result","content":"     1→#' @title Create a `forecast` object for point forecasts\n     2→#' @inherit as_forecast_doc_template params description\n     3→#' @details\n     4→#' # Target format\n     5→#'\n     6→#' The input for all further scoring needs to be a data.frame or similar with\n     7→#' the following columns:\n     8→#' - `observed`: Column of type `numeric` with observed values.\n     9→#' - `predicted`: Column of type `numeric` with predicted values.\n    10→#'\n    11→#' For convenience, we recommend an additional column `model` holding the name\n    12→#' of the forecaster or model that produced a prediction, but this is not\n    13→#' strictly necessary.\n    14→#'\n    15→#' See the [example_point] data set for an example.\n    16→#' @param ... Unused\n    17→#' @returns A `forecast` object of class `forecast_point`\n    18→#' @family functions to create forecast objects\n    19→#' @export\n    20→#' @keywords as_forecast transform\n    21→as_forecast_point <- function(data, ...) {\n    22→  UseMethod(\"as_forecast_point\")\n    23→}\n    24→\n    25→\n    26→#' @rdname as_forecast_point\n    27→#' @export\n    28→#' @method as_forecast_point default\n    29→#' @importFrom cli cli_warn\n    30→as_forecast_point.default <- function(data,\n    31→                                      forecast_unit = NULL,\n    32→                                      observed = NULL,\n    33→                                      predicted = NULL,\n    34→                                      ...) {\n    35→  data <- as_forecast_generic(\n    36→    data,\n    37→    forecast_unit,\n    38→    observed = observed,\n    39→    predicted = predicted\n    40→  )\n    41→  data <- new_forecast(data, \"forecast_point\")\n    42→  assert_forecast(data)\n    43→  return(data)\n    44→}\n    45→\n    46→\n    47→#' @export\n    48→#' @rdname assert_forecast\n    49→#' @importFrom cli cli_abort\n    50→#' @keywords validate-forecast-object\n    51→assert_forecast.forecast_point <- function(\n    52→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    53→) {\n    54→  forecast <- assert_forecast_generic(forecast, verbose)\n    55→  assert_forecast_type(forecast, actual = \"point\", desired = forecast_type)\n    56→  #nolint start: keyword_quote_linter object_usage_linter\n    57→  input_check <- check_input_point(forecast$observed, forecast$predicted)\n    58→  if (!isTRUE(input_check)) {\n    59→    cli_abort(\n    60→      c(\n    61→        \"!\" = \"Checking `forecast`: Input looks like a point forecast, but found\n    62→        the following issue: {input_check}\"\n    63→      )\n    64→    )\n    65→    #nolint end\n    66→  }\n    67→  return(invisible(NULL))\n    68→}\n    69→\n    70→\n    71→#' @export\n    72→#' @rdname is_forecast\n    73→is_forecast_point <- function(x) {\n    74→  inherits(x, \"forecast_point\") && inherits(x, \"forecast\")\n    75→}\n    76→\n    77→\n    78→#' @importFrom Metrics se ae ape\n    79→#' @importFrom stats na.omit\n    80→#' @importFrom data.table setattr copy\n    81→#' @rdname score\n    82→#' @export\n    83→score.forecast_point <- function(forecast, metrics = get_metrics(forecast), ...) {\n    84→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n    85→  metrics <- validate_metrics(metrics)\n    86→  forecast <- as.data.table(forecast)\n    87→\n    88→  scores <- apply_metrics(\n    89→    forecast, metrics,\n    90→    forecast$observed, forecast$predicted\n    91→  )\n    92→  scores[, `:=`(predicted = NULL, observed = NULL)]\n    93→\n    94→  scores <- as_scores(scores, metrics = names(metrics))\n    95→  return(scores[])\n    96→}\n    97→\n    98→\n    99→#' Get default metrics for point forecasts\n   100→#'\n   101→#' @description\n   102→#' For point forecasts, the default scoring rules are:\n   103→#' - \"ae_point\" = [ae()][Metrics::ae()]\n   104→#' - \"se_point\" = [se()][Metrics::se()]\n   105→#' - \"ape\" = [ape()][Metrics::ape()]\n   106→#'\n   107→#' A note of caution: Every scoring rule for a point forecast\n   108→#' is implicitly minimised by a specific aspect of the predictive distribution\n   109→#' (see Gneiting, 2011).\n   110→#'\n   111→#' The mean squared error, for example, is only a meaningful scoring rule if\n   112→#' the forecaster actually reported the mean of their predictive distribution\n   113→#' as a point forecast. If the forecaster reported the median, then the mean\n   114→#' absolute error would be the appropriate scoring rule. If the scoring rule\n   115→#' and the predictive task do not align, the results will be misleading.\n   116→#'\n   117→#' Failure to respect this correspondence can lead to grossly misleading\n   118→#' results! Consider the example in the section below.\n   119→#' @inheritSection illustration-input-metric-binary-point Input format\n   120→#' @inheritParams get_metrics.forecast_binary\n   121→#' @export\n   122→#' @family get_metrics functions\n   123→#' @keywords handle-metrics\n   124→#' @examples\n   125→#' get_metrics(example_point, select = \"ape\")\n   126→#'\n   127→#' set.seed(123)\n   128→#' n <- 500\n   129→#' observed <- rnorm(n, 5, 4)^2\n   130→#'\n   131→#' predicted_mu <- mean(observed)\n   132→#' predicted_not_mu <- predicted_mu - rnorm(n, 10, 2)\n   133→#'\n   134→#' df <- data.frame(\n   135→#'   model = rep(c(\"perfect\", \"bad\"), each = n),\n   136→#'   predicted = c(rep(predicted_mu, n), predicted_not_mu),\n   137→#'   observed = rep(observed, 2),\n   138→#'   id = rep(1:n, 2)\n   139→#' ) |>\n   140→#'   as_forecast_point()\n   141→#' score(df) |>\n   142→#'   summarise_scores()\n   143→#' @references\n   144→#' Making and Evaluating Point Forecasts, Gneiting, Tilmann, 2011,\n   145→#' Journal of the American Statistical Association.\n   146→get_metrics.forecast_point <- function(x, select = NULL, exclude = NULL, ...) {\n   147→  all <- list(\n   148→    ae_point = Metrics::ae,\n   149→    se_point = Metrics::se,\n   150→    ape = Metrics::ape\n   151→  )\n   152→  select_metrics(all, select, exclude)\n   153→}\n   154→\n   155→\n   156→#' Point forecast example data\n   157→#'\n   158→#' A data set with predictions for COVID-19 cases and deaths submitted to the\n   159→#' European Forecast Hub. This data set is like the quantile example data, only\n   160→#' that the median has been replaced by a point forecast.\n   161→#'\n   162→#' The data was created using the script create-example-data.R in the inst/\n   163→#' folder (or the top level folder in a compiled package).\n   164→#'\n   165→#' @format An object of class `forecast_point` (see [as_forecast_point()])\n   166→#' with the following columns:\n   167→#' \\describe{\n   168→#'   \\item{location}{the country for which a prediction was made}\n   169→#'   \\item{target_end_date}{the date for which a prediction was made}\n   170→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   171→#'   \\item{observed}{observed values}\n   172→#'   \\item{location_name}{name of the country for which a prediction was made}\n   173→#'   \\item{forecast_date}{the date on which a prediction was made}\n   174→#'   \\item{predicted}{predicted value}\n   175→#'   \\item{model}{name of the model that generated the forecasts}\n   176→#'   \\item{horizon}{forecast horizon in weeks}\n   177→#' }\n   178→# nolint start\n   179→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   180→# nolint end\n   181→\"example_point\"\n   182→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"c840bc34-5a39-453a-886e-a0c2b7703979"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01SbXcyDpoEgP8yRCSUEQ59k","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-quantile.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"075fb1f2-008e-4eb4-ac8c-9477113bf519"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01SbXcyDpoEgP8yRCSUEQ59k","type":"tool_result","content":"     1→#' @title Create a `forecast` object for quantile-based forecasts\n     2→#' @inherit as_forecast_doc_template params description\n     3→#' @details\n     4→#' # Target format\n     5→#'\n     6→#' The input for all further scoring needs to be a data.frame or similar with\n     7→#' the following columns:\n     8→#' - `observed`: Column of type `numeric` with observed values.\n     9→#' - `predicted`: Column of type `numeric` with predicted values. Predicted\n    10→#'    values represent quantiles of the predictive distribution.\n    11→#' - `quantile_level`: Column of type `numeric`, denoting the quantile level of\n    12→#'    the corresponding predicted value.\n    13→#'    Quantile levels must be between 0 and 1.\n    14→#'\n    15→#' For convenience, we recommend an additional column `model` holding the name\n    16→#' of the forecaster or model that produced a prediction, but this is not\n    17→#' strictly necessary.\n    18→#'\n    19→#' See the [example_quantile] data set for an example.\n    20→#' @inheritSection forecast_types Forecast unit\n    21→#' @param ... Unused\n    22→#' @family functions to create forecast objects\n    23→#' @returns A `forecast` object of class `forecast_quantile`\n    24→#' @export\n    25→#' @keywords as_forecast transform\n    26→#' @examples\n    27→#' as_forecast_quantile(\n    28→#'   example_quantile,\n    29→#'   predicted = \"predicted\",\n    30→#'   forecast_unit = c(\"model\", \"target_type\", \"target_end_date\",\n    31→#'                     \"horizon\", \"location\")\n    32→#' )\n    33→as_forecast_quantile <- function(data, ...) {\n    34→  UseMethod(\"as_forecast_quantile\")\n    35→}\n    36→\n    37→\n    38→#' @rdname as_forecast_quantile\n    39→#' @param quantile_level (optional) Name of the column in `data` that contains\n    40→#'   the quantile level of the predicted values. This column will be renamed to\n    41→#'   \"quantile_level\". Only applicable to quantile-based forecasts.\n    42→#' @export\n    43→#' @method as_forecast_quantile default\n    44→#' @importFrom cli cli_warn\n    45→as_forecast_quantile.default <- function(data,\n    46→                                         forecast_unit = NULL,\n    47→                                         observed = NULL,\n    48→                                         predicted = NULL,\n    49→                                         quantile_level = NULL,\n    50→                                         ...) {\n    51→  data <- as_forecast_generic(\n    52→    data,\n    53→    forecast_unit,\n    54→    observed = observed,\n    55→    predicted = predicted,\n    56→    quantile_level = quantile_level\n    57→  )\n    58→  unique_q_levels <- sort(unique(data$quantile_level))\n    59→  level_diffs <- diff(unique_q_levels)\n    60→  if (any(level_diffs <= 1e-10)) {\n    61→    cli_warn(\n    62→      \"The {.code quantile_level} column in your data\n    63→      seems to have a rounding issue\n    64→      (run {.code diff(sort(unique(data$quantile_level)))} to see this.\n    65→      As {.code scoringutils} does not support arbitrarily fine quantile level\n    66→      increments, we're going to run {.code round(x, digits = 10)} on\n    67→      the {.code quantile_level} column.\"\n    68→    )\n    69→    data$quantile_level <- round(data$quantile_level, digits = 9)\n    70→  }\n    71→\n    72→  data <- new_forecast(data, \"forecast_quantile\")\n    73→  assert_forecast(data)\n    74→  return(data)\n    75→}\n    76→\n    77→\n    78→#' @export\n    79→#' @rdname assert_forecast\n    80→#' @keywords validate-forecast-object\n    81→assert_forecast.forecast_quantile <- function(\n    82→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    83→) {\n    84→  assert(check_columns_present(forecast, \"quantile_level\"))\n    85→  forecast <- assert_forecast_generic(forecast, verbose)\n    86→  assert_forecast_type(forecast, actual = \"quantile\", desired = forecast_type)\n    87→  assert_numeric(forecast$quantile_level, lower = 0, upper = 1)\n    88→  return(invisible(NULL))\n    89→}\n    90→\n    91→\n    92→#' @export\n    93→#' @rdname is_forecast\n    94→is_forecast_quantile <- function(x) {\n    95→  inherits(x, \"forecast_quantile\") && inherits(x, \"forecast\")\n    96→}\n    97→\n    98→\n    99→#' @rdname as_forecast_point\n   100→#' @description\n   101→#' When converting a `forecast_quantile` object into a `forecast_point` object,\n   102→#' the 0.5 quantile is extracted and returned as the point forecast.\n   103→#' @export\n   104→#' @keywords as_forecast\n   105→as_forecast_point.forecast_quantile <- function(data, ...) {\n   106→  assert_forecast(data, verbose = FALSE)\n   107→  assert_subset(0.5, unique(data$quantile_level))\n   108→\n   109→  # At end of this function, the object will have be turned from a\n   110→  # forecast_quantile to a forecast_point and we don't want to validate it as a\n   111→  # forecast_point during the conversion process. The correct class is restored\n   112→  # at the end.\n   113→  data <- as.data.table(data)\n   114→\n   115→  forecast <- data[quantile_level == 0.5]\n   116→  forecast[, \"quantile_level\" := NULL]\n   117→\n   118→  point_forecast <- new_forecast(forecast, \"forecast_point\")\n   119→  return(point_forecast)\n   120→}\n   121→\n   122→\n   123→#' @importFrom stats na.omit\n   124→#' @importFrom data.table `:=` as.data.table rbindlist %like% setattr copy\n   125→#' @rdname score\n   126→#' @export\n   127→score.forecast_quantile <- function(forecast, metrics = get_metrics(forecast), ...) {\n   128→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   129→  forecast_unit <- get_forecast_unit(forecast)\n   130→  metrics <- validate_metrics(metrics)\n   131→  forecast <- as.data.table(forecast)\n   132→\n   133→  # transpose the forecasts that belong to the same forecast unit\n   134→  # make sure the quantiles and predictions are ordered in the same way\n   135→  f_transposed <- forecast[, .(\n   136→    predicted = list(predicted[order(quantile_level)]),\n   137→    observed = unique(observed),\n   138→    quantile_level = list(sort(quantile_level, na.last = TRUE)),\n   139→    scoringutils_quantile_level = toString(sort(quantile_level, na.last = TRUE))\n   140→  ), by = forecast_unit]\n   141→\n   142→  # split according to quantile_level lengths and do calculations for different\n   143→  # quantile_level lengths separately. The function `wis()` assumes that all\n   144→  # forecasts have the same quantile_levels\n   145→  f_split <- split(f_transposed, f_transposed$scoringutils_quantile_level)\n   146→\n   147→  split_result <- lapply(f_split, function(df) {\n   148→    # create a matrix out of the list of predicted values and quantile_levels\n   149→    observed <- df$observed\n   150→    predicted <- do.call(rbind, df$predicted)\n   151→    quantile_level <- unlist(unique(df$quantile_level))\n   152→    df[, c(\n   153→      \"observed\", \"predicted\", \"quantile_level\", \"scoringutils_quantile_level\"\n   154→    ) := NULL]\n   155→\n   156→    result <- apply_metrics(\n   157→      df, metrics,\n   158→      observed, predicted, quantile_level\n   159→    )\n   160→    return(result)\n   161→  })\n   162→  scores <- rbindlist(split_result, fill = TRUE)\n   163→\n   164→  scores <- as_scores(scores, metrics = names(metrics))\n   165→\n   166→  return(scores[])\n   167→}\n   168→\n   169→\n   170→#' Get default metrics for quantile-based forecasts\n   171→#'\n   172→#' @description\n   173→#' For quantile-based forecasts, the default scoring rules are:\n   174→#' - \"wis\" = [wis()]\n   175→#' - \"overprediction\" = [overprediction_quantile()]\n   176→#' - \"underprediction\" = [underprediction_quantile()]\n   177→#' - \"dispersion\" = [dispersion_quantile()]\n   178→#' - \"bias\" = [bias_quantile()]\n   179→#' - \"interval_coverage_50\" = [interval_coverage()]\n   180→#' - \"interval_coverage_90\" = purrr::partial(\n   181→#'      interval_coverage, interval_range = 90\n   182→#'    )\n   183→#' - \"ae_median\" = [ae_median_quantile()]\n   184→#'\n   185→#' Note: The `interval_coverage_90` scoring rule is created by modifying\n   186→#' [interval_coverage()], making use of the function [purrr::partial()].\n   187→#' This construct allows the function to deal with arbitrary arguments in `...`,\n   188→#' while making sure that only those that [interval_coverage()] can\n   189→#' accept get passed on to it. `interval_range = 90` is set in the function\n   190→#' definition, as passing an argument `interval_range = 90` to [score()] would\n   191→#' mean it would also get passed to `interval_coverage_50`.\n   192→#' @inheritSection illustration-input-metric-quantile Input format\n   193→#' @inheritParams get_metrics.forecast_binary\n   194→#' @export\n   195→#' @family get_metrics functions\n   196→#' @keywords handle-metrics\n   197→#' @importFrom purrr partial\n   198→#' @examples\n   199→#' get_metrics(example_quantile, select = \"wis\")\n   200→get_metrics.forecast_quantile <- function(x, select = NULL, exclude = NULL, ...) {\n   201→  all <- list(\n   202→    wis = wis,\n   203→    overprediction = overprediction_quantile,\n   204→    underprediction = underprediction_quantile,\n   205→    dispersion = dispersion_quantile,\n   206→    bias = bias_quantile,\n   207→    interval_coverage_50 = interval_coverage,\n   208→    interval_coverage_90 = purrr::partial(\n   209→      interval_coverage, interval_range = 90\n   210→    ),\n   211→    ae_median = ae_median_quantile\n   212→  )\n   213→  select_metrics(all, select, exclude)\n   214→}\n   215→\n   216→\n   217→#' @rdname get_pit_histogram\n   218→#' @importFrom stats na.omit\n   219→#' @importFrom data.table `:=` as.data.table\n   220→#' @export\n   221→get_pit_histogram.forecast_quantile <- function(forecast, num_bins = NULL,\n   222→                                                breaks = NULL, by, ...) {\n   223→  assert_number(num_bins, lower = 1, null.ok = TRUE)\n   224→  assert_numeric(breaks, lower = 0, upper = 1, null.ok = TRUE)\n   225→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   226→  forecast <- as.data.table(forecast)\n   227→  present_quantiles <- unique(c(0, forecast$quantile_level, 1))\n   228→  present_quantiles <- round(present_quantiles, 10)\n   229→\n   230→  if (!is.null(breaks)) {\n   231→    quantiles <- unique(c(0, breaks, 1))\n   232→  } else if (is.null(num_bins) || num_bins == \"auto\") {\n   233→    quantiles <- present_quantiles\n   234→  } else {\n   235→    quantiles <- seq(0, 1, 1 / num_bins)\n   236→  }\n   237→  ## avoid rounding errors\n   238→  quantiles <- round(quantiles, 10)\n   239→  diffs <- round(diff(quantiles), 10)\n   240→\n   241→  if (length(setdiff(quantiles, present_quantiles)) > 0) {\n   242→    cli::cli_warn(\n   243→      \"Some requested quantiles are missing in the forecast. \",\n   244→      \"The PIT histogram will be based on the quantiles present in the forecast.\"\n   245→    )\n   246→  }\n   247→\n   248→  forecast <- forecast[quantile_level %in% quantiles]\n   249→  forecast[, quantile_coverage := (observed <= predicted)]\n   250→\n   251→  quantile_coverage <-\n   252→    forecast[, .(quantile_coverage = mean(quantile_coverage)),\n   253→             by = c(unique(c(by, \"quantile_level\")))]\n   254→\n   255→  bins <- sprintf(\"[%s,%s)\", quantiles[-length(quantiles)], quantiles[-1])\n   256→  mids <- (quantiles[-length(quantiles)] + quantiles[-1]) / 2\n   257→\n   258→  pit_histogram <- quantile_coverage[\n   259→    order(quantile_level),\n   260→    .(\n   261→      density = diff(c(0, quantile_coverage, 1)) / diffs,\n   262→      bin = bins,\n   263→      mid = mids\n   264→    ),\n   265→    by = c(get_forecast_unit(quantile_coverage))\n   266→  ]\n   267→  return(pit_histogram[])\n   268→}\n   269→\n   270→\n   271→#' Quantile example data\n   272→#'\n   273→#' A data set with predictions for COVID-19 cases and deaths submitted to the\n   274→#' European Forecast Hub.\n   275→#'\n   276→#' The data was created using the script create-example-data.R in the inst/\n   277→#' folder (or the top level folder in a compiled package).\n   278→#'\n   279→#' @format An object of class `forecast_quantile`\n   280→#' (see [as_forecast_quantile()]) with the following columns:\n   281→#' \\describe{\n   282→#'   \\item{location}{the country for which a prediction was made}\n   283→#'   \\item{target_end_date}{the date for which a prediction was made}\n   284→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   285→#'   \\item{observed}{Numeric: observed values}\n   286→#'   \\item{location_name}{name of the country for which a prediction was made}\n   287→#'   \\item{forecast_date}{the date on which a prediction was made}\n   288→#'   \\item{quantile_level}{quantile level of the corresponding prediction}\n   289→#'   \\item{predicted}{predicted value}\n   290→#'   \\item{model}{name of the model that generated the forecasts}\n   291→#'   \\item{horizon}{forecast horizon in weeks}\n   292→#' }\n   293→# nolint start\n   294→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   295→# nolint end\n   296→\"example_quantile\"\n   297→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"0a07534d-db1b-4ebb-8472-0d8ba7a52cc1"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Wzs41jG4RP8yD3RJtDpvYN","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-sample.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"6a744ad4-3f75-4c77-b3df-f52229aeb687"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Wzs41jG4RP8yD3RJtDpvYN","type":"tool_result","content":"     1→#' @title Create a `forecast` object for sample-based forecasts\n     2→#' @inherit as_forecast_doc_template params description\n     3→#' @details\n     4→#' # Target format\n     5→#'\n     6→#' The input for all further scoring needs to be a data.frame or similar with\n     7→#' the following columns:\n     8→#' - `observed`: Column of type `numeric` with observed values.\n     9→#' - `predicted`: Column of type `numeric` with predicted values. Predicted\n    10→#'    values represent random samples from the predictive distribution.\n    11→#' - `sample_id`: Column of any type with unique identifiers\n    12→#'    (unique within a single forecast) for each sample.\n    13→#'\n    14→#' For convenience, we recommend an additional column `model` holding the name\n    15→#' of the forecaster or model that produced a prediction, but this is not\n    16→#' strictly necessary.\n    17→#'\n    18→#' See the [example_sample_continuous] and [example_sample_discrete] data set\n    19→#' for an example\n    20→#' @inheritSection forecast_types Forecast unit\n    21→#' @param ... Unused\n    22→#' @family functions to create forecast objects\n    23→#' @returns A `forecast` object of class `forecast_sample`\n    24→#' @export\n    25→#' @keywords as_forecast transform\n    26→as_forecast_sample <- function(data, ...) {\n    27→  UseMethod(\"as_forecast_sample\")\n    28→}\n    29→\n    30→\n    31→#' @rdname as_forecast_sample\n    32→#' @param sample_id (optional) Name of the column in `data` that contains the\n    33→#'   sample id. This column will be renamed to \"sample_id\".\n    34→#' @export\n    35→#' @importFrom cli cli_warn\n    36→as_forecast_sample.default <- function(data,\n    37→                                       forecast_unit = NULL,\n    38→                                       observed = NULL,\n    39→                                       predicted = NULL,\n    40→                                       sample_id = NULL,\n    41→                                       ...) {\n    42→  data <- as_forecast_generic(\n    43→    data,\n    44→    forecast_unit,\n    45→    observed = observed,\n    46→    predicted = predicted,\n    47→    sample_id = sample_id\n    48→  )\n    49→  data <- new_forecast(data, \"forecast_sample\")\n    50→  assert_forecast(data)\n    51→  return(data)\n    52→}\n    53→\n    54→\n    55→#' @export\n    56→#' @rdname assert_forecast\n    57→#' @keywords validate-forecast-object\n    58→assert_forecast.forecast_sample <- function(\n    59→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    60→) {\n    61→  forecast <- assert_forecast_generic(forecast, verbose)\n    62→  assert(check_columns_present(forecast, \"sample_id\"))\n    63→  assert_forecast_type(forecast, actual = \"sample\", desired = forecast_type)\n    64→  return(invisible(NULL))\n    65→}\n    66→\n    67→\n    68→#' @export\n    69→#' @rdname is_forecast\n    70→is_forecast_sample <- function(x) {\n    71→  inherits(x, \"forecast_sample\") && inherits(x, \"forecast\")\n    72→}\n    73→\n    74→\n    75→#' @rdname as_forecast_quantile\n    76→#' @details # Converting from `forecast_sample` to `forecast_quantile`\n    77→#' When creating a `forecast_quantile` object from a `forecast_sample` object,\n    78→#' the quantiles are estimated by computing empircal quantiles from the samples\n    79→#' via [quantile()]. Note that empirical quantiles are a biased estimator for\n    80→#' the true quantiles in particular in the tails of the distribution and\n    81→#' when the number of available samples is low.\n    82→#' @param probs A numeric vector of quantile levels for which\n    83→#'   quantiles will be computed. Corresponds to the `probs` argument in\n    84→#'   [quantile()].\n    85→#' @param type Type argument passed down to the quantile function. For more\n    86→#'   information, see [quantile()].\n    87→#' @importFrom stats quantile\n    88→#' @importFrom methods hasArg\n    89→#' @importFrom checkmate assert_numeric\n    90→#' @export\n    91→as_forecast_quantile.forecast_sample <- function(\n    92→  data,\n    93→  probs = c(0.05, 0.25, 0.5, 0.75, 0.95),\n    94→  type = 7,\n    95→  ...\n    96→) {\n    97→  forecast <- as.data.table(data)\n    98→  assert_numeric(probs, min.len = 1)\n    99→  reserved_columns <- c(\"predicted\", \"sample_id\")\n   100→  by <- setdiff(colnames(forecast), reserved_columns)\n   101→\n   102→  quantile_level <- unique(\n   103→    round(c(probs, 1 - probs), digits = 10)\n   104→  )\n   105→\n   106→  forecast <-\n   107→    forecast[, .(quantile_level = quantile_level,\n   108→                 predicted = quantile(x = predicted, probs = ..probs,\n   109→                                      type = ..type, na.rm = TRUE)),\n   110→             by = by]\n   111→\n   112→  quantile_forecast <- new_forecast(forecast, \"forecast_quantile\")\n   113→  assert_forecast(quantile_forecast)\n   114→\n   115→  return(quantile_forecast)\n   116→}\n   117→\n   118→\n   119→#' @importFrom stats na.omit\n   120→#' @importFrom data.table setattr copy\n   121→#' @rdname score\n   122→#' @export\n   123→score.forecast_sample <- function(forecast, metrics = get_metrics(forecast), ...) {\n   124→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   125→  forecast_unit <- get_forecast_unit(forecast)\n   126→  metrics <- validate_metrics(metrics)\n   127→  forecast <- as.data.table(forecast)\n   128→\n   129→  # transpose the forecasts that belong to the same forecast unit\n   130→  f_transposed <- forecast[, .(predicted = list(predicted),\n   131→                               observed = unique(observed),\n   132→                               .scoringutils_N = length(sample_id)),\n   133→                           by = forecast_unit]\n   134→\n   135→  # split according to number of samples and do calculations for different\n   136→  # sample lengths separately\n   137→  f_split <- split(f_transposed, f_transposed$.scoringutils_N)\n   138→\n   139→  split_result <- lapply(f_split, function(forecast) {\n   140→    # create a matrix\n   141→    observed <- forecast$observed\n   142→    predicted <- do.call(rbind, forecast$predicted)\n   143→    forecast[, c(\"observed\", \"predicted\", \".scoringutils_N\") := NULL]\n   144→\n   145→    forecast <- apply_metrics(\n   146→      forecast, metrics,\n   147→      observed, predicted\n   148→    )\n   149→    return(forecast)\n   150→  })\n   151→  scores <- rbindlist(split_result, fill = TRUE)\n   152→  scores <- as_scores(scores, metrics = names(metrics))\n   153→  return(scores[])\n   154→}\n   155→\n   156→\n   157→#' Get default metrics for sample-based forecasts\n   158→#'\n   159→#' @description\n   160→#' For sample-based forecasts, the default scoring rules are:\n   161→#' - \"crps\" = [crps_sample()]\n   162→#' - \"overprediction\" = [overprediction_sample()]\n   163→#' - \"underprediction\" = [underprediction_sample()]\n   164→#' - \"dispersion\" = [dispersion_sample()]\n   165→#' - \"log_score\" = [logs_sample()]\n   166→#' - \"dss\" = [dss_sample()]\n   167→#' - \"mad\" = [mad_sample()]\n   168→#' - \"bias\" = [bias_sample()]\n   169→#' - \"ae_median\" = [ae_median_sample()]\n   170→#' - \"se_mean\" = [se_mean_sample()]\n   171→#' @inheritSection illustration-input-metric-sample Input format\n   172→#' @inheritParams get_metrics.forecast_binary\n   173→#' @export\n   174→#' @family get_metrics functions\n   175→#' @keywords handle-metrics\n   176→#' @examples\n   177→#' get_metrics(example_sample_continuous, exclude = \"mad\")\n   178→get_metrics.forecast_sample <- function(x, select = NULL, exclude = NULL, ...) {\n   179→  all <- list(\n   180→    bias = bias_sample,\n   181→    dss = dss_sample,\n   182→    crps = crps_sample,\n   183→    overprediction = overprediction_sample,\n   184→    underprediction = underprediction_sample,\n   185→    dispersion = dispersion_sample,\n   186→    log_score = logs_sample,\n   187→    mad = mad_sample,\n   188→    ae_median = ae_median_sample,\n   189→    se_mean = se_mean_sample\n   190→  )\n   191→  select_metrics(all, select, exclude)\n   192→}\n   193→\n   194→\n   195→#' @rdname get_pit_histogram\n   196→#' @importFrom data.table `:=` as.data.table dcast\n   197→#' @importFrom checkmate assert_int assert_numeric\n   198→#' @inheritParams pit_histogram_sample\n   199→#' @seealso [pit_histogram_sample()]\n   200→#' @export\n   201→get_pit_histogram.forecast_sample <- function(forecast, num_bins = 10,\n   202→                                              breaks = NULL, by, integers = c(\n   203→                                                \"nonrandom\", \"random\", \"ignore\"\n   204→                                              ), n_replicates = NULL, ...) {\n   205→  integers <- match.arg(integers)\n   206→  assert_int(num_bins, lower = 1, null.ok = FALSE)\n   207→  assert_numeric(breaks, lower = 0, upper = 1, null.ok = TRUE)\n   208→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   209→  forecast <- as.data.table(forecast)\n   210→\n   211→  if (is.null(breaks)) {\n   212→    quantiles <- seq(0, 1, 1 / num_bins)\n   213→  } else {\n   214→    quantiles <- unique(c(0, breaks, 1))\n   215→  }\n   216→\n   217→  forecast_wide <- data.table::dcast(\n   218→    forecast,\n   219→    ... ~ paste0(\"InternalSampl_\", sample_id),\n   220→    value.var = \"predicted\"\n   221→  )\n   222→\n   223→  bins <- sprintf(\"[%s,%s)\", quantiles[-length(quantiles)], quantiles[-1])\n   224→  mids <- (quantiles[-length(quantiles)] + quantiles[-1]) / 2\n   225→\n   226→  pit_histogram <- forecast_wide[, .(\n   227→    density = pit_histogram_sample(\n   228→      observed = observed,\n   229→      predicted = as.matrix(.SD),\n   230→      quantiles = quantiles,\n   231→      integers = integers,\n   232→      n_replicates = n_replicates\n   233→    ),\n   234→    bin = bins,\n   235→    mid = mids\n   236→  ),\n   237→  by = by,\n   238→  .SDcols = grepl(\"InternalSampl_\", names(forecast_wide), fixed = TRUE)\n   239→  ]\n   240→\n   241→  return(pit_histogram[])\n   242→}\n   243→\n   244→\n   245→#' Continuous forecast example data\n   246→#'\n   247→#' A data set with continuous predictions for COVID-19 cases and deaths\n   248→#' constructed from data submitted to the European Forecast Hub.\n   249→#'\n   250→#' The data was created using the script create-example-data.R in the inst/\n   251→#' folder (or the top level folder in a compiled package).\n   252→#'\n   253→#' @format An object of class `forecast_sample` (see [as_forecast_sample()])\n   254→#' with the following columns:\n   255→#' \\describe{\n   256→#'   \\item{location}{the country for which a prediction was made}\n   257→#'   \\item{target_end_date}{the date for which a prediction was made}\n   258→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   259→#'   \\item{observed}{observed values}\n   260→#'   \\item{location_name}{name of the country for which a prediction was made}\n   261→#'   \\item{forecast_date}{the date on which a prediction was made}\n   262→#'   \\item{model}{name of the model that generated the forecasts}\n   263→#'   \\item{horizon}{forecast horizon in weeks}\n   264→#'   \\item{predicted}{predicted value}\n   265→#'   \\item{sample_id}{id for the corresponding sample}\n   266→#' }\n   267→# nolint start\n   268→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   269→# nolint end\n   270→\"example_sample_continuous\"\n   271→\n   272→\n   273→#' Discrete forecast example data\n   274→#'\n   275→#' A data set with integer predictions for COVID-19 cases and deaths\n   276→#' constructed from data submitted to the European Forecast Hub.\n   277→#'\n   278→#' The data was created using the script create-example-data.R in the inst/\n   279→#' folder (or the top level folder in a compiled package).\n   280→#'\n   281→#' @format An object of class `forecast_sample` (see [as_forecast_sample()])\n   282→#' with the following columns:\n   283→#' \\describe{\n   284→#'   \\item{location}{the country for which a prediction was made}\n   285→#'   \\item{target_end_date}{the date for which a prediction was made}\n   286→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   287→#'   \\item{observed}{observed values}\n   288→#'   \\item{location_name}{name of the country for which a prediction was made}\n   289→#'   \\item{forecast_date}{the date on which a prediction was made}\n   290→#'   \\item{model}{name of the model that generated the forecasts}\n   291→#'   \\item{horizon}{forecast horizon in weeks}\n   292→#'   \\item{predicted}{predicted value}\n   293→#'   \\item{sample_id}{id for the corresponding sample}\n   294→#' }\n   295→# nolint start\n   296→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   297→# nolint end\n   298→\"example_sample_discrete\"\n   299→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"5718beb3-c8e7-4567-901e-1464d9d44750"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012QZZuedZZDQu8Ceqf8BVDr","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-nominal.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"c348ac7c-b2ec-4c63-ac3e-13a37f37222b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012QZZuedZZDQu8Ceqf8BVDr","type":"tool_result","content":"     1→#' @title Create a `forecast` object for nominal forecasts\n     2→#' @inherit as_forecast_doc_template params description\n     3→#' @details\n     4→#' Nominal forecasts are a form of categorical forecasts and represent a\n     5→#' generalisation of binary forecasts to multiple outcomes. The possible\n     6→#' outcomes that the observed values can assume are not ordered.\n     7→#'\n     8→#' # Target format\n     9→#'\n    10→#' The input for all further scoring needs to be a data.frame or similar with\n    11→#' the following columns:\n    12→#' - `observed`: Column with observed values of type `factor` with N levels,\n    13→#'    where N is the number of possible outcomes.\n    14→#'    The levels of the factor represent the possible outcomes that\n    15→#'    the observed values can assume.\n    16→#' - `predicted`: `numeric` column with predicted probabilities. The values\n    17→#'    represent the probability that the observed value is equal to the factor\n    18→#'    level denoted in `predicted_label`. Note that forecasts must be complete,\n    19→#'    i.e. there must be a probability assigned to every possible outcome and\n    20→#'    those probabilities must sum to one.\n    21→#' - `predicted_label`: `factor` with N levels, denoting the outcome that the\n    22→#'    probabilities in `predicted` correspond to.\n    23→#'\n    24→#' For convenience, we recommend an additional column `model` holding the name\n    25→#' of the forecaster or model that produced a prediction, but this is not\n    26→#' strictly necessary.\n    27→#'\n    28→#' See the [example_nominal] data set for an example.\n    29→#' @inheritSection forecast_types Forecast unit\n    30→#' @param ... Unused\n    31→#' @returns A `forecast` object of class `forecast_nominal`\n    32→#' @family functions to create forecast objects\n    33→#' @keywords as_forecast transform\n    34→#' @export\n    35→#' @examples\n    36→#' as_forecast_nominal(\n    37→#'   na.omit(example_nominal),\n    38→#'   predicted = \"predicted\",\n    39→#'   forecast_unit = c(\"model\", \"target_type\", \"target_end_date\",\n    40→#'                     \"horizon\", \"location\")\n    41→#' )\n    42→as_forecast_nominal <- function(data, ...) {\n    43→  UseMethod(\"as_forecast_nominal\")\n    44→}\n    45→\n    46→#' @rdname as_forecast_nominal\n    47→#' @param predicted_label (optional) Name of the column in `data` that denotes\n    48→#'   the outcome to which a predicted probability corresponds to.\n    49→#'   This column will be renamed to \"predicted_label\".\n    50→#' @export\n    51→#' @method as_forecast_nominal default\n    52→#' @importFrom cli cli_warn\n    53→as_forecast_nominal.default <- function(data,\n    54→                                        forecast_unit = NULL,\n    55→                                        observed = NULL,\n    56→                                        predicted = NULL,\n    57→                                        predicted_label = NULL,\n    58→                                        ...) {\n    59→  data <- as_forecast_generic(\n    60→    data,\n    61→    forecast_unit,\n    62→    observed = observed,\n    63→    predicted = predicted,\n    64→    predicted_label = predicted_label\n    65→  )\n    66→  data <- new_forecast(data, \"forecast_nominal\")\n    67→  assert_forecast(data)\n    68→  return(data)\n    69→}\n    70→\n    71→\n    72→#' @export\n    73→#' @keywords check-forecasts\n    74→#' @importFrom checkmate assert_names assert_set_equal test_set_equal\n    75→assert_forecast.forecast_nominal <- function(\n    76→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    77→) {\n    78→  assert(check_columns_present(forecast, \"predicted_label\"))\n    79→  assert_names(\n    80→    colnames(forecast),\n    81→    disjunct.from = c(\"sample_id\", \"quantile_level\")\n    82→  )\n    83→  assert_forecast_type(forecast, actual = \"nominal\", desired = forecast_type)\n    84→\n    85→  assert_factor(forecast$observed, ordered = FALSE)\n    86→  assert_factor(forecast$predicted_label, ordered = FALSE)\n    87→\n    88→  # levels need to be the same\n    89→  outcomes <- levels(forecast$observed)\n    90→  assert_set_equal(levels(forecast$predicted_label), outcomes)\n    91→\n    92→  forecast <- assert_forecast_generic(forecast, verbose)\n    93→\n    94→  # forecasts need to be complete\n    95→  forecast_unit <- get_forecast_unit(forecast)\n    96→  complete <- as.data.table(forecast)[, .(\n    97→    correct = test_set_equal(as.character(predicted_label), outcomes)\n    98→  ), by = forecast_unit]\n    99→\n   100→  if (!all(complete$correct)) {\n   101→    first_issue <- complete[(correct), ..forecast_unit][1]\n   102→    first_issue <- lapply(first_issue, FUN = as.character)\n   103→    #nolint start: keyword_quote_linter object_usage_linter duplicate_argument_linter\n   104→    issue_location <- paste(names(first_issue), \"==\", first_issue)\n   105→    cli_abort(\n   106→      c(`!` = \"Found incomplete forecasts\",\n   107→        `i` = \"For a nominal forecast, all possible outcomes must be assigned\n   108→        a probability explicitly.\",\n   109→        `i` = \"Found first missing probabilities in the forecast identified by\n   110→        {.emph {issue_location}}\")\n   111→    )\n   112→    #nolint end\n   113→  }\n   114→  return(forecast[])\n   115→}\n   116→\n   117→\n   118→#' @export\n   119→#' @rdname is_forecast\n   120→is_forecast_nominal <- function(x) {\n   121→  inherits(x, \"forecast_nominal\") && inherits(x, \"forecast\")\n   122→}\n   123→\n   124→\n   125→#' @importFrom stats na.omit\n   126→#' @importFrom data.table setattr\n   127→#' @rdname score\n   128→#' @export\n   129→score.forecast_nominal <- function(forecast, metrics = get_metrics(forecast), ...) {\n   130→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   131→  forecast_unit <- get_forecast_unit(forecast)\n   132→  metrics <- validate_metrics(metrics)\n   133→  forecast <- as.data.table(forecast)\n   134→\n   135→  # transpose the forecasts that belong to the same forecast unit\n   136→  # make sure the labels and predictions are ordered in the same way\n   137→  f_transposed <- forecast[, .(\n   138→    predicted = list(predicted[order(predicted_label)]),\n   139→    observed = unique(observed)\n   140→  ), by = forecast_unit]\n   141→\n   142→  observed <- f_transposed$observed\n   143→  predicted <- do.call(rbind, f_transposed$predicted)\n   144→  predicted_label <- sort(unique(forecast$predicted_label, na.last = TRUE))\n   145→  f_transposed[, c(\"observed\", \"predicted\") := NULL]\n   146→\n   147→  scores <- apply_metrics(\n   148→    f_transposed, metrics,\n   149→    observed, predicted, predicted_label, ...\n   150→  )\n   151→  scores <- as_scores(scores, metrics = names(metrics))\n   152→  return(scores[])\n   153→}\n   154→\n   155→\n   156→#' Get default metrics for nominal forecasts\n   157→#' @inheritParams get_metrics.forecast_binary\n   158→#' @description\n   159→#' For nominal forecasts, the default scoring rule is:\n   160→#' - \"log_score\" = [logs_categorical()]\n   161→#' @export\n   162→#' @family get_metrics functions\n   163→#' @keywords handle-metrics\n   164→#' @examples\n   165→#' get_metrics(example_nominal)\n   166→get_metrics.forecast_nominal <- function(x, select = NULL, exclude = NULL, ...) {\n   167→  all <- list(\n   168→    log_score = logs_categorical\n   169→  )\n   170→  select_metrics(all, select, exclude)\n   171→}\n   172→\n   173→\n   174→#' Nominal example data\n   175→#'\n   176→#' A data set with predictions for COVID-19 cases and deaths submitted to the\n   177→#' European Forecast Hub.\n   178→#'\n   179→#' The data was created using the script create-example-data.R in the inst/\n   180→#' folder (or the top level folder in a compiled package).\n   181→#'\n   182→#' @format An object of class `forecast_nominal`\n   183→#' (see [as_forecast_nominal()]) with the following columns:\n   184→#' \\describe{\n   185→#'   \\item{location}{the country for which a prediction was made}\n   186→#'   \\item{target_end_date}{the date for which a prediction was made}\n   187→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   188→#'   \\item{observed}{Numeric: observed values}\n   189→#'   \\item{location_name}{name of the country for which a prediction was made}\n   190→#'   \\item{forecast_date}{the date on which a prediction was made}\n   191→#'   \\item{predicted_label}{outcome that a probabilty corresponds to}\n   192→#'   \\item{predicted}{predicted value}\n   193→#'   \\item{model}{name of the model that generated the forecasts}\n   194→#'   \\item{horizon}{forecast horizon in weeks}\n   195→#' }\n   196→# nolint start\n   197→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   198→# nolint end\n   199→\"example_nominal\"\n   200→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"52de7957-8555-42f1-8ee5-e533b1c59b3b"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PP6PK7EAxmy3iSAwB5SP8B","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-ordinal.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"43301e46-60ef-4bde-bb95-1fa80463673a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PP6PK7EAxmy3iSAwB5SP8B","type":"tool_result","content":"     1→#' @title Create a `forecast` object for ordinal forecasts\n     2→#' @inherit as_forecast_doc_template params description\n     3→#' @details\n     4→#' Ordinal forecasts are a form of categorical forecasts and represent a\n     5→#' generalisation of binary forecasts to multiple outcomes. The possible\n     6→#' outcomes that the observed values can assume are ordered.\n     7→#'\n     8→#' # Target format\n     9→#'\n    10→#' The input for all further scoring needs to be a data.frame or similar with\n    11→#' the following columns:\n    12→#' - `observed`: Column with observed values of type `factor` with N ordered\n    13→#'   levels, where N is the number of possible outcomes.\n    14→#'   The levels of the factor represent the possible outcomes that\n    15→#'   the observed values can assume.\n    16→#' - `predicted`: `numeric` column with predicted probabilities. The values\n    17→#'    represent the probability that the observed value is equal to the factor\n    18→#'    level denoted in `predicted_label`. Note that forecasts must be complete,\n    19→#'    i.e. there must be a probability assigned to every possible outcome and\n    20→#'    those probabilities must sum to one.\n    21→#' - `predicted_label`: `factor` with N levels, denoting the outcome that the\n    22→#'    probabilities in `predicted` correspond to.\n    23→#'\n    24→#' For convenience, we recommend an additional column `model` holding the name\n    25→#' of the forecaster or model that produced a prediction, but this is not\n    26→#' strictly necessary.\n    27→#'\n    28→#' See the [example_ordinal] data set for an example.\n    29→#' @inheritSection forecast_types Forecast unit\n    30→#' @param ... Unused\n    31→#' @family functions to create forecast objects\n    32→#' @returns A `forecast` object of class `forecast_ordinal`\n    33→#' @export\n    34→#' @keywords as_forecast transform\n    35→#' @examples\n    36→#' as_forecast_ordinal(\n    37→#'   na.omit(example_ordinal),\n    38→#'   predicted = \"predicted\",\n    39→#'   forecast_unit = c(\"model\", \"target_type\", \"target_end_date\",\n    40→#'                     \"horizon\", \"location\")\n    41→#' )\n    42→as_forecast_ordinal <- function(data, ...) {\n    43→  UseMethod(\"as_forecast_ordinal\")\n    44→}\n    45→\n    46→#' @rdname as_forecast_ordinal\n    47→#' @param predicted_label (optional) Name of the column in `data` that denotes\n    48→#'   the outcome to which a predicted probability corresponds to.\n    49→#'   This column will be renamed to \"predicted_label\".\n    50→#' @export\n    51→#' @method as_forecast_ordinal default\n    52→#' @importFrom cli cli_warn\n    53→as_forecast_ordinal.default <- function(data,\n    54→                                        forecast_unit = NULL,\n    55→                                        observed = NULL,\n    56→                                        predicted = NULL,\n    57→                                        predicted_label = NULL,\n    58→                                        ...) {\n    59→  data <- as_forecast_generic(\n    60→    data,\n    61→    forecast_unit,\n    62→    observed = observed,\n    63→    predicted = predicted,\n    64→    predicted_label = predicted_label\n    65→  )\n    66→  data <- new_forecast(data, \"forecast_ordinal\")\n    67→  assert_forecast(data)\n    68→  return(data)\n    69→}\n    70→\n    71→\n    72→#' @export\n    73→#' @keywords check-forecasts\n    74→#' @importFrom checkmate assert_names assert_set_equal test_set_equal assert_factor\n    75→assert_forecast.forecast_ordinal <- function(\n    76→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    77→) {\n    78→  assert(check_columns_present(forecast, \"predicted_label\"))\n    79→  assert_names(\n    80→    colnames(forecast),\n    81→    disjunct.from = c(\"sample_id\", \"quantile_level\")\n    82→  )\n    83→  assert_forecast_type(forecast, actual = \"ordinal\", desired = forecast_type)\n    84→\n    85→  assert_factor(forecast$observed, ordered = TRUE)\n    86→  assert_factor(forecast$predicted_label, ordered = TRUE)\n    87→\n    88→  forecast <- assert_forecast_generic(forecast, verbose)\n    89→\n    90→  observed_levels <- levels(forecast$observed)\n    91→  predicted_label_levels <- levels(forecast$predicted_label)\n    92→  if (!identical(predicted_label_levels, observed_levels)) {\n    93→    cli_abort(\n    94→      \"Levels of `predicted_label` and `observed` must be identical\n    95→      and in the same order. Found levels {.val {predicted_label_levels}}\n    96→      and {.val {observed_levels}}.\"\n    97→    )\n    98→  }\n    99→\n   100→  # forecasts need to be complete\n   101→  forecast_unit <- get_forecast_unit(forecast)\n   102→  complete <- as.data.table(forecast)[, .(\n   103→    correct = test_set_equal(as.character(predicted_label), observed_levels)\n   104→  ), by = forecast_unit]\n   105→\n   106→  if (!all(complete$correct)) {\n   107→    first_issue <- complete[(correct), ..forecast_unit][1]\n   108→    first_issue <- lapply(first_issue, FUN = as.character)\n   109→    #nolint start: keyword_quote_linter object_usage_linter duplicate_argument_linter\n   110→    issue_location <- paste(names(first_issue), \"==\", first_issue)\n   111→    cli_abort(\n   112→      c(`!` = \"Found incomplete forecasts\",\n   113→        `i` = \"For an ordinal forecast, all possible outcomes must be assigned\n   114→        a probability explicitly.\",\n   115→        `i` = \"Found first missing probabilities in the forecast identified by\n   116→        {.emph {issue_location}}\")\n   117→    )\n   118→    #nolint end\n   119→  }\n   120→  return(forecast[])\n   121→}\n   122→\n   123→\n   124→#' @export\n   125→#' @rdname is_forecast\n   126→is_forecast_ordinal <- function(x) {\n   127→  inherits(x, \"forecast_ordinal\") && inherits(x, \"forecast\")\n   128→}\n   129→\n   130→\n   131→#' @importFrom stats na.omit\n   132→#' @importFrom data.table setattr\n   133→#' @rdname score\n   134→#' @export\n   135→score.forecast_ordinal <- function(forecast, metrics = get_metrics(forecast), ...) {\n   136→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   137→  forecast_unit <- get_forecast_unit(forecast)\n   138→  metrics <- validate_metrics(metrics)\n   139→  forecast <- as.data.table(forecast)\n   140→\n   141→  # transpose the forecasts that belong to the same forecast unit\n   142→  # make sure the labels and predictions are ordered in the same way\n   143→  f_transposed <- forecast[, .(\n   144→    predicted = list(predicted[order(predicted_label)]),\n   145→    observed = unique(observed)\n   146→  ), by = forecast_unit]\n   147→\n   148→  observed <- f_transposed$observed\n   149→  predicted <- do.call(rbind, f_transposed$predicted)\n   150→  predicted_label <- sort(unique(forecast$predicted_label, na.last = TRUE))\n   151→  f_transposed[, c(\"observed\", \"predicted\") := NULL]\n   152→\n   153→  scores <- apply_metrics(\n   154→    f_transposed, metrics,\n   155→    observed, predicted, predicted_label, ...\n   156→  )\n   157→  scores <- as_scores(scores, metrics = names(metrics))\n   158→  return(scores[])\n   159→}\n   160→\n   161→\n   162→#' Get default metrics for nominal forecasts\n   163→#' @inheritParams get_metrics.forecast_binary\n   164→#' @description\n   165→#' For ordinal forecasts, the default scoring rules are:\n   166→#' - \"log_score\" = [logs_categorical()]\n   167→#' - \"rps\" = [rps_ordinal()]\n   168→#' @export\n   169→#' @family get_metrics functions\n   170→#' @keywords handle-metrics\n   171→#' @examples\n   172→#' get_metrics(example_ordinal)\n   173→get_metrics.forecast_ordinal <- function(x, select = NULL, exclude = NULL, ...) {\n   174→  all <- list(\n   175→    log_score = logs_categorical,\n   176→    rps = rps_ordinal\n   177→  )\n   178→  select_metrics(all, select, exclude)\n   179→}\n   180→\n   181→\n   182→#' Ordinal example data\n   183→#'\n   184→#' A data set with predictions for COVID-19 cases and deaths submitted to the\n   185→#' European Forecast Hub.\n   186→#'\n   187→#' The data was created using the script create-example-data.R in the inst/\n   188→#' folder (or the top level folder in a compiled package).\n   189→#'\n   190→#' @format An object of class `forecast_ordinal`\n   191→#' (see [as_forecast_ordinal()]) with the following columns:\n   192→#' \\describe{\n   193→#'   \\item{location}{the country for which a prediction was made}\n   194→#'   \\item{target_end_date}{the date for which a prediction was made}\n   195→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   196→#'   \\item{observed}{Numeric: observed values}\n   197→#'   \\item{location_name}{name of the country for which a prediction was made}\n   198→#'   \\item{forecast_date}{the date on which a prediction was made}\n   199→#'   \\item{predicted_label}{outcome that a probabilty corresponds to}\n   200→#'   \\item{predicted}{predicted value}\n   201→#'   \\item{model}{name of the model that generated the forecasts}\n   202→#'   \\item{horizon}{forecast horizon in weeks}\n   203→#' }\n   204→# nolint start\n   205→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   206→# nolint end\n   207→\"example_ordinal\"\n   208→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"105beff9-c001-4cc3-9a31-6139567aba72"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019nVuEvLDmUHxd2FmkJaQnX","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-multivariate-sample.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"4a65c375-8240-4cfa-b209-29cb1c678954"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019nVuEvLDmUHxd2FmkJaQnX","type":"tool_result","content":"     1→#' @title Create a `forecast` object for sample-based multivariate forecasts\n     2→#' @inherit as_forecast_doc_template params description\n     3→#' @param forecast_unit (optional) Name of the columns in `data` (after\n     4→#'   any renaming of columns) that denote the unit of a\n     5→#'   single univariate (!) forecast. See [get_forecast_unit()] for details.\n     6→#'   If `NULL` (the default), all columns that are not required columns are\n     7→#'   assumed to form the unit of a single forecast. If specified, all columns\n     8→#'   that are not part of the forecast unit (or required columns) will be removed.\n     9→#'   Multivariate forecasts are defined by a) specifying the univariate forecast\n    10→#'   unit (i.e. the unit of a single forecast if that forecast were univariate)\n    11→#'   and b) specifying which variables are pooled together to form a\n    12→#'   multivariate forecast.\n    13→#' @details\n    14→#' # Target format\n    15→#'\n    16→#' The input for all further scoring needs to be a data.frame or similar\n    17→#' with the following columns:\n    18→#' - `observed`: Column of type `numeric` with observed values.\n    19→#' - `predicted`: Column of type `numeric` with predicted values. Predicted\n    20→#'    values represent random samples from the predictive distribution.\n    21→#' - `sample_id`: Column of any type with unique identifiers\n    22→#'    (unique within a single forecast) for each sample.\n    23→#' - `mv_group_id`: Column of any type with unique identifiers\n    24→#'    (unique within a single forecast) for each multivariate group. This column\n    25→#'    is created automatically using the `forecast_unit` and the `joint_across`\n    26→#'    arguments.\n    27→#'\n    28→#' For convenience, we recommend an additional column `model` holding the name\n    29→#' of the forecaster or model that produced a prediction, but this is not\n    30→#' strictly necessary.\n    31→#'\n    32→#' See the [example_sample_continuous] and [example_sample_discrete] data set\n    33→#' for an example\n    34→#' @inheritSection forecast_types Forecast unit\n    35→#' @param ... Unused\n    36→#' @family functions to create forecast objects\n    37→#' @returns A `forecast` object of class `forecast_sample`\n    38→#' @export\n    39→#' @keywords as_forecast transform\n    40→# nolint start: object_name_linter\n    41→as_forecast_multivariate_sample <- function(data, ...) {\n    42→  UseMethod(\"as_forecast_multivariate_sample\")\n    43→}\n    44→# nolint end\n    45→\n    46→\n    47→#' @rdname as_forecast_multivariate_sample\n    48→#' @param sample_id (optional) Name of the column in `data` that contains the\n    49→#'   sample id. This column will be renamed to \"sample_id\".\n    50→#' @param joint_across Character vector with columns names that define the\n    51→#'   variables which are forecasted jointly. Conceptually, several univariate\n    52→#'   forecasts are pooled together to form a single multivariate forecasts.\n    53→#'   For example, if you have a column `country` and want to define\n    54→#'   a multivariate forecast for several countries at once, you could set\n    55→#'   `joint_across = \"country\"`.\n    56→#' @export\n    57→#' @importFrom cli cli_warn\n    58→as_forecast_multivariate_sample.default <- function(data,\n    59→                                                    joint_across,\n    60→                                                    forecast_unit = NULL,\n    61→                                                    observed = NULL,\n    62→                                                    predicted = NULL,\n    63→                                                    sample_id = NULL,\n    64→                                                    ...) {\n    65→  data <- as_forecast_generic(\n    66→    data,\n    67→    forecast_unit,\n    68→    observed = observed,\n    69→    predicted = predicted,\n    70→    sample_id = sample_id\n    71→  )\n    72→  data <- set_grouping(data, joint_across)\n    73→\n    74→  data <- new_forecast(data, \"forecast_sample_multivariate\")\n    75→  assert_forecast(data)\n    76→  return(data)\n    77→}\n    78→\n    79→\n    80→#' @export\n    81→#' @rdname assert_forecast\n    82→#' @importFrom cli cli_abort qty\n    83→#' @keywords validate-forecast-object\n    84→assert_forecast.forecast_sample_multivariate <- function(\n    85→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    86→) {\n    87→  assert(check_columns_present(forecast, c(\"sample_id\", \".mv_group_id\")))\n    88→  forecast <- assert_forecast_generic(forecast, verbose)\n    89→\n    90→  # make sure that for every .mv_group_id, the number of samples per\n    91→  # forecast unit is the same\n    92→  sample_lengths <- as.data.table(forecast)[, .(\n    93→    .scoringutils_N = length(sample_id)\n    94→  ),\n    95→  by = c(get_forecast_unit(forecast), \".mv_group_id\")\n    96→  ]\n    97→  group_variations <- sample_lengths[, .(\n    98→    .scoringutils_N = length(unique(.scoringutils_N))\n    99→  ),\n   100→  by = .mv_group_id\n   101→  ]\n   102→  if (any(group_variations$.scoringutils_N > 1)) {\n   103→    # nolint start: object_usage_linter\n   104→    problematic_groups <- group_variations[.scoringutils_N > 1, .mv_group_id]\n   105→    cli_abort(\n   106→      \"Found the following {qty(length(problematic_groups))} group{?s} with an\n   107→      inconsistent sample length, compared to other groups:\n   108→      {.val {problematic_groups}}\"\n   109→    )\n   110→    # nolint end\n   111→  }\n   112→\n   113→  assert_forecast_type(forecast, actual = \"forecast_sample_multivariate\", desired = forecast_type)\n   114→  return(invisible(NULL))\n   115→}\n   116→\n   117→\n   118→#' @export\n   119→#' @rdname is_forecast\n   120→# nolint start: object_name_linter\n   121→is_forecast_sample_multivariate <- function(x) {\n   122→  inherits(x, \"forecast_sample_multivariate\") && inherits(x, \"forecast\")\n   123→}\n   124→# nolint end\n   125→\n   126→\n   127→#' @importFrom stats na.omit\n   128→#' @importFrom data.table setattr copy\n   129→#' @importFrom methods formalArgs\n   130→#' @rdname score\n   131→#' @export\n   132→score.forecast_sample_multivariate <- function(forecast, metrics = get_metrics(forecast), ...) {\n   133→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n   134→  forecast_unit <- get_forecast_unit(forecast)\n   135→  metrics <- validate_metrics(metrics)\n   136→  forecast <- as.data.table(forecast)\n   137→\n   138→  # transpose the forecasts that belong to the same forecast unit\n   139→  f_transposed <- forecast[, .(\n   140→    predicted = list(predicted),\n   141→    observed = unique(observed),\n   142→    .scoringutils_N = length(sample_id)\n   143→  ),\n   144→  by = c(forecast_unit, \".mv_group_id\")\n   145→  ]\n   146→\n   147→  # split according to number of samples and do calculations for different\n   148→  # sample lengths separately\n   149→  f_split <- split(f_transposed, f_transposed$.scoringutils_N)\n   150→\n   151→  split_result <- lapply(f_split, function(forecast_same_length) {\n   152→    observed <- forecast_same_length$observed\n   153→    predicted <- do.call(rbind, forecast_same_length$predicted)\n   154→    forecast_same_length[, c(\"observed\", \"predicted\", \".scoringutils_N\") := NULL]\n   155→\n   156→    mv_group_id <- forecast_same_length$.mv_group_id\n   157→\n   158→    # for multivariate scores, multiple rows collapse to a single score.\n   159→    # we therefore have to create a new data.table with the correct dimensions\n   160→    # and groups\n   161→    grouping_cols <- get_grouping(forecast_same_length)\n   162→    temp_dt <- unique(forecast_same_length[, .SD, .SDcols = c(grouping_cols, \".mv_group_id\")])\n   163→    result <- apply_metrics(\n   164→      temp_dt,\n   165→      metrics = metrics,\n   166→      observed, predicted,\n   167→      mv_group_id\n   168→    )\n   169→\n   170→    setcolorder(result, c(setdiff(colnames(result), \".mv_group_id\"), \".mv_group_id\"))\n   171→\n   172→    return(result)\n   173→  })\n   174→  scores <- rbindlist(split_result, fill = TRUE)\n   175→  scores <- as_scores(scores, metrics = names(metrics))\n   176→\n   177→  return(scores[])\n   178→}\n   179→\n   180→\n   181→#' Get default metrics for sample-based forecasts\n   182→#'\n   183→#' @description\n   184→#' For sample-based multivariate forecasts, the default scoring rules are:\n   185→#' - \"energy_score\" = [energy_score_multivariate()]\n   186→#' @inheritSection illustration-input-metric-sample Input format\n   187→#' @inheritParams get_metrics.forecast_binary\n   188→#' @export\n   189→#' @family get_metrics functions\n   190→#' @keywords handle-metrics\n   191→#' @examples\n   192→#' example <- as_forecast_multivariate_sample(\n   193→#'   example_sample_continuous, joint_across = c(\"location\", \"location_name\")\n   194→#' )\n   195→#' get_metrics(example)\n   196→get_metrics.forecast_sample_multivariate <- function(x, select = NULL, exclude = NULL, ...) {\n   197→  all <- list(\n   198→    energy_score = energy_score_multivariate\n   199→  )\n   200→  select_metrics(all, select, exclude)\n   201→}\n   202→\n   203→#' @title Set grouping\n   204→#' @description\n   205→#' Helper function to set the grouping of a forecast.\n   206→#' @inheritParams as_forecast_doc_template\n   207→#' @inheritParams as_forecast_multivariate_sample\n   208→#' @importFrom data.table ':=' is.data.table copy setkeyv key\n   209→#' @importFrom checkmate assert_character assert_subset\n   210→#' @importFrom cli cli_abort\n   211→#' @return\n   212→#' A data.table with an additional column `.mv_group_id` that\n   213→#' contains the group id for each row.\n   214→#' @keywords internal\n   215→set_grouping <- function(data, joint_across) {\n   216→  data <- ensure_data.table(data)\n   217→  assert_character(joint_across, min.len = 1)\n   218→  assert_subset(joint_across, colnames(data))\n   219→\n   220→  by <- setdiff(get_forecast_unit(data), joint_across)\n   221→\n   222→  data[, .mv_group_id := .GRP, by = by]\n   223→\n   224→  existing_keys <- key(data)\n   225→  data[, .scoringutils_count := .N, by = eval(get_forecast_unit(data))]\n   226→\n   227→  for (mv_group_id in unique(data$.mv_group_id)) {\n   228→    counts <- data[.mv_group_id == mv_group_id, .scoringutils_count]\n   229→    unique_counts <- unique(counts)\n   230→\n   231→    if (length(unique_counts) > 1) {\n   232→      cli_abort(\n   233→        \"All univariate forecasts (as defined by the univariate forecast unit)\n   234→        in a group must have the same number of samples.\n   235→        This is not the case for group\n   236→        {.val {mv_group_id}}. Seeing {.val {unique_counts}} samples.\"\n   237→      )\n   238→    }\n   239→  }\n   240→  setkeyv(data, existing_keys)\n   241→  data[, .scoringutils_count := NULL]\n   242→  return(data)\n   243→}\n   244→\n   245→\n   246→#' @title Get grouping for a multivariate forecast\n   247→#' @description\n   248→#' Helper function to get the grouping for a multivariate forecast.\n   249→#' @inheritParams score\n   250→#' @return\n   251→#' A character vector with the names of the columns that define the grouping.\n   252→#' @export\n   253→#' @keywords diagnose-inputs\n   254→get_grouping <- function(forecast) {\n   255→  if (!(\".mv_group_id\" %in% names(forecast))) {\n   256→    cli_abort(\"No column `.mv_group_id` found in the forecast object.\")\n   257→  }\n   258→  data <- as.data.table(forecast)\n   259→  # this iterates over every column, and for every column checks if there\n   260→  # is always only one unique value per group specified by .mv_group_id\n   261→  # if that is the case, the column is part of the grouping vector.\n   262→  grouping_cols <- names(data)[sapply(names(data), function(col) {\n   263→    data[, all(length(unique(.SD[[col]])) == 1), by = \".mv_group_id\"][, all(V1)]\n   264→  })]\n   265→  return(grouping_cols)\n   266→}\n   267→\n   268→\n   269→#' Multivariate forecast example data\n   270→#'\n   271→#' A data set with continuous multivariate predictions for COVID-19 cases and\n   272→#' deaths constructed from data submitted to the European Forecast Hub.\n   273→#'\n   274→#' The data was created using the script create-example-data.R in the inst/\n   275→#' folder (or the top level folder in a compiled package).\n   276→#'\n   277→#' @format An object of class `forecast_sample_multivariate`\n   278→#' (see [as_forecast_multivariate_sample()]) with the following columns:\n   279→#' \\describe{\n   280→#'   \\item{location}{the country for which a prediction was made}\n   281→#'   \\item{target_end_date}{the date for which a prediction was made}\n   282→#'   \\item{target_type}{the target to be predicted (cases or deaths)}\n   283→#'   \\item{observed}{observed values}\n   284→#'   \\item{location_name}{name of the country for which a prediction was made}\n   285→#'   \\item{forecast_date}{the date on which a prediction was made}\n   286→#'   \\item{model}{name of the model that generated the forecasts}\n   287→#'   \\item{horizon}{forecast horizon in weeks}\n   288→#'   \\item{predicted}{predicted value}\n   289→#'   \\item{sample_id}{id for the corresponding sample}\n   290→#'   \\item{.mv_group_id}{id for the corresponding group}\n   291→#' }\n   292→# nolint start\n   293→#' @source \\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/}\n   294→# nolint end\n   295→\"example_multivariate_sample\"\n   296→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"9783fc66-b4b1-4699-bb13-b26a927049c0"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013frMG9W2berWiH576jzVEX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VehcTrXqnsaxr2bfxg4DEp","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":528,"cache_read_input_tokens":11757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":528},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"4a72a135-efd7-4383-bd7d-49c248fd3919"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VehcTrXqnsaxr2bfxg4DEp","type":"tool_result","content":"     1→#' Common functionality for `as_forecast_<type>` functions\n     2→#' @details This function splits out part of the functionality of\n     3→#' `as_forecast_<type>` that is the same for all `as_forecast_<type>` functions.\n     4→#' It renames the required columns, where appropriate, and sets the forecast\n     5→#' unit.\n     6→#' @inheritParams as_forecast_doc_template\n     7→#' @param ... Named arguments that are used to rename columns. The names of the\n     8→#'  arguments are the names of the columns that should be renamed. The values\n     9→#'  are the new names.\n    10→#' @keywords as_forecast\n    11→as_forecast_generic <- function(data,\n    12→                                forecast_unit = NULL,\n    13→                                ...) {\n    14→  data <- ensure_data.table(data)\n    15→  oldnames <- list(...)\n    16→  newnames <- names(oldnames)\n    17→  provided <- !sapply(oldnames, is.null)\n    18→\n    19→  lapply(seq_along(oldnames), function(i) {\n    20→    var <- oldnames[[i]]\n    21→    varname <- names(oldnames)[i]\n    22→    assert_character(var, len = 1, null.ok = TRUE, .var.name = varname)\n    23→    assert_subset(var, names(data), empty.ok = TRUE, .var.name = varname)\n    24→  })\n    25→\n    26→  oldnames <- unlist(oldnames[provided])\n    27→  newnames <- unlist(newnames[provided])\n    28→  if (!is.null(oldnames) && length(oldnames) > 0) {\n    29→    setnames(data, old = oldnames, new = newnames)\n    30→  }\n    31→\n    32→  # set forecast unit (error handling is done in `set_forecast_unit()`)\n    33→  if (!is.null(forecast_unit)) {\n    34→    data <- set_forecast_unit(data, forecast_unit)\n    35→  }\n    36→  return(data)\n    37→}\n    38→\n    39→\n    40→#' @title Assert that input is a forecast object and passes validations\n    41→#'\n    42→#' @description\n    43→#' Assert that an object is a forecast object (i.e. a `data.table` with a class\n    44→#' `forecast` and an additional class `forecast_<type>` corresponding to the\n    45→#' forecast type).\n    46→#'\n    47→#' See the corresponding `assert_forecast_<type>` functions for more details on\n    48→#' the required input formats.\n    49→#'\n    50→#' @inheritParams as_forecast_doc_template\n    51→#' @inheritParams score\n    52→#' @param forecast_type (optional) The forecast type you expect the forecasts\n    53→#'   to have. If the forecast type as determined by `scoringutils` based on the\n    54→#'   input does not match this, an error will be thrown. If `NULL` (the\n    55→#'   default), the forecast type will be inferred from the data.\n    56→#' @param verbose Logical. If `FALSE` (default is `TRUE`), no messages and\n    57→#'   warnings will be created.\n    58→#' @return\n    59→#' Returns `NULL` invisibly.\n    60→#' @importFrom data.table ':=' is.data.table\n    61→#' @importFrom checkmate assert_data_frame\n    62→#' @export\n    63→#' @keywords validate-forecast-object\n    64→#' @examples\n    65→#' forecast <- as_forecast_binary(example_binary)\n    66→#' assert_forecast(forecast)\n    67→assert_forecast <- function(\n    68→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    69→) {\n    70→  UseMethod(\"assert_forecast\")\n    71→}\n    72→\n    73→\n    74→#' @importFrom cli cli_abort\n    75→#' @rdname assert_forecast\n    76→#' @export\n    77→#' @keywords validate-forecast-object\n    78→assert_forecast.default <- function(\n    79→  forecast, forecast_type = NULL, verbose = TRUE, ...\n    80→) {\n    81→  cli_abort(\n    82→    #nolint start: keyword_quote_linter\n    83→    c(\n    84→      \"!\" = \"The input needs to be a valid forecast object.\",\n    85→      \"i\" = \"Please convert to `forecast` object first by calling the\n    86→      appropriate {.fn as_forecast_<type>} function).\"\n    87→    )\n    88→    #nolint end\n    89→  )\n    90→}\n    91→\n    92→\n    93→#' @title Validation common to all forecast types\n    94→#'\n    95→#' @description\n    96→#' The function runs input checks that apply to all input data, regardless of\n    97→#' forecast type. The function\n    98→#' - asserts that the forecast is a data.table which has columns `observed` and\n    99→#' `predicted`\n   100→#' - checks the forecast type and forecast unit\n   101→#' - checks there are no duplicate forecasts\n   102→#' - if appropriate, checks the number of samples / quantiles is the same\n   103→#' for all forecasts.\n   104→#' @param data A data.table with forecasts and observed values that should\n   105→#' be validated.\n   106→#' @inheritParams assert_forecast\n   107→#' @returns returns the input\n   108→#' @importFrom data.table ':=' is.data.table\n   109→#' @importFrom checkmate assert_data_table\n   110→#' @importFrom cli cli_abort cli_inform cli_warn\n   111→#' @keywords internal_input_check\n   112→assert_forecast_generic <- function(data, verbose = TRUE) {\n   113→  # check that data is a data.table and that the columns look fine\n   114→  assert_data_table(data, min.rows = 1)\n   115→  assert(check_columns_present(data, c(\"observed\", \"predicted\")))\n   116→  problem <- test_columns_present(data, c(\"sample_id\", \"quantile_level\"))\n   117→  if (problem) {\n   118→    cli_abort(\n   119→      c(\n   120→        \"!\" = \"Found columns `quantile_level` and `sample_id`.\n   121→      Only one of these is allowed\"\n   122→      )\n   123→    )\n   124→  }\n   125→\n   126→  # check that there aren't any duplicated forecasts\n   127→  forecast_unit <- get_forecast_unit(data)\n   128→  assert(check_duplicates(data))\n   129→\n   130→  # check that the number of forecasts per sample / quantile level is the same\n   131→  number_quantiles_samples <- check_number_per_forecast(data, forecast_unit)\n   132→  if (!isTRUE(number_quantiles_samples) && verbose) {\n   133→    cli_warn(number_quantiles_samples)\n   134→  }\n   135→\n   136→  # check whether there are any NA values\n   137→  if (anyNA(data)) {\n   138→    if (nrow(na.omit(data)) == 0) {\n   139→      #nolint start: keyword_quote_linter\n   140→      cli_abort(\n   141→        c(\n   142→          \"!\" = \"After removing rows with NA values in the data, no forecasts\n   143→          are left.\"\n   144→        )\n   145→      )\n   146→    }\n   147→    if (verbose) {\n   148→      cli_inform(\n   149→        c(\n   150→          \"i\" = \"Some rows containing NA values may be removed.\n   151→        This is fine if not unexpected.\"\n   152→        )\n   153→      )\n   154→    }\n   155→    #nolint end\n   156→  }\n   157→\n   158→  return(data[])\n   159→}\n   160→\n   161→\n   162→#' Check that all forecasts have the same number of rows\n   163→#' @description\n   164→#' Helper function that checks the number of rows (corresponding e.g to\n   165→#' quantiles or samples) per forecast.\n   166→#' If the number of quantiles or samples is the same for all forecasts, it\n   167→#' returns TRUE and a string with an error message otherwise.\n   168→#' @param forecast_unit Character vector denoting the unit of a single forecast.\n   169→#' @importFrom checkmate assert_subset\n   170→#' @inherit document_check_functions params return\n   171→#' @keywords internal_input_check\n   172→check_number_per_forecast <- function(data, forecast_unit) {\n   173→  # This function doesn't return a forecast object so it's fine to unclass it\n   174→  # to avoid validation error while subsetting\n   175→  data <- as.data.table(data)\n   176→  data <- na.omit(data)\n   177→  # check whether there are the same number of quantiles, samples --------------\n   178→  data[, scoringutils_InternalNumCheck := length(predicted), by = forecast_unit]\n   179→  n <- unique(data$scoringutils_InternalNumCheck)\n   180→  data[, scoringutils_InternalNumCheck := NULL]\n   181→  if (length(n) > 1) {\n   182→    msg <- paste0(\n   183→      \"Some forecasts have different numbers of rows \",\n   184→      \"(e.g. quantiles or samples). \",\n   185→      \"scoringutils found: \", toString(n),\n   186→      \". This may be a problem (it can potentially distort scores, \",\n   187→      \"making it more difficult to compare them), \",\n   188→      \"so make sure this is intended.\"\n   189→    )\n   190→    return(msg)\n   191→  }\n   192→  return(TRUE)\n   193→}\n   194→\n   195→\n   196→\n   197→#' Clean forecast object\n   198→#' @description\n   199→#' The function makes it possible to silently validate an object. In addition,\n   200→#' it can return a copy of the data and remove rows with missing values.\n   201→#'\n   202→#' @inheritParams score\n   203→#' @param copy Logical, default is `FALSE`. If `TRUE`, a copy of the input data\n   204→#' is created.\n   205→#' @param na.omit Logical, default is `FALSE`. If `TRUE`, rows with missing\n   206→#' values are removed.\n   207→#' @importFrom data.table copy\n   208→#' @importFrom stats na.omit\n   209→#' @keywords internal\n   210→clean_forecast <- function(forecast, copy = FALSE, na.omit = FALSE) {\n   211→  if (copy) {\n   212→    forecast <- copy(forecast)\n   213→  }\n   214→  assert_forecast(forecast, verbose = FALSE)\n   215→  if (na.omit) {\n   216→    forecast <- na.omit(forecast)\n   217→  }\n   218→  return(forecast)\n   219→}\n   220→\n   221→\n   222→#' @title Class constructor for `forecast` objects\n   223→#'\n   224→#' @description\n   225→#' Construct a class based on a data.frame or similar. The constructor\n   226→#' - coerces the data into a data.table\n   227→#' - assigns a class\n   228→#'\n   229→#' @inheritParams as_forecast_doc_template\n   230→#' @param classname name of the class to be created\n   231→#' @returns An object of the class indicated by `classname`\n   232→#' @export\n   233→#' @keywords internal\n   234→new_forecast <- function(data, classname) {\n   235→  data <- as.data.table(data)\n   236→  class(data) <- c(classname, \"forecast\", class(data))\n   237→  data <- copy(data)\n   238→  return(data[])\n   239→}\n   240→\n   241→\n   242→#' @title Test whether an object is a forecast object\n   243→#'\n   244→#' @description\n   245→#' Test whether an object is a forecast object.\n   246→#'\n   247→#' You can test for a specific `forecast_<type>` class using the appropriate\n   248→#' `is_forecast_<type>` function.\n   249→#'\n   250→#' @param x An R object.\n   251→#' @return\n   252→#' *`is_forecast`*: `TRUE` if the object is of class `forecast`,\n   253→#' `FALSE` otherwise.\n   254→#'\n   255→#' *`is_forecast_<type>*`*: `TRUE` if the object is of class `forecast_*` in addition\n   256→#' to class `forecast`, `FALSE` otherwise.\n   257→#' @export\n   258→#' @keywords validate-forecast-object\n   259→#' @examples\n   260→#' forecast_binary <- as_forecast_binary(example_binary)\n   261→#' is_forecast(forecast_binary)\n   262→is_forecast <- function(x) {\n   263→  inherits(x, \"forecast\")\n   264→}\n   265→\n   266→\n   267→#' @export\n   268→`[.forecast` <- function(x, ...) {\n   269→\n   270→  out <- NextMethod()\n   271→\n   272→  # (identical(x, out) && ...length() == 1) is the best way I have found to\n   273→  #   selectively catch x[], which we don't want to revalidate. ...length()\n   274→  #   alone will skip cases with dplyr verbs and identical alone will skip cases\n   275→  #   where we used data.table := operator which will turn x into out before we\n   276→  #   arrive to this function.\n   277→  is_dt_force_print <- identical(x, out) && ...length() == 1\n   278→  #   ...length() as it still returns 1 in x[] and then skips validations in\n   279→  #   undesired situation if we set ...length() > 1\n   280→  # is.data.table: when [.data.table returns an atomic vector, it's clear it\n   281→  #   cannot be a valid forecast object, and it is likely intended by the user\n   282→\n   283→  # in addition, we also check for a maximum length. The reason is that\n   284→  # print.data.table will internally subset the data.table before printing.\n   285→  # this subsetting triggers the validation, which is not desired in this case.\n   286→  # this is a hack and ideally, we'd do things differently.\n   287→  if (nrow(out) > 30 && data.table::is.data.table(out) && !is_dt_force_print) {\n   288→    # check whether subset object passes validation\n   289→    validation <- try(\n   290→      assert_forecast(forecast = out, verbose = FALSE),\n   291→      silent = TRUE\n   292→    )\n   293→    if (inherits(validation, \"try-error\")) {\n   294→      cli_warn(\n   295→        #nolint start: keyword_quote_linter\n   296→        c(\n   297→          \"!\" = \"Error in validating forecast object: {validation}.\",\n   298→          \"i\" = \"Note this error is sometimes related to `data.table`s `print`.\n   299→          Run {.help [{.fun assert_forecast}](scoringutils::assert_forecast)}\n   300→          to confirm. To get rid of this warning entirely,\n   301→          call `as.data.table()` on the forecast object.\"\n   302→        )\n   303→        #nolint end\n   304→      )\n   305→    }\n   306→  }\n   307→\n   308→  return(out)\n   309→\n   310→}\n   311→\n   312→\n   313→#' @export\n   314→`$<-.forecast` <- function(x, ..., value) {\n   315→\n   316→  out <- NextMethod()\n   317→\n   318→  # check whether subset object passes validation\n   319→  validation <- try(\n   320→    assert_forecast(forecast = out, verbose = FALSE),\n   321→    silent = TRUE\n   322→  )\n   323→  if (inherits(validation, \"try-error\")) {\n   324→    cli_warn(\n   325→      c(\n   326→        \"!\" = \"Error in validating forecast object: {validation}\"\n   327→      )\n   328→    )\n   329→  }\n   330→\n   331→  return(out)\n   332→\n   333→}\n   334→\n   335→\n   336→#' @export\n   337→`[[<-.forecast` <- function(x, ..., value) {\n   338→\n   339→  out <- NextMethod()\n   340→\n   341→  # check whether subset object passes validation\n   342→  validation <- try(\n   343→    assert_forecast(forecast = out, verbose = FALSE),\n   344→    silent = TRUE\n   345→  )\n   346→  if (inherits(validation, \"try-error\")) {\n   347→    cli_warn(\n   348→      c(\n   349→        \"!\" = \"Error in validating forecast object: {validation}\"\n   350→      )\n   351→    )\n   352→  }\n   353→\n   354→  return(out)\n   355→\n   356→}\n   357→\n   358→\n   359→#' @export\n   360→`[<-.forecast` <- function(x, ..., value) {\n   361→\n   362→  out <- NextMethod()\n   363→\n   364→  # check whether subset object passes validation\n   365→  validation <- try(\n   366→    assert_forecast(forecast = out, verbose = FALSE),\n   367→    silent = TRUE\n   368→  )\n   369→  if (inherits(validation, \"try-error\")) {\n   370→    cli_warn(\n   371→      c(\n   372→        \"!\" = \"Error in validating forecast object: {validation}\"\n   373→      )\n   374→    )\n   375→  }\n   376→\n   377→  return(out)\n   378→\n   379→}\n   380→\n   381→\n   382→#' @export\n   383→#' @importFrom utils head\n   384→head.forecast <- function(x, ...) {\n   385→  # We use this custom method just to unclass before forwarding to avoid\n   386→  # validation when we expect (and don't care) that objects are invalidated\n   387→  head(as.data.table(x), ...)\n   388→}\n   389→\n   390→\n   391→#' @export\n   392→#' @importFrom utils tail\n   393→tail.forecast <- function(x, ...) {\n   394→  # We use this custom method just to unclass before forwarding to avoid\n   395→  # validation when we expect (and don't care) that objects are invalidated\n   396→  utils::tail(as.data.table(x), ...)\n   397→}\n   398→\n   399→\n   400→#' @title Print information about a forecast object\n   401→#' @description\n   402→#' This function prints information about a forecast object,\n   403→#' including \"Forecast type\", \"Score columns\",\n   404→#' \"Forecast unit\".\n   405→#'\n   406→#' @param x A forecast object\n   407→#' @param ... Additional arguments for [print()].\n   408→#' @returns Returns `x` invisibly.\n   409→#' @importFrom cli cli_inform cli_warn col_blue cli_text\n   410→#' @export\n   411→#' @keywords gain-insights\n   412→#' @examples\n   413→#' dat <- as_forecast_quantile(example_quantile)\n   414→#' print(dat)\n   415→print.forecast <- function(x, ...) {\n   416→\n   417→  # get forecast type, forecast unit and score columns\n   418→  forecast_type <- try(\n   419→    do.call(get_forecast_type, list(forecast = x)),\n   420→    silent = TRUE\n   421→  )\n   422→  forecast_unit <- try(\n   423→    do.call(get_forecast_unit, list(data = x)),\n   424→    silent = TRUE\n   425→  )\n   426→\n   427→  # Print forecast object information\n   428→  if (inherits(forecast_type, \"try-error\")) {\n   429→    cli_inform(\n   430→      c(\n   431→        \"!\" = \"Could not determine forecast type due to error in validation.\" #nolint\n   432→      )\n   433→    )\n   434→  } else {\n   435→    cli_text(\n   436→      col_blue(\n   437→        \"Forecast type: \"\n   438→      ),\n   439→      \"{forecast_type}\"\n   440→    )\n   441→  }\n   442→\n   443→  if (inherits(forecast_unit, \"try-error\")) {\n   444→    cli_inform(\n   445→      c(\n   446→        \"!\" = \"Could not determine forecast unit.\" #nolint\n   447→      )\n   448→    )\n   449→  } else {\n   450→    cli_text(\n   451→      col_blue(\n   452→        \"Forecast unit:\"\n   453→      )\n   454→    )\n   455→    cli_text(\n   456→      \"{forecast_unit}\"\n   457→    )\n   458→  }\n   459→\n   460→  cat(\"\\n\")\n   461→\n   462→  NextMethod()\n   463→\n   464→  return(invisible(x))\n   465→}\n   466→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"76429882-3d01-49ad-9c3a-071f8acfe2dd"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JkFRC7LbhXsxWmixpLn6ib","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012YBtsEaskwDsK3sAWXoaL1","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-binary.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":29758,"cache_read_input_tokens":12285,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":29758},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"d8c0bfc3-2936-4197-b8d4-88ef337b45e1"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012YBtsEaskwDsK3sAWXoaL1","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_binary() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→test_that(\"output of as_forecast_binary() is accepted as input to score()\", {\n     5→  check <- suppressMessages(as_forecast_binary(example_binary))\n     6→  score_check <- expect_no_error(score(na.omit(check)))\n     7→  expect_identical(score_check, suppressMessages(score(as_forecast_binary(example_binary))))\n     8→})\n     9→\n    10→\n    11→# ==============================================================================\n    12→# is_forecast_binary() # nolint: commented_code_linter\n    13→# ==============================================================================\n    14→test_that(\"is_forecast_binary() works as expected\", {\n    15→  expect_true(is_forecast_binary(example_binary))\n    16→  expect_false(is_forecast_binary(example_point))\n    17→  expect_false(is_forecast_binary(example_quantile))\n    18→  expect_false(is_forecast_binary(example_sample_continuous))\n    19→  expect_false(is_forecast_binary(example_nominal))\n    20→})\n    21→\n    22→\n    23→# ==============================================================================\n    24→# assert_forecast.forecast_binary() # nolint: commented_code_linter\n    25→# ==============================================================================\n    26→test_that(\"assert_forecast.forecast_binary works as expected\", {\n    27→  test <- na.omit(as.data.table(example_binary))\n    28→  test[, \"sample_id\" := seq_len(nrow(test))]\n    29→\n    30→  # error if there is a superfluous sample_id column\n    31→  expect_error(\n    32→    as_forecast_binary(test),\n    33→    paste(\n    34→      \"Input looks like a binary forecast, but an additional column\",\n    35→      \"called `sample_id` or `quantile` was found.\"\n    36→    )\n    37→  )\n    38→\n    39→  # expect error if probabilties are not in [0, 1]\n    40→  test <- na.omit(as.data.table(example_binary))\n    41→  test[, \"predicted\" := predicted + 1]\n    42→  expect_error(\n    43→    as_forecast_binary(test),\n    44→    \"Input looks like a binary forecast, but found the following issue\"\n    45→  )\n    46→})\n    47→\n    48→\n    49→\n    50→# ==============================================================================\n    51→# score.forecast_binary() # nolint: commented_code_linter\n    52→# ==============================================================================\n    53→test_that(\"function produces output for a binary case\", {\n    54→  expect_named(\n    55→    scores_binary,\n    56→    c(get_forecast_unit(example_binary), names(get_metrics(example_binary)))\n    57→  )\n    58→\n    59→  eval <- summarise_scores(scores_binary, by = c(\"model\", \"target_type\"))\n    60→\n    61→  expect_gt(\n    62→    nrow(eval), 1\n    63→  )\n    64→  expect_identical(\n    65→    colnames(eval),\n    66→    c(\n    67→      \"model\", \"target_type\",\n    68→      \"brier_score\",\n    69→      \"log_score\"\n    70→    )\n    71→  )\n    72→\n    73→  expect_true(\"brier_score\" %in% names(eval))\n    74→\n    75→  expect_s3_class(eval, c(\"scores\", \"data.table\", \"data.frame\"), exact = TRUE)\n    76→})\n    77→\n    78→test_that(\"score.forecast_binary() errors with only NA values\", {\n    79→  # [.forecast()` will warn even before score()\n    80→  only_nas <- suppressWarnings(\n    81→    copy(example_binary)[, predicted := NA_real_]\n    82→  )\n    83→  expect_error(\n    84→    score(only_nas),\n    85→    \"After removing rows with NA values in the data, no forecasts are left.\"\n    86→  )\n    87→})\n    88→\n    89→test_that(\"score() gives same result for binary as regular function\", {\n    90→  manual_eval <- brier_score(\n    91→    factor(example_binary$observed),\n    92→    example_binary$predicted\n    93→  )\n    94→  expect_equal( # nolint: expect_identical_linter\n    95→    scores_binary$brier_score, manual_eval[!is.na(manual_eval)]\n    96→  )\n    97→})\n    98→\n    99→test_that(\n   100→  \"passing additional functions to score binary works handles them\",\n   101→  {\n   102→    test_fun <- function(x, y, ...) {\n   103→      if (hasArg(\"test\")) {\n   104→        message(\"test argument found\")\n   105→      }\n   106→      return(y)\n   107→    }\n   108→\n   109→    df <- as_forecast_binary(\n   110→      example_binary[model == \"EuroCOVIDhub-ensemble\" &\n   111→                       target_type == \"Cases\" & location == \"DE\"]\n   112→    )\n   113→\n   114→    # passing a simple function works\n   115→    expect_identical(\n   116→      score(df,\n   117→        metrics = list(identity = function(x, y) {\n   118→          return(y)\n   119→        })\n   120→      )$identity,\n   121→      df$predicted\n   122→    )\n   123→  }\n   124→)\n   125→\n   126→# ==============================================================================\n   127→# get_metrics.forecast_binary() # nolint: commented_code_linter\n   128→# ==============================================================================\n   129→\n   130→test_that(\"get_metrics.forecast_binary() works as expected\", {\n   131→  expect_type(\n   132→    get_metrics(example_binary), \"list\"\n   133→  )\n   134→})\n   135→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"61ef27cd-24e0-4bee-9e81-8e80e47f9444"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JkFRC7LbhXsxWmixpLn6ib","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WACmDASmULjQRaJSKwwKVX","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-point.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":29758,"cache_read_input_tokens":12285,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":29758},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"c0cbc89e-9e34-4dbd-9c58-e9742ee28c73"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WACmDASmULjQRaJSKwwKVX","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_point() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→\n     5→test_that(\"as_forecast_point() works\", {\n     6→  expect_no_condition(\n     7→    as_forecast_point(as_forecast_quantile(na.omit(example_quantile)))\n     8→  )\n     9→})\n    10→\n    11→\n    12→# ==============================================================================\n    13→# is_forecast_point() # nolint: commented_code_linter\n    14→# ==============================================================================\n    15→test_that(\"is_forecast_point() works as expected\", {\n    16→  expect_true(is_forecast_point(example_point))\n    17→  expect_false(is_forecast_point(example_binary))\n    18→  expect_false(is_forecast_point(example_quantile))\n    19→  expect_false(is_forecast_point(example_sample_continuous))\n    20→  expect_false(is_forecast_point(example_nominal))\n    21→})\n    22→\n    23→\n    24→# ==============================================================================\n    25→# assert_forecast.forecast_point() # nolint: commented_code_linter\n    26→# ==============================================================================\n    27→\n    28→test_that(\"assert_forecast.forecast_point() works as expected\", {\n    29→  test <- na.omit(data.table::as.data.table(example_point))\n    30→  test <- as_forecast_point(test)\n    31→\n    32→  # expect an error if column is changed to character after initial validation.\n    33→  expect_warning(\n    34→    test[, \"predicted\" := as.character(predicted)],\n    35→    \"Input looks like a point forecast, but found the following issue\"\n    36→  )\n    37→  expect_error(\n    38→    assert_forecast(test),\n    39→    \"Input looks like a point forecast, but found the following issue\"\n    40→  )\n    41→})\n    42→\n    43→test_that(\"assert_forecast.forecast_point() complains if the forecast type is wrong\", {\n    44→  expect_error(\n    45→    assert_forecast(na.omit(example_point), forecast_type = \"quantile\"),\n    46→    \"Forecast type determined by scoringutils based on input:\"\n    47→  )\n    48→})\n    49→\n    50→\n    51→# ==============================================================================\n    52→# score.forecast_point() # nolint: commented_code_linter\n    53→# ==============================================================================\n    54→test_that(\"function produces output for a point case\", {\n    55→  expect_named(\n    56→    scores_binary,\n    57→    c(get_forecast_unit(example_binary), names(get_metrics(example_binary)))\n    58→  )\n    59→\n    60→  eval <- summarise_scores(scores_point, by = c(\"model\", \"target_type\"))\n    61→\n    62→  expect_gt(\n    63→    nrow(eval), 1\n    64→  )\n    65→  expect_identical(\n    66→    colnames(eval),\n    67→    c(\"model\", \"target_type\", names(get_metrics(example_point)))\n    68→  )\n    69→\n    70→  expect_s3_class(eval, c(\"scores\", \"data.table\", \"data.frame\"), exact = TRUE)\n    71→})\n    72→\n    73→test_that(\"Changing metrics names works\", {\n    74→  metrics_test <- get_metrics(example_point)\n    75→  names(metrics_test)[1] <- \"just_testing\"\n    76→  eval <- suppressMessages(score(as_forecast_point(example_point),\n    77→    metrics = metrics_test\n    78→  ))\n    79→  eval_summarised <- summarise_scores(eval, by = \"model\")\n    80→  expect_identical(\n    81→    colnames(eval_summarised),\n    82→    c(\"model\", \"just_testing\", names(get_metrics(example_point))[-1])\n    83→  )\n    84→})\n    85→\n    86→test_that(\"score.forecast_point() errors with only NA values\", {\n    87→  # [.forecast()` will warn even before score()\n    88→  only_nas <- suppressWarnings(\n    89→    copy(example_point)[, predicted := NA_real_]\n    90→  )\n    91→  expect_error(\n    92→    score(only_nas),\n    93→    \"After removing rows with NA values in the data, no forecasts are left.\"\n    94→  )\n    95→})\n    96→\n    97→# ==============================================================================\n    98→# get_metrics.forecast_point() # nolint: commented_code_linter\n    99→# ==============================================================================\n   100→test_that(\"get_metrics.forecast_point() works as expected\", {\n   101→  expect_type(\n   102→    get_metrics(example_point), \"list\"\n   103→  )\n   104→\n   105→  expect_identical(\n   106→    get_metrics.scores(scores_point),\n   107→    c(\"ae_point\", \"se_point\", \"ape\")\n   108→  )\n   109→})\n   110→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"3989ef18-5cf0-4cff-90ed-98544fbfc3a8"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JkFRC7LbhXsxWmixpLn6ib","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BAiVw6Cfhz1dNJExrRcTgq","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-quantile.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":29758,"cache_read_input_tokens":12285,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":29758},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"0eda47eb-f876-41ff-965f-ca6a6991722a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BAiVw6Cfhz1dNJExrRcTgq","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_quantile() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→\n     5→test_that(\"as_forecast_quantile() works as expected\", {\n     6→  test <- na.omit(data.table::copy(example_quantile))\n     7→\n     8→  expect_s3_class(\n     9→    as_forecast_quantile(test),\n    10→    c(\"forecast_quantile\", \"forecast\", \"data.table\", \"data.frame\"),\n    11→    exact = TRUE\n    12→  )\n    13→\n    14→  # expect error when arguments are not correct\n    15→  expect_error(as_forecast_quantile(test, observed = 3), \"Must be of type 'character'\")\n    16→  expect_error(as_forecast_quantile(test, quantile_level = c(\"1\", \"2\")), \"Must have length 1\")\n    17→  expect_error(as_forecast_quantile(test, observed = \"missing\"), \"Must be a subset of\")\n    18→\n    19→  # expect no condition with columns already present\n    20→  expect_no_condition(\n    21→    as_forecast_quantile(test,\n    22→      observed = \"observed\", predicted = \"predicted\",\n    23→      forecast_unit = c(\n    24→        \"location\", \"model\", \"target_type\",\n    25→        \"target_end_date\", \"horizon\"\n    26→      ),\n    27→      quantile_level = \"quantile_level\"\n    28→    )\n    29→  )\n    30→})\n    31→\n    32→test_that(\"as_forecast_quantile() function works\", {\n    33→  check <- suppressMessages(as_forecast_quantile(example_quantile))\n    34→  expect_s3_class(check, \"forecast_quantile\")\n    35→})\n    36→\n    37→test_that(\"as_forecast_quantile() errors if there is both a sample_id and a quantile_level column\", {\n    38→  example <- as.data.table(example_quantile)[, sample_id := 1]\n    39→  expect_error(\n    40→    as_forecast_quantile(example),\n    41→    \"Found columns `quantile_level` and `sample_id`. Only one of these is allowed\"\n    42→  )\n    43→})\n    44→\n    45→test_that(\"as_forecast_quantile() warns if there are different numbers of quantiles\", {\n    46→  example <- as.data.table(example_quantile)[-1000, ]\n    47→  w <- suppressWarnings(as_forecast_quantile(na.omit(example)))\n    48→  expect_warning(\n    49→    as_forecast_quantile(na.omit(example)),\n    50→    \"Some forecasts have different numbers of rows\"\n    51→  )\n    52→  # printing should work without a warning because printing is silent\n    53→  expect_no_condition(w)\n    54→})\n    55→\n    56→test_that(\"as_forecast_quantile() function throws an error with duplicate forecasts\", {\n    57→  example <- rbind(\n    58→    example_quantile,\n    59→    example_quantile[1000:1010]\n    60→  )\n    61→\n    62→  expect_error(\n    63→    suppressMessages(suppressWarnings(as_forecast_quantile(example))),\n    64→    \"Assertion on 'data' failed: There are instances with more than one forecast for the same target. This can't be right and needs to be resolved. Maybe you need to check the unit of a single forecast and add missing columns? Use the function get_duplicate_forecasts() to identify duplicate rows.\", # nolint\n    65→    fixed = TRUE\n    66→  )\n    67→})\n    68→\n    69→test_that(\"as_forecast_quantile() function throws an error when no predictions or observed values are present\", {\n    70→  expect_error(\n    71→    suppressMessages(suppressWarnings(as_forecast_quantile(\n    72→      data.table::copy(example_quantile)[, predicted := NULL]\n    73→    ))),\n    74→    \"Assertion on 'data' failed: Column 'predicted' not found in data.\"\n    75→  )\n    76→\n    77→  expect_error(\n    78→    suppressMessages(suppressWarnings(as_forecast_quantile(\n    79→      data.table::copy(example_quantile)[, observed := NULL]\n    80→    ))),\n    81→    \"Assertion on 'data' failed: Column 'observed' not found in data.\"\n    82→  )\n    83→\n    84→  expect_error(\n    85→    suppressMessages(suppressWarnings(as_forecast_quantile(\n    86→      data.table::copy(example_quantile)[, c(\"observed\", \"predicted\") := NULL]\n    87→    ))),\n    88→    \"Assertion on 'data' failed: Columns 'observed', 'predicted' not found in data.\"\n    89→  )\n    90→})\n    91→\n    92→test_that(\"as_forecast_quantile() works with a data.frame\", {\n    93→  expect_no_condition(as_forecast_quantile(example_quantile_df))\n    94→})\n    95→\n    96→test_that(\"as_forecast_quantiles works\", {\n    97→  samples <- data.frame(\n    98→    date = as.Date(\"2020-01-01\") + 1:10,\n    99→    model = \"model1\",\n   100→    observed = 1:10,\n   101→    predicted = c(rep(0, 10), 2:11, 3:12, 4:13, rep(100, 10)),\n   102→    sample_id = rep(1:5, each = 10),\n   103→    stringsAsFactors = FALSE\n   104→  ) |>\n   105→    as_forecast_sample()\n   106→\n   107→  quantile <- data.frame(\n   108→    date = rep(as.Date(\"2020-01-01\") + 1:10, each = 2),\n   109→    model = \"model1\",\n   110→    observed = rep(1:10, each = 2),\n   111→    quantile_level = c(0.25, 0.75),\n   112→    predicted = rep(2:11, each = 2) + c(0, 2),\n   113→    stringsAsFactors = FALSE\n   114→  )\n   115→\n   116→  expect_no_condition(\n   117→    as_forecast_quantile(samples, probs = c(0.25, 0.75))\n   118→  )\n   119→\n   120→  wrongclass <- as_forecast_sample(samples)\n   121→  class(wrongclass) <- c(\"forecast_point\", \"data.table\", \"data.frame\")\n   122→  expect_error(\n   123→    as_forecast_quantile(wrongclass, quantile_level = c(0.25, 0.75)),\n   124→    \"Assertion on 'quantile_level' failed: Must be of type\"\n   125→  )\n   126→\n   127→\n   128→  quantile2 <- as_forecast_quantile(\n   129→    as_forecast_sample(samples),\n   130→    probs = c(0.25, 0.75)\n   131→  )\n   132→\n   133→  expect_identical(quantile, as.data.frame(quantile2))\n   134→\n   135→  # Verify that `type` is correctly scoped in as_forecast_quantile(), as it is\n   136→  # also an argument.\n   137→  # If it's not scoped well, the call to `as_forecast_quantile()` will fail.\n   138→  samples$type <- \"test\"\n   139→\n   140→  quantile3 <- as_forecast_quantile(\n   141→    as_forecast_sample(samples),\n   142→    probs = c(0.25, 0.75)\n   143→  )\n   144→  quantile3$type <- NULL\n   145→\n   146→  expect_identical(\n   147→    quantile2,\n   148→    quantile3\n   149→  )\n   150→})\n   151→\n   152→test_that(\"as_forecast_quantiles issue 557 fix\", {\n   153→  out <- example_sample_discrete |>\n   154→    na.omit() |>\n   155→    as_forecast_quantile(\n   156→      probs = c(0.01, 0.025, seq(0.05, 0.95, 0.05), 0.975, 0.99)\n   157→    ) |>\n   158→    score()\n   159→\n   160→  expect_false(anyNA(out$interval_coverage_deviation))\n   161→})\n   162→\n   163→test_that(\"as_forecast_quantile doesn't modify column names in place\", {\n   164→  quantile_data <- data.table(\n   165→    my_quantile = c(0.25, 0.5),\n   166→    forecast_value = c(1, 2),\n   167→    observed_value = c(5, 5)\n   168→  )\n   169→  pre <- names(quantile_data)\n   170→\n   171→  quantile_forecast <- as_forecast_quantile(\n   172→    quantile_data,\n   173→    predicted = \"forecast_value\",\n   174→    observed = \"observed_value\",\n   175→    quantile_level = \"my_quantile\"\n   176→  )\n   177→\n   178→  post <- names(quantile_data)\n   179→  expect_identical(pre, post)\n   180→})\n   181→\n   182→test_that(\"as_forecast_quantile handles rounding issues correctly\", {\n   183→  quantile_data <- data.table(\n   184→    quantile_level = c(0.25, 0.75 + 1e-13, 0.25, 0.75),\n   185→    predicted = c(1, 2, 3, 4),\n   186→    observed = c(5, 5, 5, 5),\n   187→    location = c(1, 1, 2, 2)\n   188→  )\n   189→  quantile_forecast <- suppressWarnings(as_forecast_quantile(quantile_data))\n   190→  expect_warning(\n   191→    as_forecast_quantile(quantile_data),\n   192→    \"rounding issue\"\n   193→  )\n   194→  expect_no_condition(\n   195→    score(quantile_forecast, metrics = c(wis = wis))\n   196→  )\n   197→})\n   198→\n   199→\n   200→\n   201→\n   202→\n   203→# ==============================================================================\n   204→# is_forecast_quantile() # nolint: commented_code_linter\n   205→# ==============================================================================\n   206→test_that(\"is_forecast_quantile() works as expected\", {\n   207→  expect_true(is_forecast_quantile(example_quantile))\n   208→  expect_false(is_forecast_quantile(example_binary))\n   209→  expect_false(is_forecast_quantile(example_point))\n   210→  expect_false(is_forecast_quantile(example_sample_continuous))\n   211→  expect_false(is_forecast_quantile(example_nominal))\n   212→})\n   213→\n   214→# ==============================================================================\n   215→# score.forecast_quantile() # nolint: commented_code_linter\n   216→# ==============================================================================\n   217→test_that(\"score_quantile correctly handles separate results = FALSE\", {\n   218→  df <- example_quantile[model == \"EuroCOVIDhub-ensemble\" &\n   219→                           target_type == \"Cases\" & location == \"DE\"]\n   220→  metrics <- get_metrics(example_quantile)\n   221→  metrics$wis <- purrr::partial(wis, separate_results = FALSE)\n   222→  eval <- score(df[!is.na(predicted)], metrics = metrics)\n   223→\n   224→  expect_gt(\n   225→    nrow(eval), 1\n   226→  )\n   227→  expect_true(all(names(get_metrics(example_quantile)) %in% colnames(eval)))\n   228→\n   229→  expect_s3_class(eval, c(\"scores\", \"data.table\", \"data.frame\"), exact = TRUE)\n   230→})\n   231→\n   232→\n   233→test_that(\"score() quantile produces desired metrics\", {\n   234→  data <- data.frame(\n   235→    observed = rep(1:10, each = 3),\n   236→    predicted = rep(c(-0.3, 0, 0.3), 10) + rep(1:10, each = 3),\n   237→    model = \"Model 1\",\n   238→    date = as.Date(\"2020-01-01\") + rep(1:10, each = 3),\n   239→    quantile_level = rep(c(0.1, 0.5, 0.9), times = 10),\n   240→    stringsAsFactors = FALSE\n   241→  )\n   242→\n   243→  data <- suppressWarnings(suppressMessages(as_forecast_quantile(data)))\n   244→\n   245→  out <- score(forecast = data, metrics = metrics_no_cov)\n   246→  metrics <- c(\n   247→    \"dispersion\", \"underprediction\", \"overprediction\",\n   248→    \"bias\", \"ae_median\"\n   249→  )\n   250→\n   251→  expect_true(all(metrics %in% colnames(out)))\n   252→})\n   253→\n   254→\n   255→test_that(\"calculation of ae_median is correct for a quantile format case\", {\n   256→  eval <- summarise_scores(scores_quantile, by = \"model\")\n   257→\n   258→  example <- as.data.table(example_quantile)\n   259→  ae <- example[quantile_level == 0.5, ae := abs(observed - predicted)][!is.na(model), .(mean = mean(ae, na.rm = TRUE)),\n   260→    by = \"model\"\n   261→  ]$mean\n   262→\n   263→  expect_equal( # nolint: expect_identical_linter\n   264→    sort(eval$ae_median), sort(ae)\n   265→  )\n   266→})\n   267→\n   268→\n   269→test_that(\"all quantile and range formats yield the same result\", {\n   270→  eval1 <- summarise_scores(scores_quantile, by = \"model\")\n   271→\n   272→  df <- as.data.table(example_quantile)\n   273→\n   274→  ae <- df[\n   275→    quantile_level == 0.5, ae := abs(observed - predicted)\n   276→  ][\n   277→    !is.na(model), .(mean = mean(ae, na.rm = TRUE)),\n   278→    by = \"model\"\n   279→  ]$mean\n   280→\n   281→  expect_equal( # nolint: expect_identical_linter\n   282→    sort(eval1$ae_median), sort(ae)\n   283→  )\n   284→})\n   285→\n   286→test_that(\"WIS is the same with other metrics omitted or included\", {\n   287→  eval <- score(example_quantile,\n   288→    metrics = list(wis = wis)\n   289→  )\n   290→\n   291→  eval2 <- scores_quantile\n   292→\n   293→  expect_equal( # nolint: expect_identical_linter\n   294→    sum(eval$wis),\n   295→    sum(eval2$wis)\n   296→  )\n   297→})\n   298→\n   299→\n   300→test_that(\"score.forecast_quantile() errors with only NA values\", {\n   301→  # [.forecast()` will warn even before score()\n   302→  only_nas <- suppressWarnings(\n   303→    copy(example_quantile)[, predicted := NA_real_]\n   304→  )\n   305→  expect_error(\n   306→    score(only_nas),\n   307→    \"After removing rows with NA values in the data, no forecasts are left.\"\n   308→  )\n   309→})\n   310→\n   311→test_that(\"score.forecast_quantile() works as expected in edge cases\", {\n   312→  # only the median\n   313→  onlymedian <- example_quantile[quantile_level == 0.5]\n   314→  s <- expect_no_condition(\n   315→    score(onlymedian, metrics = get_metrics(\n   316→      example_quantile,\n   317→      exclude = c(\"interval_coverage_50\", \"interval_coverage_90\")\n   318→    ))\n   319→  )\n   320→  expect_equal( # nolint: expect_identical_linter\n   321→    s$wis, abs(onlymedian$observed - onlymedian$predicted)\n   322→  )\n   323→\n   324→  # only one symmetric interval is present\n   325→  oneinterval <- example_quantile[quantile_level %in% c(0.25, 0.75)] |>\n   326→    as_forecast_quantile()\n   327→  s <- suppressMessages(score(\n   328→    oneinterval,\n   329→    metrics = get_metrics(\n   330→      example_quantile,\n   331→      exclude = c(\"interval_coverage_90\", \"ae_median\")\n   332→    )\n   333→  ))\n   334→  expect_message(\n   335→    score(\n   336→      oneinterval,\n   337→      metrics = get_metrics(\n   338→        example_quantile,\n   339→        exclude = c(\"interval_coverage_90\", \"ae_median\")\n   340→      )\n   341→    ),\n   342→    \"Median not available\"\n   343→  )\n   344→})\n   345→\n   346→test_that(\"score() works even if only some quantiles are missing\", {\n   347→  # only the median is there\n   348→  onlymedian <- example_quantile[quantile_level == 0.5]\n   349→  expect_no_condition(\n   350→    score(onlymedian, metrics = get_metrics(\n   351→      example_quantile,\n   352→      exclude = c(\"interval_coverage_50\", \"interval_coverage_90\")\n   353→    ))\n   354→  )\n   355→\n   356→  # asymmetric intervals\n   357→  asymm <- example_quantile[quantile_level <= 0.6]\n   358→  metrics <- get_metrics(\n   359→    example_quantile,\n   360→    exclude = c(\"overprediction\", \"underprediction\", \"dispersion\")\n   361→  )\n   362→  metrics$wis <- purrr::partial(wis, na.rm = TRUE)\n   363→  score_a <- suppressWarnings({\n   364→    scores_temp <- score(asymm, metrics = metrics)\n   365→    summarise_scores(scores_temp, by = \"model\")\n   366→  })\n   367→  expect_warning(\n   368→    expect_warning({\n   369→      scores_temp <- score(asymm, metrics = metrics)\n   370→      summarise_scores(scores_temp, by = \"model\")\n   371→    },\n   372→    \"Computation for `interval_coverage_50` failed.\"\n   373→    ),\n   374→    \"Computation for `interval_coverage_90` failed.\"\n   375→  )\n   376→\n   377→  # expect a failure with the regular wis wihtout ma.rm=TRUE\n   378→  expect_warning(\n   379→    score(asymm, metrics = c(wis = wis)),\n   380→    \"Not all quantile levels specified form symmetric prediction intervals.\"\n   381→  )\n   382→\n   383→  # check that the result is equal to a case where we discard the entire\n   384→  # interval in terms of WIS\n   385→  inner <- example_quantile[quantile_level %in% c(0.4, 0.45, 0.5, 0.55, 0.6)]\n   386→  score_b <- score(inner, metrics = c(wis = wis)) |>\n   387→    summarise_scores(by = \"model\")\n   388→  expect_equal( # nolint: expect_identical_linter\n   389→    score_a$wis,\n   390→    score_b$wis\n   391→  )\n   392→\n   393→  # median is not there, but only in a single model\n   394→  test <- data.table::copy(example_quantile)\n   395→  test_no_median <- test[model == \"epiforecasts-EpiNow2\" & quantile_level != 0.5, ]\n   396→  test <- rbind(test[model != \"epiforecasts-EpiNow2\"], test_no_median)\n   397→\n   398→  test <- suppressWarnings(as_forecast_quantile(test))\n   399→  expect_message(\n   400→    expect_warning(\n   401→      score(test),\n   402→      \"Computation for `ae_median` failed.\"\n   403→    ),\n   404→    \"interpolating median from the two innermost quantiles\"\n   405→  )\n   406→})\n   407→\n   408→# ==============================================================================\n   409→# get_metrics.forecast_quantile() # nolint: commented_code_linter\n   410→# ==============================================================================\n   411→test_that(\"get_metrics.forecast_quantile() works as expected\", {\n   412→  expect_type(\n   413→    get_metrics(example_quantile), \"list\"\n   414→  )\n   415→})\n   416→\n   417→\n   418→# ==============================================================================\n   419→# get_pit_histogram.forecast_quantile() # nolint: commented_code_linter\n   420→# ==============================================================================\n   421→test_that(\"get_pit_histogram.forecast_quantile() works as expected\", {\n   422→  pit_quantile <- get_pit_histogram(example_quantile, by = \"model\")\n   423→\n   424→  expect_named(pit_quantile, c(\"model\", \"density\", \"bin\", \"mid\"))\n   425→  expect_s3_class(pit_quantile, c(\"data.table\", \"data.frame\"), exact = TRUE)\n   426→\n   427→  # check printing works\n   428→  expect_output(print(pit_quantile))\n   429→})\n   430→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"d656909e-b0be-4e35-bb1e-0d3933a84fc2"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JkFRC7LbhXsxWmixpLn6ib","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Tyzw4v3ZAoxndXXsQhwUvV","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-sample.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":29758,"cache_read_input_tokens":12285,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":29758},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"46f443a5-32ca-49b9-91e3-4b00a3d8a238"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Tyzw4v3ZAoxndXXsQhwUvV","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_sample() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→test_that(\"as_forecast_sample() works as expected\", {\n     5→  test <- na.omit(data.table::copy(example_sample_continuous))\n     6→  data.table::setnames(test,\n     7→    old = c(\"observed\", \"predicted\", \"sample_id\"),\n     8→    new = c(\"obs\", \"pred\", \"sample\")\n     9→  )\n    10→  expect_no_condition(\n    11→    as_forecast_sample(test,\n    12→      observed = \"obs\", predicted = \"pred\",\n    13→      forecast_unit = c(\n    14→        \"location\", \"model\", \"target_type\",\n    15→        \"target_end_date\", \"horizon\"\n    16→      ),\n    17→      sample_id = \"sample\"\n    18→    )\n    19→  )\n    20→})\n    21→\n    22→test_that(\"Running `as_forecast_sample()` twice returns the same object\", {\n    23→  ex <- na.omit(example_sample_continuous)\n    24→\n    25→  expect_identical(\n    26→    as_forecast_sample(as_forecast_sample(ex)),\n    27→    as_forecast_sample(ex)\n    28→  )\n    29→})\n    30→\n    31→\n    32→# ==============================================================================\n    33→# is_forecast_sample() # nolint: commented_code_linter\n    34→# ==============================================================================\n    35→test_that(\"is_forecast_sample() works as expected\", {\n    36→  expect_true(is_forecast_sample(example_sample_continuous))\n    37→  expect_false(is_forecast_sample(example_binary))\n    38→  expect_false(is_forecast_sample(example_point))\n    39→  expect_false(is_forecast_sample(example_quantile))\n    40→  expect_false(is_forecast_sample(example_nominal))\n    41→  expect_false(is_forecast_sample(1:10))\n    42→})\n    43→\n    44→\n    45→# ==============================================================================\n    46→# get_metrics.forecast_sample() # nolint: commented_code_linter\n    47→# ==============================================================================\n    48→\n    49→test_that(\"get_metrics.forecast_sample() works as expected\", {\n    50→  expect_type(\n    51→    get_metrics(example_sample_continuous), \"list\"\n    52→  )\n    53→  expect_type(\n    54→    get_metrics(example_sample_discrete), \"list\"\n    55→  )\n    56→})\n    57→\n    58→\n    59→# ==============================================================================\n    60→# get_pit_histogram.forecast_sample() # nolint: commented_code_linter\n    61→# ==============================================================================\n    62→test_that(\"get_pit_histogram.forecast_sample() works as expected\", {\n    63→  pit_continuous <- get_pit_histogram(example_sample_continuous, by = c(\"model\", \"target_type\"))\n    64→  pit_integer <- get_pit_histogram(example_sample_discrete, by = c(\"model\", \"location\"))\n    65→\n    66→  expect_named(pit_continuous, c(\"model\", \"target_type\", \"density\", \"bin\", \"mid\"))\n    67→  expect_named(pit_integer, c(\"model\", \"location\", \"density\", \"bin\", \"mid\"))\n    68→\n    69→  # check printing works\n    70→  expect_output(print(pit_continuous))\n    71→  expect_output(print(pit_integer))\n    72→\n    73→  # check class is correct\n    74→  expect_s3_class(pit_continuous, c(\"data.table\", \"data.frame\"), exact = TRUE)\n    75→  expect_s3_class(pit_integer, c(\"data.table\", \"data.frame\"), exact = TRUE)\n    76→})\n    77→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"a7591f3a-ab67-4cbf-93b8-981b19867e2e"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JkFRC7LbhXsxWmixpLn6ib","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GNDn8KCLUuWnyyCY73U2xz","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-nominal.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":29758,"cache_read_input_tokens":12285,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":29758},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"6ce5d076-ea2c-41e3-8065-defc5bd0fab3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GNDn8KCLUuWnyyCY73U2xz","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_nominal() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→test_that(\"as_forecast.forecast_nominal() works as expected\", {\n     5→  ex <- data.table::copy(example_nominal) |>\n     6→    na.omit()\n     7→\n     8→\n     9→  expect_s3_class(\n    10→    as_forecast_nominal(ex),\n    11→    c(\"forecast_nominal\", \"forecast\", \"data.table\", \"data.frame\"),\n    12→    exact = TRUE\n    13→  )\n    14→\n    15→  setnames(ex, old = \"predicted_label\", new = \"label\")\n    16→  expect_no_condition(\n    17→    as_forecast_nominal(ex, predicted_label = \"label\")\n    18→  )\n    19→})\n    20→\n    21→test_that(\"as_forecast.forecast_nominal() breaks when rows with zero probability are missing\", {\n    22→  ex_faulty <- as.data.table(example_nominal)\n    23→  ex_faulty <- ex_faulty[predicted != 0]\n    24→  expect_warning(\n    25→    expect_error(\n    26→      as_forecast_nominal(ex_faulty),\n    27→      \"Found incomplete forecasts\"\n    28→    ),\n    29→    \"Some forecasts have different numbers of rows\"\n    30→  )\n    31→})\n    32→\n    33→\n    34→# ==============================================================================\n    35→# is_forecast_nominal() # nolint: commented_code_linter\n    36→# ==============================================================================\n    37→test_that(\"is_forecast_nominal() works as expected\", {\n    38→  expect_true(is_forecast_nominal(example_nominal))\n    39→  expect_false(is_forecast_nominal(example_binary))\n    40→  expect_false(is_forecast_nominal(example_point))\n    41→  expect_false(is_forecast_nominal(example_quantile))\n    42→  expect_false(is_forecast_nominal(example_sample_continuous))\n    43→  expect_false(is_forecast_nominal(1:10))\n    44→})\n    45→\n    46→\n    47→# ==============================================================================\n    48→# get_metrics.forecast_nominal() # nolint: commented_code_linter\n    49→# ==============================================================================\n    50→\n    51→test_that(\"get_metrics.forecast_nominal() works as expected\", {\n    52→  expect_type(\n    53→    get_metrics(example_nominal), \"list\"\n    54→  )\n    55→})\n    56→\n    57→\n    58→# ==============================================================================\n    59→# Printing # nolint: commented_code_linter\n    60→# ==============================================================================\n    61→test_that(\"Printing works as expected\", {\n    62→  suppressMessages(\n    63→    expect_message(\n    64→      expect_message(\n    65→        capture.output(print(example_nominal)),\n    66→        \"Forecast type: nominal\"\n    67→      ),\n    68→      \"Forecast unit:\"\n    69→    )\n    70→  )\n    71→})\n    72→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"fb2caffc-bbd5-4f48-b458-a9aa01368116"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JkFRC7LbhXsxWmixpLn6ib","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016HWegCJ2rnqyS2AT7NscEp","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-ordinal.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":29758,"cache_read_input_tokens":12285,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":29758},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"949b3a9d-f891-42be-aebb-42ef3a4e5720"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016HWegCJ2rnqyS2AT7NscEp","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_nominal() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→test_that(\"as_forecast.forecast_ordinal() works as expected\", {\n     5→  ex <- data.table::copy(example_ordinal) |>\n     6→    na.omit()\n     7→\n     8→\n     9→  expect_s3_class(\n    10→    as_forecast_ordinal(ex),\n    11→    c(\"forecast_ordinal\", \"forecast\", \"data.table\", \"data.frame\"),\n    12→    exact = TRUE\n    13→  )\n    14→\n    15→  setnames(ex, old = \"predicted_label\", new = \"label\")\n    16→  expect_no_condition(\n    17→    as_forecast_ordinal(ex, predicted_label = \"label\")\n    18→  )\n    19→})\n    20→\n    21→test_that(\"as_forecast.forecast_ordinal() breaks when rows with zero probability are missing\", {\n    22→  ex_faulty <- as.data.table(example_ordinal)\n    23→  ex_faulty <- ex_faulty[predicted != 0]\n    24→  expect_warning(\n    25→    expect_error(\n    26→      as_forecast_ordinal(ex_faulty),\n    27→      \"Found incomplete forecasts\"\n    28→    ),\n    29→    \"Some forecasts have different numbers of rows\"\n    30→  )\n    31→})\n    32→\n    33→test_that(\"assert_forecast.forecast_ordinal() fails if factors are not ordered\", {\n    34→  ex_faulty <- na.omit(data.table::copy(example_nominal))\n    35→  expect_error(\n    36→    as_forecast_ordinal(ex_faulty),\n    37→    \"Assertion on 'forecast\\\\$observed' failed: Must be an ordered factor, but is unordered.\"\n    38→  )\n    39→})\n    40→\n    41→# ==============================================================================\n    42→# is_forecast_nominal() # nolint: commented_code_linter\n    43→# ==============================================================================\n    44→test_that(\"is_forecast_nominal() works as expected\", {\n    45→  expect_true(is_forecast_ordinal(example_ordinal))\n    46→  expect_false(is_forecast_ordinal(example_binary))\n    47→  expect_false(is_forecast_ordinal(example_point))\n    48→  expect_false(is_forecast_ordinal(example_quantile))\n    49→  expect_false(is_forecast_ordinal(example_sample_continuous))\n    50→  expect_false(is_forecast_ordinal(1:10))\n    51→})\n    52→\n    53→\n    54→# ==============================================================================\n    55→# get_metrics.forecast_nominal() # nolint: commented_code_linter\n    56→# ==============================================================================\n    57→\n    58→test_that(\"get_metrics.forecast_nominal() works as expected\", {\n    59→  expect_type(\n    60→    get_metrics(example_ordinal), \"list\"\n    61→  )\n    62→})\n    63→\n    64→\n    65→# ==============================================================================\n    66→# Printing # nolint: commented_code_linter\n    67→# ==============================================================================\n    68→test_that(\"Printing works as expected\", {\n    69→  suppressMessages(\n    70→    expect_message(\n    71→      expect_message(\n    72→        capture.output(print(example_ordinal)),\n    73→        \"Forecast type: ordinal\"\n    74→      ),\n    75→      \"Forecast unit:\"\n    76→    )\n    77→  )\n    78→})\n    79→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"97d551e7-4ea6-4759-9ea1-40371e2ee016"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JkFRC7LbhXsxWmixpLn6ib","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XYUfGmUxXP49arrb9gdoMC","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-multivariate-sample.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":29758,"cache_read_input_tokens":12285,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":29758},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"e2c273b5-7ebe-4289-adb4-b9e9fc64fab2"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XYUfGmUxXP49arrb9gdoMC","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_multivariate_sample() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→test_that(\"as_forecast_multivariate_sample() works as expected\", {\n     5→  test <- na.omit(data.table::copy(example_sample_continuous))\n     6→  data.table::setnames(test,\n     7→    old = c(\"observed\", \"predicted\", \"sample_id\"),\n     8→    new = c(\"obs\", \"pred\", \"sample\")\n     9→  )\n    10→  expect_no_condition(\n    11→    as_forecast_multivariate_sample(test,\n    12→      observed = \"obs\", predicted = \"pred\",\n    13→      forecast_unit = c(\n    14→        \"location\", \"model\", \"target_type\",\n    15→        \"target_end_date\", \"horizon\"\n    16→      ),\n    17→      joint_across = \"location\",\n    18→      sample_id = \"sample\"\n    19→    )\n    20→  )\n    21→})\n    22→\n    23→test_that(\"as_forecast_multivariate_sample() creates expected structure\", {\n    24→  test <- na.omit(data.table::copy(example_sample_continuous))\n    25→  data.table::setnames(test,\n    26→    old = c(\"observed\", \"predicted\", \"sample_id\"),\n    27→    new = c(\"obs\", \"pred\", \"sample\")\n    28→  )\n    29→\n    30→  result <- as_forecast_multivariate_sample(test,\n    31→    observed = \"obs\", predicted = \"pred\",\n    32→    forecast_unit = c(\n    33→      \"location\", \"model\", \"target_type\",\n    34→      \"target_end_date\", \"horizon\"\n    35→    ),\n    36→    joint_across = \"location\",\n    37→    sample_id = \"sample\"\n    38→  )\n    39→\n    40→  # Snapshot the print output to capture the structure\n    41→  expect_snapshot(print(result))\n    42→\n    43→  # Snapshot the class and structure\n    44→  expect_snapshot({\n    45→    cat(\"Class:\", class(result), \"\\n\")\n    46→    cat(\"Forecast type:\", scoringutils:::get_forecast_type(result), \"\\n\") # nolint: undesirable_operator_linter\n    47→    cat(\"Forecast unit:\", toString(get_forecast_unit(result)), \"\\n\")\n    48→    cat(\"Number of rows:\", nrow(result), \"\\n\")\n    49→    cat(\"Number of columns:\", ncol(result), \"\\n\")\n    50→    cat(\"Column names:\", toString(names(result)), \"\\n\")\n    51→    cat(\"Number of unique groups:\", length(unique(result$.mv_group_id)), \"\\n\")\n    52→  })\n    53→})\n    54→\n    55→\n    56→# ==============================================================================\n    57→# is_forecast_sample() # nolint: commented_code_linter\n    58→# ==============================================================================\n    59→test_that(\"is_forecast_sample() works as expected\", {\n    60→  expect_true(is_forecast_sample_multivariate(example_multivariate_sample))\n    61→  expect_false(is_forecast_sample_multivariate(example_binary))\n    62→  expect_false(is_forecast_sample(example_multivariate_sample))\n    63→})\n    64→\n    65→\n    66→# ==============================================================================\n    67→# get_metrics.forecast_sample() # nolint: commented_code_linter\n    68→# ==============================================================================\n    69→\n    70→test_that(\"get_metrics.forecast_sample_multivariate() works as expected\", {\n    71→  expect_type(\n    72→    get_metrics(example_multivariate_sample), \"list\"\n    73→  )\n    74→})\n    75→\n    76→\n    77→# ==============================================================================\n    78→# set_grouping() and get_grouping()\n    79→# ==============================================================================\n    80→test_that(\"set_grouping() works as expected\", {\n    81→  data <- example_multivariate_sample\n    82→  grouping <- c(\"model\", \"target_type\", \"target_end_date\", \"horizon\")\n    83→\n    84→  # Test basic functionality\n    85→  result <- set_grouping(data, grouping)\n    86→  expect_true(\".mv_group_id\" %in% names(result))\n    87→  expect_type(result$.mv_group_id, \"integer\")\n    88→\n    89→  # Test that groups are consistent\n    90→  group_counts <- as.data.table(result)[, .N, by = .mv_group_id]\n    91→  expect_true(all(group_counts$N > 0))\n    92→})\n    93→\n    94→test_that(\"get_grouping() works as expected\", {\n    95→  data <- example_multivariate_sample\n    96→  grouping <- c(\"model\", \"target_type\", \"target_end_date\", \"horizon\")\n    97→  joint_across <- setdiff(get_forecast_unit(data), grouping)\n    98→  data <- scoringutils:::set_grouping(data, joint_across) # nolint: undesirable_operator_linter\n    99→\n   100→  # Test that get_grouping returns the correct columns\n   101→  result <- get_grouping(data)\n   102→  expect_type(result, \"character\")\n   103→  expect_true(all(grouping %in% result))\n   104→})\n   105→\n   106→test_that(\"get_grouping() falls back to forecast_unit when mv_group_id is missing\", {\n   107→  data <- example_multivariate_sample\n   108→\n   109→  # Remove the mv_group_id column\n   110→  data_bad <- as.data.table(data)\n   111→  data_bad[, .mv_group_id := NULL]\n   112→\n   113→  expect_error(\n   114→    get_grouping(data_bad),\n   115→    \"No column `.mv_group_id` found in the forecast object.\"\n   116→  )\n   117→})\n   118→\n   119→test_that(\"set_grouping() preserves existing keys correctly\", {\n   120→  data <- example_multivariate_sample\n   121→  grouping <- c(\"model\", \"target_type\", \"target_end_date\", \"horizon\")\n   122→\n   123→  # Test case 1: No existing keys\n   124→  data_no_keys <- copy(data)\n   125→  nokeys <- NULL\n   126→  setkeyv(data_no_keys, nokeys) # Ensure no keys\n   127→  expect_null(key(data_no_keys))\n   128→\n   129→  result_no_keys <- set_grouping(data_no_keys, grouping)\n   130→  expect_null(key(result_no_keys)) # Should still have no keys\n   131→\n   132→  # Test case 2: With existing keys\n   133→  data_with_keys <- copy(data)\n   134→  keys <- c(\"location\", \"model\")\n   135→  setkeyv(data_with_keys, keys) # Set some keys\n   136→  original_keys <- key(data_with_keys)\n   137→  expect_identical(original_keys, c(\"location\", \"model\"))\n   138→\n   139→  result_with_keys <- scoringutils:::set_grouping(data_with_keys, grouping) # nolint: undesirable_operator_linter\n   140→  expect_identical(key(result_with_keys), original_keys) # Should preserve original keys\n   141→\n   142→  # Test case 3: Verify functionality still works with keys preserved\n   143→  expect_true(\".mv_group_id\" %in% names(result_with_keys))\n   144→  expect_type(result_with_keys$.mv_group_id, \"integer\")\n   145→\n   146→  # Test that groups are consistent\n   147→  group_counts <- as.data.table(result_with_keys)[, .N, by = .mv_group_id]\n   148→  expect_true(all(group_counts$N > 0))\n   149→})\n   150→\n   151→# ==============================================================================\n   152→# score.forecast_sample_multivariate() # nolint: commented_code_linter\n   153→# ==============================================================================\n   154→test_that(\"score.forecast_sample_multivariate() works as expected\", {\n   155→  data <- example_multivariate_sample\n   156→\n   157→  # Test basic scoring\n   158→  scores <- score(data)\n   159→  expect_true(is.data.table(scores))\n   160→  expect_true(\"energy_score\" %in% names(scores))\n   161→\n   162→  # Test with specific metrics\n   163→  scores <- score(data, metrics = list(energy_score = energy_score_multivariate))\n   164→  expect_true(is.data.table(scores))\n   165→  expect_true(\"energy_score\" %in% names(scores))\n   166→\n   167→  # Test that scores are numeric\n   168→  expect_type(scores$energy_score, \"double\")\n   169→})\n   170→\n   171→test_that(\"score.forecast_sample_multivariate() creates expected output structure\", {\n   172→  data <- example_multivariate_sample\n   173→\n   174→  # Test basic scoring and capture snapshot\n   175→  scores <- score(data)\n   176→\n   177→  # Snapshot the print output to capture the structure\n   178→  expect_snapshot(print(scores))\n   179→\n   180→  # Snapshot the detailed structure\n   181→  expect_snapshot({\n   182→    cat(\"Class:\", class(scores), \"\\n\")\n   183→    cat(\"Number of rows:\", nrow(scores), \"\\n\")\n   184→    cat(\"Number of columns:\", ncol(scores), \"\\n\")\n   185→    cat(\"Column names:\", toString(names(scores)), \"\\n\")\n   186→    cat(\"Energy score range:\", paste(range(scores$energy_score, na.rm = TRUE), collapse = \" to \"), \"\\n\")\n   187→    cat(\"Number of non-NA energy scores:\", sum(!is.na(scores$energy_score)), \"\\n\")\n   188→    cat(\"Sample of energy scores:\", toString(head(scores$energy_score, 5)), \"\\n\")\n   189→  })\n   190→\n   191→  # Test with specific metrics and capture snapshot\n   192→  scores_specific <- score(data, metrics = list(energy_score = energy_score_multivariate))\n   193→\n   194→  expect_snapshot({\n   195→    cat(\"=== Specific Metrics Test ===\\n\")\n   196→    cat(\"Class:\", class(scores_specific), \"\\n\")\n   197→    cat(\"Number of rows:\", nrow(scores_specific), \"\\n\")\n   198→    cat(\"Number of columns:\", ncol(scores_specific), \"\\n\")\n   199→    cat(\"Column names:\", toString(names(scores_specific)), \"\\n\")\n   200→    cat(\"Energy score range:\", paste(range(scores_specific$energy_score, na.rm = TRUE), collapse = \" to \"), \"\\n\")\n   201→  })\n   202→})\n   203→\n   204→# ==============================================================================\n   205→# Error cases for as_forecast_multivariate_sample()\n   206→# ==============================================================================\n   207→test_that(\"as_forecast_multivariate_sample() handles errors appropriately\", {\n   208→  data <- as.data.table(example_multivariate_sample)\n   209→\n   210→  # Test with missing required columns\n   211→  data_bad <- data[, !\"sample_id\"]\n   212→  expect_error(\n   213→    as_forecast_multivariate_sample(\n   214→      data_bad, joint_across = c(\"location\", \"location_name\")\n   215→    ),\n   216→    \"Assertion on 'forecast' failed: Column 'sample_id' not found in data.\"\n   217→  )\n   218→\n   219→  # Test with inconsistent sample lengths within groups\n   220→  data_bad <- copy(data)\n   221→  data_bad <- rbind(data_bad[1000:1010], data_bad)\n   222→  expect_error(\n   223→    as_forecast_multivariate_sample(\n   224→      data_bad, joint_across = c(\"location\", \"location_name\")\n   225→    ),\n   226→    \"All univariate forecasts.*must have the same number of samples\"\n   227→  )\n   228→\n   229→  # Test with inconsistent sample lengths after forecast object creation\n   230→  expect_warning(\n   231→    example_multivariate_sample[-(1000:1010), ],\n   232→    \"Found the following group with an inconsistent sample length\"\n   233→  )\n   234→\n   235→  # Test with invalid grouping columns\n   236→  expect_error(\n   237→    as_forecast_multivariate_sample(data,\n   238→      joint_across = \"nonexistent_column\"\n   239→    ),\n   240→    \"Must be a subset of\"\n   241→  )\n   242→})\n   243→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"b49e691a-143b-4ed8-be52-ddf5b3eb2b3e"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PaRheEhKwkyBvPWFBTcRES","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01RF3U8tp5Wmg7cTRUMNDisJ","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/NAMESPACE"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":15602,"cache_read_input_tokens":42043,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":15602},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"cee2eb26-c02e-494c-99ef-7b61a9d7cf8b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RF3U8tp5Wmg7cTRUMNDisJ","type":"tool_result","content":"     1→# Generated by roxygen2: do not edit by hand\n     2→\n     3→S3method(\"$<-\",forecast)\n     4→S3method(\"[\",forecast)\n     5→S3method(\"[<-\",forecast)\n     6→S3method(\"[[<-\",forecast)\n     7→S3method(`[`,scores)\n     8→S3method(as_forecast_binary,default)\n     9→S3method(as_forecast_multivariate_sample,default)\n    10→S3method(as_forecast_nominal,default)\n    11→S3method(as_forecast_ordinal,default)\n    12→S3method(as_forecast_point,default)\n    13→S3method(as_forecast_point,forecast_quantile)\n    14→S3method(as_forecast_quantile,default)\n    15→S3method(as_forecast_quantile,forecast_sample)\n    16→S3method(as_forecast_sample,default)\n    17→S3method(assert_forecast,default)\n    18→S3method(assert_forecast,forecast_binary)\n    19→S3method(assert_forecast,forecast_nominal)\n    20→S3method(assert_forecast,forecast_ordinal)\n    21→S3method(assert_forecast,forecast_point)\n    22→S3method(assert_forecast,forecast_quantile)\n    23→S3method(assert_forecast,forecast_sample)\n    24→S3method(assert_forecast,forecast_sample_multivariate)\n    25→S3method(get_metrics,forecast_binary)\n    26→S3method(get_metrics,forecast_nominal)\n    27→S3method(get_metrics,forecast_ordinal)\n    28→S3method(get_metrics,forecast_point)\n    29→S3method(get_metrics,forecast_quantile)\n    30→S3method(get_metrics,forecast_sample)\n    31→S3method(get_metrics,forecast_sample_multivariate)\n    32→S3method(get_metrics,scores)\n    33→S3method(get_pit_histogram,default)\n    34→S3method(get_pit_histogram,forecast_quantile)\n    35→S3method(get_pit_histogram,forecast_sample)\n    36→S3method(head,forecast)\n    37→S3method(print,forecast)\n    38→S3method(score,default)\n    39→S3method(score,forecast_binary)\n    40→S3method(score,forecast_nominal)\n    41→S3method(score,forecast_ordinal)\n    42→S3method(score,forecast_point)\n    43→S3method(score,forecast_quantile)\n    44→S3method(score,forecast_sample)\n    45→S3method(score,forecast_sample_multivariate)\n    46→S3method(tail,forecast)\n    47→export(add_relative_skill)\n    48→export(ae_median_quantile)\n    49→export(ae_median_sample)\n    50→export(as_forecast_binary)\n    51→export(as_forecast_multivariate_sample)\n    52→export(as_forecast_nominal)\n    53→export(as_forecast_ordinal)\n    54→export(as_forecast_point)\n    55→export(as_forecast_quantile)\n    56→export(as_forecast_sample)\n    57→export(assert_forecast)\n    58→export(bias_quantile)\n    59→export(bias_sample)\n    60→export(brier_score)\n    61→export(crps_sample)\n    62→export(dispersion_quantile)\n    63→export(dispersion_sample)\n    64→export(dss_sample)\n    65→export(energy_score_multivariate)\n    66→export(get_correlations)\n    67→export(get_coverage)\n    68→export(get_duplicate_forecasts)\n    69→export(get_forecast_counts)\n    70→export(get_forecast_unit)\n    71→export(get_grouping)\n    72→export(get_metrics)\n    73→export(get_pairwise_comparisons)\n    74→export(get_pit_histogram)\n    75→export(interval_coverage)\n    76→export(is_forecast)\n    77→export(is_forecast_binary)\n    78→export(is_forecast_nominal)\n    79→export(is_forecast_ordinal)\n    80→export(is_forecast_point)\n    81→export(is_forecast_quantile)\n    82→export(is_forecast_sample)\n    83→export(is_forecast_sample_multivariate)\n    84→export(log_shift)\n    85→export(logs_binary)\n    86→export(logs_categorical)\n    87→export(logs_sample)\n    88→export(mad_sample)\n    89→export(new_forecast)\n    90→export(overprediction_quantile)\n    91→export(overprediction_sample)\n    92→export(pit_histogram_sample)\n    93→export(plot_correlations)\n    94→export(plot_forecast_counts)\n    95→export(plot_heatmap)\n    96→export(plot_interval_coverage)\n    97→export(plot_pairwise_comparisons)\n    98→export(plot_quantile_coverage)\n    99→export(plot_wis)\n   100→export(quantile_score)\n   101→export(rps_ordinal)\n   102→export(score)\n   103→export(se_mean_sample)\n   104→export(select_metrics)\n   105→export(summarise_scores)\n   106→export(summarize_scores)\n   107→export(theme_scoringutils)\n   108→export(transform_forecasts)\n   109→export(underprediction_quantile)\n   110→export(underprediction_sample)\n   111→export(wis)\n   112→importFrom(Metrics,ae)\n   113→importFrom(Metrics,ape)\n   114→importFrom(Metrics,se)\n   115→importFrom(checkmate,assert)\n   116→importFrom(checkmate,assert_character)\n   117→importFrom(checkmate,assert_class)\n   118→importFrom(checkmate,assert_data_frame)\n   119→importFrom(checkmate,assert_data_table)\n   120→importFrom(checkmate,assert_disjunct)\n   121→importFrom(checkmate,assert_factor)\n   122→importFrom(checkmate,assert_function)\n   123→importFrom(checkmate,assert_int)\n   124→importFrom(checkmate,assert_list)\n   125→importFrom(checkmate,assert_logical)\n   126→importFrom(checkmate,assert_matrix)\n   127→importFrom(checkmate,assert_names)\n   128→importFrom(checkmate,assert_number)\n   129→importFrom(checkmate,assert_numeric)\n   130→importFrom(checkmate,assert_set_equal)\n   131→importFrom(checkmate,assert_subset)\n   132→importFrom(checkmate,assert_vector)\n   133→importFrom(checkmate,check_atomic_vector)\n   134→importFrom(checkmate,check_function)\n   135→importFrom(checkmate,check_matrix)\n   136→importFrom(checkmate,check_numeric)\n   137→importFrom(checkmate,check_vector)\n   138→importFrom(checkmate,test_atomic_vector)\n   139→importFrom(checkmate,test_list)\n   140→importFrom(checkmate,test_names)\n   141→importFrom(checkmate,test_set_equal)\n   142→importFrom(checkmate,test_subset)\n   143→importFrom(cli,cli_abort)\n   144→importFrom(cli,cli_inform)\n   145→importFrom(cli,cli_text)\n   146→importFrom(cli,cli_warn)\n   147→importFrom(cli,col_blue)\n   148→importFrom(cli,qty)\n   149→importFrom(data.table,\"%like%\")\n   150→importFrom(data.table,':=')\n   151→importFrom(data.table,.I)\n   152→importFrom(data.table,.N)\n   153→importFrom(data.table,`:=`)\n   154→importFrom(data.table,as.data.table)\n   155→importFrom(data.table,copy)\n   156→importFrom(data.table,data.table)\n   157→importFrom(data.table,dcast)\n   158→importFrom(data.table,fcase)\n   159→importFrom(data.table,is.data.table)\n   160→importFrom(data.table,key)\n   161→importFrom(data.table,melt)\n   162→importFrom(data.table,nafill)\n   163→importFrom(data.table,rbindlist)\n   164→importFrom(data.table,setDT)\n   165→importFrom(data.table,setattr)\n   166→importFrom(data.table,setcolorder)\n   167→importFrom(data.table,setkeyv)\n   168→importFrom(data.table,setnames)\n   169→importFrom(data.table,setorderv)\n   170→importFrom(ggplot2,.data)\n   171→importFrom(ggplot2,`%+replace%`)\n   172→importFrom(ggplot2,aes)\n   173→importFrom(ggplot2,coord_cartesian)\n   174→importFrom(ggplot2,coord_flip)\n   175→importFrom(ggplot2,element_blank)\n   176→importFrom(ggplot2,element_line)\n   177→importFrom(ggplot2,element_text)\n   178→importFrom(ggplot2,facet_grid)\n   179→importFrom(ggplot2,facet_wrap)\n   180→importFrom(ggplot2,geom_col)\n   181→importFrom(ggplot2,geom_line)\n   182→importFrom(ggplot2,geom_linerange)\n   183→importFrom(ggplot2,geom_polygon)\n   184→importFrom(ggplot2,geom_text)\n   185→importFrom(ggplot2,geom_tile)\n   186→importFrom(ggplot2,ggplot)\n   187→importFrom(ggplot2,guide_legend)\n   188→importFrom(ggplot2,guides)\n   189→importFrom(ggplot2,labs)\n   190→importFrom(ggplot2,scale_colour_manual)\n   191→importFrom(ggplot2,scale_fill_discrete)\n   192→importFrom(ggplot2,scale_fill_gradient)\n   193→importFrom(ggplot2,scale_fill_gradient2)\n   194→importFrom(ggplot2,scale_fill_manual)\n   195→importFrom(ggplot2,scale_y_continuous)\n   196→importFrom(ggplot2,theme)\n   197→importFrom(ggplot2,theme_light)\n   198→importFrom(ggplot2,theme_minimal)\n   199→importFrom(ggplot2,unit)\n   200→importFrom(ggplot2,xlab)\n   201→importFrom(ggplot2,ylab)\n   202→importFrom(methods,formalArgs)\n   203→importFrom(methods,hasArg)\n   204→importFrom(purrr,partial)\n   205→importFrom(scoringRules,crps_sample)\n   206→importFrom(scoringRules,dss_sample)\n   207→importFrom(scoringRules,es_sample)\n   208→importFrom(scoringRules,logs_sample)\n   209→importFrom(scoringRules,rps_probs)\n   210→importFrom(stats,cor)\n   211→importFrom(stats,mad)\n   212→importFrom(stats,median)\n   213→importFrom(stats,na.omit)\n   214→importFrom(stats,p.adjust)\n   215→importFrom(stats,quantile)\n   216→importFrom(stats,rbinom)\n   217→importFrom(stats,reorder)\n   218→importFrom(stats,runif)\n   219→importFrom(stats,sd)\n   220→importFrom(stats,weighted.mean)\n   221→importFrom(stats,wilcox.test)\n   222→importFrom(utils,combn)\n   223→importFrom(utils,head)\n   224→importFrom(utils,tail)\n   225→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"22793c43-e95c-4379-a942-9b6f83b09068"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PaRheEhKwkyBvPWFBTcRES","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015UHGDTsyBdUKq5zBFmpCqY","name":"Grep","input":{"pattern":"as_forecast_generic|ensure_data.table","path":"/Users/nikos/github-synced/scoringutils-pipeline/R","output_mode":"files_with_matches"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":15602,"cache_read_input_tokens":42043,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":15602},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"6a97a97b-b885-42e4-b921-bfa17a3237c4"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015UHGDTsyBdUKq5zBFmpCqY","type":"tool_result","content":"Found 19 files\nR/summarise_scores.R\nR/plot-heatmap.R\nR/plot-wis.R\nR/get-coverage.R\nR/get-forecast-counts.R\nR/pairwise-comparisons.R\nR/forecast-unit.R\nR/get-correlations.R\nR/class-forecast-point.R\nR/helper-quantile-interval-range.R\nR/get-duplicate-forecasts.R\nR/class-forecast.R\nR/class-forecast-quantile.R\nR/class-forecast-sample.R\nR/class-forecast-multivariate-sample.R\nR/class-forecast-nominal.R\nR/class-forecast-ordinal.R\nR/class-forecast-binary.R\nR/check-input-helpers.R"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"e96f5799-c4df-46bd-a857-7bf8c56d9957"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_018jHADqRJqjbHbJYJWonA1s","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01LnPjT52mf9pYQirkHucEJA","name":"Grep","input":{"pattern":"^ensure_data\\.table\\s*<-|^ensure_data\\.table\\s*=","path":"/Users/nikos/github-synced/scoringutils-pipeline/R","output_mode":"files_with_matches"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":3716,"cache_read_input_tokens":57645,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3716},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"ac09a38b-bbb2-4945-9c2d-c3597082304e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01LnPjT52mf9pYQirkHucEJA","type":"tool_result","content":"Found 1 file\nR/check-input-helpers.R"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8570b838-2220-4889-8dac-69211360b115"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_018jHADqRJqjbHbJYJWonA1s","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TXXxZBGWkeSaWsLSWc9JoU","name":"Bash","input":{"command":"grep -r \"ensure_data.table\" /Users/nikos/github-synced/scoringutils-pipeline/R --include=\"*.R\" | grep -E \"(function|<-|=)\" | head -5"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":3716,"cache_read_input_tokens":57645,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3716},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"d53ba5c6-66be-4dfb-b89f-d40860d29270"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TXXxZBGWkeSaWsLSWc9JoU","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/R/plot-heatmap.R:  scores <- ensure_data.table(scores)\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-duplicate-forecasts.R:  data <- ensure_data.table(data)\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-correlations.R:  scores <- ensure_data.table(scores)\n/Users/nikos/github-synced/scoringutils-pipeline/R/summarise_scores.R:  scores <- ensure_data.table(scores)\n/Users/nikos/github-synced/scoringutils-pipeline/R/forecast-unit.R:  data <- ensure_data.table(data)","is_error":false}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"af714f30-be78-4f79-826b-9c1ba836201c"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_015CEyXE8XcEEBCytTrSv8d4","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019N24nSTQZHuJPrUs6hRgCa","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/check-input-helpers.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":531,"cache_read_input_tokens":61361,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":531},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"07967ae6-29b1-470e-ad4f-d9770c992223"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019N24nSTQZHuJPrUs6hRgCa","type":"tool_result","content":"     1→#' Ensure that an object is a `data.table`\n     2→#' @description\n     3→#' This function ensures that an object is a `data table`.\n     4→#' If the object is not a data table, it is converted to one. If the object\n     5→#' is a data table, a copy of the object is returned.\n     6→#' @param data An object to ensure is a data table.\n     7→#' @returns A data.table/a copy of an existing data.table.\n     8→#' @keywords internal\n     9→#' @importFrom data.table copy is.data.table as.data.table\n    10→ensure_data.table <- function(data) {\n    11→  if (is.data.table(data)) {\n    12→    data <- copy(data)\n    13→  } else {\n    14→    data <- as.data.table(data)\n    15→  }\n    16→  return(data)\n    17→}\n    18→\n    19→\n    20→#' @title Check whether an input is an atomic vector of mode 'numeric'\n    21→#'\n    22→#' @description Helper function to check whether an input is a numeric vector.\n    23→#' @param x input to check\n    24→#' @inheritDotParams checkmate::check_numeric\n    25→#' @importFrom checkmate check_atomic_vector check_numeric\n    26→#' @inherit document_check_functions return\n    27→#' @keywords internal_input_check\n    28→check_numeric_vector <- function(x, ...) {\n    29→  # check functions must return TRUE on success\n    30→  # and a custom error message otherwise\n    31→  numeric <- check_numeric(x, ...)\n    32→  vector <- check_atomic_vector(x)\n    33→  if (!isTRUE(numeric)) {\n    34→    return(numeric)\n    35→  } else if (!isTRUE(vector)) {\n    36→    return(vector)\n    37→  }\n    38→  return(TRUE)\n    39→}\n    40→\n    41→\n    42→# ==============================================================================\n    43→# functinos below will be deleted in the future\n    44→\n    45→#' @title Helper function to convert assert statements into checks\n    46→#'\n    47→#' @description\n    48→#' Tries to execute an expression. Internally, this is used to\n    49→#' see whether assertions fail when checking inputs (i.e. to convert an\n    50→#' `assert_*()` statement into a check). If the expression fails, the error\n    51→#' message is returned. If the expression succeeds, `TRUE` is returned.\n    52→#' @param expr an expression to be evaluated\n    53→#' @importFrom checkmate assert assert_numeric check_matrix\n    54→#' @inherit document_check_functions return\n    55→#' @keywords internal_input_check\n    56→check_try <- function(expr) {\n    57→  result <- try(expr, silent = TRUE)\n    58→  if (is.null(result)) {\n    59→    return(TRUE)\n    60→  }\n    61→  msg <- conditionMessage(attr(result, \"condition\"))\n    62→  return(msg)\n    63→}\n    64→\n    65→\n    66→#' Check column names are present in a data.frame\n    67→#' @description\n    68→#' The functions loops over the column names and checks whether they are\n    69→#' present. If an issue is encountered, the function immediately stops\n    70→#' and returns a message with the first issue encountered.\n    71→#' @inherit document_check_functions params return\n    72→#' @importFrom checkmate assert_character\n    73→#' @keywords internal_input_check\n    74→check_columns_present <- function(data, columns) {\n    75→  if (is.null(columns)) {\n    76→    return(TRUE)\n    77→  }\n    78→  assert_character(columns, min.len = 1)\n    79→  colnames <- colnames(data)\n    80→  missing <- list()\n    81→  for (x in columns) {\n    82→    if (!(x %in% colnames)) {\n    83→      missing[[x]] <- x\n    84→    }\n    85→  }\n    86→  missing <- unlist(missing)\n    87→  if (length(missing) > 1) {\n    88→    msg <- paste0(\n    89→      \"Columns '\", paste(missing, collapse = \"', '\"), \"' not found in data\"\n    90→    )\n    91→    return(msg)\n    92→  } else if (length(missing) == 1) {\n    93→    msg <- paste0(\"Column '\", missing, \"' not found in data\")\n    94→    return(msg)\n    95→  }\n    96→  return(TRUE)\n    97→}\n    98→\n    99→#' Test whether all column names are present in a data.frame\n   100→#' @description The function checks whether all column names are present. If\n   101→#' one or more columns are missing, the function returns FALSE. If all columns\n   102→#' are present, the function returns TRUE.\n   103→#' @inheritParams document_check_functions\n   104→#' @returns Returns TRUE if all columns are present and FALSE otherwise\n   105→#' @keywords internal_input_check\n   106→test_columns_present <- function(data, columns) {\n   107→  check <- check_columns_present(data, columns)\n   108→  return(isTRUE(check))\n   109→}\n   110→\n   111→#' Test whether column names are NOT present in a data.frame\n   112→#' @description The function checks whether all column names are NOT present.\n   113→#' If none of the columns are present, the function returns TRUE. If one or\n   114→#' more columns are present, the function returns FALSE.\n   115→#' @inheritParams document_check_functions\n   116→#' @returns Returns TRUE if none of the columns are present and FALSE otherwise\n   117→#' @importFrom checkmate test_names\n   118→#' @keywords internal_input_check\n   119→test_columns_not_present <- function(data, columns) {\n   120→  test_names(colnames(data), disjunct.from = columns)\n   121→}\n   122→"}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"55222973-8c36-4a3c-8069-32135128e31f"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01SwPy5to5eA8qdyYNfqn5mu","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Fou4y3v9oEA9x64qgyURv4","name":"Bash","input":{"command":"grep -r \"\\.data\\.frame\\|\\.data\\.frame\" /Users/nikos/github-synced/scoringutils-pipeline/R --include=\"*.R\" | head -20"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":1733,"cache_read_input_tokens":61892,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1733},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"18f9eacf-e5d6-4954-8166-301d77868f4e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Fou4y3v9oEA9x64qgyURv4","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/R/get-correlations.R:    as.data.frame((cor_mat)),\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-interval-range.R:#'   If you want a `data.frame` instead, simply call [as.data.frame()] on the\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-sample.R:#'   If you want a `data.frame` instead, simply call [as.data.frame()] on the\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-scores.R:  } else if (is.data.frame(ret)) {\n/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R:  if (is.data.frame(dots[[1]])) {","is_error":false}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"af10777e-4a8c-4074-acdf-2ae52e6739bc"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01SwPy5to5eA8qdyYNfqn5mu","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Hu9hWRELQidW7LPWqa4Bga","name":"Bash","input":{"command":"grep -r \"data\\.frame.*method\" /Users/nikos/github-synced/scoringutils-pipeline/R --include=\"*.R\""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":1733,"cache_read_input_tokens":61892,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1733},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"e7009c2c-e62d-4bb4-a92f-ffc0724ec043"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Hu9hWRELQidW7LPWqa4Bga","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"4a7c4fc9-f2d1-4ad6-993e-a8cf024f7a2b"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01TJfB199Lmz8qPXUMgo5jSg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01U9ubYDhsMxZcPtTLRnGJBW","name":"Bash","input":{"command":"find /Users/nikos/github-synced/scoringutils-pipeline -name \"*983*\" -type f 2>/dev/null"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":506,"cache_read_input_tokens":63625,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":506},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"e44139ee-cd63-4f12-9199-b95e2532bdda"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01U9ubYDhsMxZcPtTLRnGJBW","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"3dbbe1b6-1fda-4a1a-adad-a169b91615f0"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PkqPT8yuGFJ6ZR4X3xhjjw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EUEiUUKPBk9U2od2xgCJjR","name":"Bash","input":{"command":"grep -A 20 \"983\" /Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md | head -30"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":249,"cache_read_input_tokens":64131,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":249},"output_tokens":7,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"67d430a0-a3dc-4e5a-8818-d9c93d23dca4"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EUEiUUKPBk9U2od2xgCJjR","type":"tool_result","content":"| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [ ] | [ ] | — | [ ] |\n| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [ ] | [ ] | — | [ ] |\n| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [ ] | [ ] | — | [ ] |\n| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [ ] | [ ] | — | [ ] |\n| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n--\n| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |","is_error":false}]},"parent_tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"1732e60f-da7f-4755-a06b-6123d8955108"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01LXyWG7DywvWPH3J5HQcNsC","type":"tool_result","content":[{"type":"text","text":"Perfect! Now I have the complete picture. Let me compile the comprehensive findings:\n\n## Comprehensive Research Findings on `as_forecast_*` S3 Methods\n\nBased on my thorough analysis of the scoringutils-pipeline codebase, here are the detailed findings for issue #983:\n\n### 1. All Generic Definitions and Default Methods\n\nAll 7 forecast types have a generic function and a `.default` S3 method located in their respective files:\n\n| Type | Generic Definition | `.default` Method | File | Lines |\n|------|-------------------|-------------------|------|-------|\n| **binary** | Line 34 | Lines 42-56 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-binary.R` | Generic: 34-36; Method: 40-56 |\n| **point** | Line 21 | Lines 30-44 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-point.R` | Generic: 21-23; Method: 28-44 |\n| **quantile** | Line 33 | Lines 45-75 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-quantile.R` | Generic: 33-35; Method: 43-75 |\n| **sample** | Line 26 | Lines 36-52 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-sample.R` | Generic: 26-28; Method: 35-52 |\n| **nominal** | Line 42 | Lines 53-69 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-nominal.R` | Generic: 42-44; Method: 51-69 |\n| **ordinal** | Line 42 | Lines 53-69 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-ordinal.R` | Generic: 42-44; Method: 51-69 |\n| **multivariate_sample** | Line 41 | Lines 58-77 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-multivariate-sample.R` | Generic: 41-43; Method: 58-77 |\n\n---\n\n### 2. Detailed Analysis of Each Type\n\n#### **as_forecast_binary** (Lines 34-56 of class-forecast-binary.R)\n```r\nas_forecast_binary <- function(data, ...) {\n  UseMethod(\"as_forecast_binary\")\n}\n\nas_forecast_binary.default <- function(data,\n                                       forecast_unit = NULL,\n                                       observed = NULL,\n                                       predicted = NULL,\n                                       ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed, predicted = predicted)\n  data <- new_forecast(data, \"forecast_binary\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key characteristics:**\n- Calls `as_forecast_generic()` (internal helper at lines 11-37 of class-forecast.R)\n- Does NOT explicitly call `as.data.table()` — this happens deep inside `as_forecast_generic()` via `ensure_data.table()`\n- Assumes data.frame-like input with columns\n- No special handling for type-specific columns beyond `observed` and `predicted`\n\n#### **as_forecast_point** (Lines 21-44 of class-forecast-point.R)\n- **Identical structure to binary** except for class assignment\n- Expected columns: `observed` (numeric), `predicted` (numeric)\n- No quantile_level or sample_id columns\n\n#### **as_forecast_quantile** (Lines 33-75 of class-forecast-quantile.R)\n```r\nas_forecast_quantile.default <- function(data,\n                                         forecast_unit = NULL,\n                                         observed = NULL,\n                                         predicted = NULL,\n                                         quantile_level = NULL,\n                                         ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted,\n                              quantile_level = quantile_level)\n  # Additional: Rounding correction for quantile_level (lines 58-70)\n  unique_q_levels <- sort(unique(data$quantile_level))\n  level_diffs <- diff(unique_q_levels)\n  if (any(level_diffs <= 1e-10)) {\n    cli_warn(\"rounding issue...\")\n    data$quantile_level <- round(data$quantile_level, digits = 9)\n  }\n  data <- new_forecast(data, \"forecast_quantile\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key differences:**\n- Has extra `quantile_level` parameter (not in binary/point)\n- Includes rounding correction logic for quantile_level precision issues\n- Required column: `quantile_level` (numeric, between 0 and 1)\n\n#### **as_forecast_sample** (Lines 26-52 of class-forecast-sample.R)\n```r\nas_forecast_sample.default <- function(data,\n                                       forecast_unit = NULL,\n                                       observed = NULL,\n                                       predicted = NULL,\n                                       sample_id = NULL,\n                                       ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted, sample_id = sample_id)\n  data <- new_forecast(data, \"forecast_sample\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key characteristics:**\n- Has extra `sample_id` parameter\n- Required column: `sample_id` (any type, unique within forecast)\n\n#### **as_forecast_nominal** (Lines 42-69 of class-forecast-nominal.R)\n```r\nas_forecast_nominal.default <- function(data,\n                                        forecast_unit = NULL,\n                                        observed = NULL,\n                                        predicted = NULL,\n                                        predicted_label = NULL,\n                                        ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted,\n                              predicted_label = predicted_label)\n  data <- new_forecast(data, \"forecast_nominal\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key characteristics:**\n- Has extra `predicted_label` parameter\n- `observed`: factor with N unordered levels\n- `predicted`: numeric probabilities\n- `predicted_label`: factor with N unordered levels (must match observed's levels)\n\n#### **as_forecast_ordinal** (Lines 42-69 of class-forecast-ordinal.R)\n- **Identical structure to nominal** except:\n- `observed`: factor with N **ordered** levels\n- `predicted_label`: factor with N **ordered** levels (must match observed's levels exactly, in same order)\n\n#### **as_forecast_multivariate_sample** (Lines 41-77 of class-forecast-multivariate-sample.R)\n```r\nas_forecast_multivariate_sample.default <- function(data,\n                                                    joint_across,\n                                                    forecast_unit = NULL,\n                                                    observed = NULL,\n                                                    predicted = NULL,\n                                                    sample_id = NULL,\n                                                    ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted, sample_id = sample_id)\n  data <- set_grouping(data, joint_across)  # ADDITIONAL STEP\n  data <- new_forecast(data, \"forecast_sample_multivariate\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key differences:**\n- Requires `joint_across` parameter (character vector, not optional)\n- Calls `set_grouping()` to create `.mv_group_id` column\n- Same base columns as `forecast_sample` plus the grouping layer\n\n---\n\n### 3. The Core Helper: `as_forecast_generic()` (Lines 11-37 of class-forecast.R)\n\nThis is the **internal** function all `.default` methods delegate to:\n\n```r\nas_forecast_generic <- function(data, forecast_unit = NULL, ...) {\n  # KEY LINE: Ensures data is a data.table\n  data <- ensure_data.table(data)\n  \n  # Column renaming logic\n  oldnames <- list(...)\n  newnames <- names(oldnames)\n  provided <- !sapply(oldnames, is.null)\n  \n  # Validates rename args and applies renames\n  oldnames <- unlist(oldnames[provided])\n  newnames <- unlist(newnames[provided])\n  if (!is.null(oldnames) && length(oldnames) > 0) {\n    setnames(data, old = oldnames, new = newnames)\n  }\n  \n  # Set forecast unit (error handling done in set_forecast_unit)\n  if (!is.null(forecast_unit)) {\n    data <- set_forecast_unit(data, forecast_unit)\n  }\n  return(data)\n}\n```\n\n**What `ensure_data.table()` does** (Lines 10-17 of check-input-helpers.R):\n```r\nensure_data.table <- function(data) {\n  if (is.data.table(data)) {\n    data <- copy(data)\n  } else {\n    data <- as.data.table(data)  # <-- COERCION HAPPENS HERE\n  }\n  return(data)\n}\n```\n\n---\n\n### 4. Current Test Coverage\n\n#### **test-class-forecast-binary.R**\n- Lines 4-8: `as_forecast_binary(example_binary)` — accepts pre-validated example data\n- Lines 27-46: `assert_forecast` validation tests; tests call `as_forecast_binary()` on modified data\n- **Input types tested:** `example_binary` (which is already a `forecast_binary` object), no `.data.frame` method tests\n\n#### **test-class-forecast-point.R**\n- Lines 5-9: `as_forecast_point(as_forecast_quantile(na.omit(example_quantile)))` — converts quantile to point\n- Lines 28-41: Validation tests modify `example_point` then pass to `as_forecast_point()`\n- **Input types tested:** forecast objects and data.tables, no explicit `.data.frame` tests\n\n#### **test-class-forecast-quantile.R**\n- Lines 5-30: `as_forecast_quantile(example_quantile_df)` on line 93 — **EXPLICITLY TESTS .data.frame INPUT**\n- Lines 92-94: **Line 93 is key:** `expect_no_condition(as_forecast_quantile(example_quantile_df))` explicitly tests `.data.frame` input\n- This is the only existing `.data.frame` test case\n- Lines 96-150: Comprehensive tests with `data.frame()` objects, tibbles, and modifications\n\n#### **test-class-forecast-sample.R**\n- Lines 4-20: `as_forecast_sample(test, observed = \"obs\", predicted = \"pred\", sample_id = \"sample\")` renames columns on renamed data.table\n- No `.data.frame` tests\n\n#### **test-class-forecast-nominal.R**\n- Lines 4-19: `as_forecast_nominal(ex)` on copied `example_nominal` data.table\n- Lines 16-18: `setnames()` to rename, then calls `as_forecast_nominal(ex, predicted_label = \"label\")`\n- No `.data.frame` tests\n\n#### **test-class-forecast-ordinal.R**\n- Lines 4-19: Same pattern as nominal — `as_forecast_ordinal(ex)` on data.table\n- No `.data.frame` tests\n\n#### **test-class-forecast-multivariate-sample.R**\n- Lines 4-20: `as_forecast_multivariate_sample(test, ...)` with renamed data.table\n- Lines 207-242: Error handling tests\n- No `.data.frame` tests\n\n---\n\n### 5. NAMESPACE Exports\n\n**All 7 generics are exported:**\n```\nexport(as_forecast_binary)\nexport(as_forecast_multivariate_sample)\nexport(as_forecast_nominal)\nexport(as_forecast_ordinal)\nexport(as_forecast_point)\nexport(as_forecast_quantile)\nexport(as_forecast_sample)\n```\n\n**All 7 `.default` methods are registered:**\n```\nS3method(as_forecast_binary,default)\nS3method(as_forecast_multivariate_sample,default)\nS3method(as_forecast_nominal,default)\nS3method(as_forecast_ordinal,default)\nS3method(as_forecast_point,default)\nS3method(as_forecast_quantile,default)\nS3method(as_forecast_sample,default)\n```\n\n**No `.data.frame` methods are registered** (which is the goal of issue #983).\n\n---\n\n### 6. Key Insights for Issue #983\n\n**Current Behavior:**\n1. All 7 `.default` methods silently accept **any type** of input\n2. They all delegate to `as_forecast_generic()` which calls `ensure_data.table()`\n3. `ensure_data.table()` uses `as.data.table()` for type coercion (works for data.frames, tibbles, lists, etc.)\n4. The `.default` methods are effectively `.data.frame` methods (they assume tabular input)\n\n**Why This is a Problem (per issue #983):**\n- The naming is misleading: `.default` suggests \"any type,\" but really only data.frame-like inputs work\n- No helpful error for unsupported types (e.g., vectors, matrices, non-tabular structures)\n- No explicit contract that data.frame input is expected\n\n**Required Changes (per ISSUE_TRIAGE.md):**\n> \"Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests.\"\n\n**What Needs to Happen:**\n1. For each of 7 types:\n   - Rename `as_forecast_TYPE.default` to `as_forecast_TYPE.data.frame`\n   - Create new `as_forecast_TYPE.default` that calls `cli_abort()` with helpful message\n   - Consider adding methods for `tibble` or other common types if desired\n\n2. Update NAMESPACE (7 S3method lines)\n\n3. Update tests to:\n   - Explicitly test `.data.frame` inputs (currently only quantile has one example)\n   - Test that `.default` methods error appropriately\n\n4. Update documentation to clarify that `.data.frame` method is the main entry point\n\n---\n\n### 7. Example of How to Implement\n\nHere's the pattern for one type (binary):\n\n**Current code (lines 34-56 of class-forecast-binary.R):**\n```r\nas_forecast_binary <- function(data, ...) {\n  UseMethod(\"as_forecast_binary\")\n}\n\nas_forecast_binary.default <- function(data, forecast_unit = NULL, \n                                       observed = NULL, predicted = NULL, ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed, predicted = predicted)\n  data <- new_forecast(data, \"forecast_binary\")\n  assert_forecast(data)\n  return(data)\n}\n```\n\n**After issue #983:**\n```r\nas_forecast_binary <- function(data, ...) {\n  UseMethod(\"as_forecast_binary\")\n}\n\nas_forecast_binary.data.frame <- function(data, forecast_unit = NULL, \n                                          observed = NULL, predicted = NULL, ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed, predicted = predicted)\n  data <- new_forecast(data, \"forecast_binary\")\n  assert_forecast(data)\n  return(data)\n}\n\nas_forecast_binary.default <- function(data, ...) {\n  cli_abort(\n    c(\n      \"!\" = \"{.arg data} must be a data.frame-like object (data.frame, data.table, tibble, etc.)\",\n      \"i\" = \"Got {.cls {class(data)}} instead.\",\n      \"i\" = \"Please convert to a data.frame or use {.fn as.data.frame} first.\"\n    )\n  )\n}\n```\n\n---\n\n### 8. Summary Table: What Each Method Expects\n\n| Method | Input Type | Key Columns | Special Handling |\n|--------|-----------|-------------|-----------------|\n| `.default` (all) | ANY | — | Currently accepts all; should error |\n| `.data.frame` (new) | data.frame/table/tibble | `observed`, `predicted` | Column renaming via parameters |\n| `binary.data.frame` | tabular | `observed` (factor, 2 levels), `predicted` (numeric, [0,1]) | None |\n| `point.data.frame` | tabular | `observed` (numeric), `predicted` (numeric) | None |\n| `quantile.data.frame` | tabular | `observed`, `predicted`, `quantile_level` | Rounding correction (lines 58-70) |\n| `sample.data.frame` | tabular | `observed`, `predicted`, `sample_id` | None |\n| `nominal.data.frame` | tabular | `observed` (unordered factor), `predicted` (numeric), `predicted_label` (unordered factor) | Completeness validation |\n| `ordinal.data.frame` | tabular | `observed` (ordered factor), `predicted` (numeric), `predicted_label` (ordered factor) | Level matching validation |\n| `multivariate_sample.data.frame` | tabular | `observed`, `predicted`, `sample_id`, + requires `joint_across` | Creates `.mv_group_id` grouping column |\n\n---\n\nThis research provides a complete foundation for implementing issue #983. The key insight is that all 7 existing `.default` methods assume data.frame-like input and should be renamed to `.data.frame` with appropriate new `.default` methods added to signal unsupported types clearly to users."},{"type":"text","text":"agentId: a9b204f (for resuming to continue this agent's work if needed)\n<usage>total_tokens: 75827\ntool_uses: 27\nduration_ms: 56214</usage>"}]}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"06db3476-d7bc-4c1b-8654-5586a0d1347c","tool_use_result":{"status":"completed","prompt":"I need to understand the as_forecast_* S3 methods in this R package for issue #983 which asks to create `.data.frame` methods for `as_forecast<type>`.\n\nPlease do the following research:\n\n1. Find all `as_forecast_*` generic definitions and their `.default` methods in the R/ directory. List each generic and its default method's file + line number.\n\n2. For each of the 7 forecast types, identify:\n   - The generic definition (e.g., `as_forecast_binary <- function(...)`)\n   - The `.default` method (e.g., `as_forecast_binary.default <- function(...)`)\n   - What the `.default` method does - does it assume data.frame-like input? Does it call `as.data.table()`?\n\n3. Look at the existing test files for these classes:\n   - `tests/testthat/test-class-forecast-binary.R`\n   - `tests/testthat/test-class-forecast-point.R`  \n   - `tests/testthat/test-class-forecast-quantile.R`\n   - `tests/testthat/test-class-forecast-sample.R`\n   - `tests/testthat/test-class-forecast-nominal.R`\n   - `tests/testthat/test-class-forecast-ordinal.R`\n   - `tests/testthat/test-class-forecast-multivariate-sample.R`\n   \n   For each, find any tests that call `as_forecast_*()` and note what inputs are used (data.table, data.frame, tibble, etc.) and what the tests verify.\n\n4. Check if `as_forecast_generic()` exists (the triage mentions it's internal) and understand its role.\n\n5. Look at the NAMESPACE file to see what's exported.\n\nReturn comprehensive findings.","agentId":"a9b204f","content":[{"type":"text","text":"Perfect! Now I have the complete picture. Let me compile the comprehensive findings:\n\n## Comprehensive Research Findings on `as_forecast_*` S3 Methods\n\nBased on my thorough analysis of the scoringutils-pipeline codebase, here are the detailed findings for issue #983:\n\n### 1. All Generic Definitions and Default Methods\n\nAll 7 forecast types have a generic function and a `.default` S3 method located in their respective files:\n\n| Type | Generic Definition | `.default` Method | File | Lines |\n|------|-------------------|-------------------|------|-------|\n| **binary** | Line 34 | Lines 42-56 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-binary.R` | Generic: 34-36; Method: 40-56 |\n| **point** | Line 21 | Lines 30-44 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-point.R` | Generic: 21-23; Method: 28-44 |\n| **quantile** | Line 33 | Lines 45-75 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-quantile.R` | Generic: 33-35; Method: 43-75 |\n| **sample** | Line 26 | Lines 36-52 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-sample.R` | Generic: 26-28; Method: 35-52 |\n| **nominal** | Line 42 | Lines 53-69 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-nominal.R` | Generic: 42-44; Method: 51-69 |\n| **ordinal** | Line 42 | Lines 53-69 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-ordinal.R` | Generic: 42-44; Method: 51-69 |\n| **multivariate_sample** | Line 41 | Lines 58-77 | `/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-multivariate-sample.R` | Generic: 41-43; Method: 58-77 |\n\n---\n\n### 2. Detailed Analysis of Each Type\n\n#### **as_forecast_binary** (Lines 34-56 of class-forecast-binary.R)\n```r\nas_forecast_binary <- function(data, ...) {\n  UseMethod(\"as_forecast_binary\")\n}\n\nas_forecast_binary.default <- function(data,\n                                       forecast_unit = NULL,\n                                       observed = NULL,\n                                       predicted = NULL,\n                                       ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed, predicted = predicted)\n  data <- new_forecast(data, \"forecast_binary\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key characteristics:**\n- Calls `as_forecast_generic()` (internal helper at lines 11-37 of class-forecast.R)\n- Does NOT explicitly call `as.data.table()` — this happens deep inside `as_forecast_generic()` via `ensure_data.table()`\n- Assumes data.frame-like input with columns\n- No special handling for type-specific columns beyond `observed` and `predicted`\n\n#### **as_forecast_point** (Lines 21-44 of class-forecast-point.R)\n- **Identical structure to binary** except for class assignment\n- Expected columns: `observed` (numeric), `predicted` (numeric)\n- No quantile_level or sample_id columns\n\n#### **as_forecast_quantile** (Lines 33-75 of class-forecast-quantile.R)\n```r\nas_forecast_quantile.default <- function(data,\n                                         forecast_unit = NULL,\n                                         observed = NULL,\n                                         predicted = NULL,\n                                         quantile_level = NULL,\n                                         ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted,\n                              quantile_level = quantile_level)\n  # Additional: Rounding correction for quantile_level (lines 58-70)\n  unique_q_levels <- sort(unique(data$quantile_level))\n  level_diffs <- diff(unique_q_levels)\n  if (any(level_diffs <= 1e-10)) {\n    cli_warn(\"rounding issue...\")\n    data$quantile_level <- round(data$quantile_level, digits = 9)\n  }\n  data <- new_forecast(data, \"forecast_quantile\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key differences:**\n- Has extra `quantile_level` parameter (not in binary/point)\n- Includes rounding correction logic for quantile_level precision issues\n- Required column: `quantile_level` (numeric, between 0 and 1)\n\n#### **as_forecast_sample** (Lines 26-52 of class-forecast-sample.R)\n```r\nas_forecast_sample.default <- function(data,\n                                       forecast_unit = NULL,\n                                       observed = NULL,\n                                       predicted = NULL,\n                                       sample_id = NULL,\n                                       ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted, sample_id = sample_id)\n  data <- new_forecast(data, \"forecast_sample\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key characteristics:**\n- Has extra `sample_id` parameter\n- Required column: `sample_id` (any type, unique within forecast)\n\n#### **as_forecast_nominal** (Lines 42-69 of class-forecast-nominal.R)\n```r\nas_forecast_nominal.default <- function(data,\n                                        forecast_unit = NULL,\n                                        observed = NULL,\n                                        predicted = NULL,\n                                        predicted_label = NULL,\n                                        ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted,\n                              predicted_label = predicted_label)\n  data <- new_forecast(data, \"forecast_nominal\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key characteristics:**\n- Has extra `predicted_label` parameter\n- `observed`: factor with N unordered levels\n- `predicted`: numeric probabilities\n- `predicted_label`: factor with N unordered levels (must match observed's levels)\n\n#### **as_forecast_ordinal** (Lines 42-69 of class-forecast-ordinal.R)\n- **Identical structure to nominal** except:\n- `observed`: factor with N **ordered** levels\n- `predicted_label`: factor with N **ordered** levels (must match observed's levels exactly, in same order)\n\n#### **as_forecast_multivariate_sample** (Lines 41-77 of class-forecast-multivariate-sample.R)\n```r\nas_forecast_multivariate_sample.default <- function(data,\n                                                    joint_across,\n                                                    forecast_unit = NULL,\n                                                    observed = NULL,\n                                                    predicted = NULL,\n                                                    sample_id = NULL,\n                                                    ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed,\n                              predicted = predicted, sample_id = sample_id)\n  data <- set_grouping(data, joint_across)  # ADDITIONAL STEP\n  data <- new_forecast(data, \"forecast_sample_multivariate\")\n  assert_forecast(data)\n  return(data)\n}\n```\n**Key differences:**\n- Requires `joint_across` parameter (character vector, not optional)\n- Calls `set_grouping()` to create `.mv_group_id` column\n- Same base columns as `forecast_sample` plus the grouping layer\n\n---\n\n### 3. The Core Helper: `as_forecast_generic()` (Lines 11-37 of class-forecast.R)\n\nThis is the **internal** function all `.default` methods delegate to:\n\n```r\nas_forecast_generic <- function(data, forecast_unit = NULL, ...) {\n  # KEY LINE: Ensures data is a data.table\n  data <- ensure_data.table(data)\n  \n  # Column renaming logic\n  oldnames <- list(...)\n  newnames <- names(oldnames)\n  provided <- !sapply(oldnames, is.null)\n  \n  # Validates rename args and applies renames\n  oldnames <- unlist(oldnames[provided])\n  newnames <- unlist(newnames[provided])\n  if (!is.null(oldnames) && length(oldnames) > 0) {\n    setnames(data, old = oldnames, new = newnames)\n  }\n  \n  # Set forecast unit (error handling done in set_forecast_unit)\n  if (!is.null(forecast_unit)) {\n    data <- set_forecast_unit(data, forecast_unit)\n  }\n  return(data)\n}\n```\n\n**What `ensure_data.table()` does** (Lines 10-17 of check-input-helpers.R):\n```r\nensure_data.table <- function(data) {\n  if (is.data.table(data)) {\n    data <- copy(data)\n  } else {\n    data <- as.data.table(data)  # <-- COERCION HAPPENS HERE\n  }\n  return(data)\n}\n```\n\n---\n\n### 4. Current Test Coverage\n\n#### **test-class-forecast-binary.R**\n- Lines 4-8: `as_forecast_binary(example_binary)` — accepts pre-validated example data\n- Lines 27-46: `assert_forecast` validation tests; tests call `as_forecast_binary()` on modified data\n- **Input types tested:** `example_binary` (which is already a `forecast_binary` object), no `.data.frame` method tests\n\n#### **test-class-forecast-point.R**\n- Lines 5-9: `as_forecast_point(as_forecast_quantile(na.omit(example_quantile)))` — converts quantile to point\n- Lines 28-41: Validation tests modify `example_point` then pass to `as_forecast_point()`\n- **Input types tested:** forecast objects and data.tables, no explicit `.data.frame` tests\n\n#### **test-class-forecast-quantile.R**\n- Lines 5-30: `as_forecast_quantile(example_quantile_df)` on line 93 — **EXPLICITLY TESTS .data.frame INPUT**\n- Lines 92-94: **Line 93 is key:** `expect_no_condition(as_forecast_quantile(example_quantile_df))` explicitly tests `.data.frame` input\n- This is the only existing `.data.frame` test case\n- Lines 96-150: Comprehensive tests with `data.frame()` objects, tibbles, and modifications\n\n#### **test-class-forecast-sample.R**\n- Lines 4-20: `as_forecast_sample(test, observed = \"obs\", predicted = \"pred\", sample_id = \"sample\")` renames columns on renamed data.table\n- No `.data.frame` tests\n\n#### **test-class-forecast-nominal.R**\n- Lines 4-19: `as_forecast_nominal(ex)` on copied `example_nominal` data.table\n- Lines 16-18: `setnames()` to rename, then calls `as_forecast_nominal(ex, predicted_label = \"label\")`\n- No `.data.frame` tests\n\n#### **test-class-forecast-ordinal.R**\n- Lines 4-19: Same pattern as nominal — `as_forecast_ordinal(ex)` on data.table\n- No `.data.frame` tests\n\n#### **test-class-forecast-multivariate-sample.R**\n- Lines 4-20: `as_forecast_multivariate_sample(test, ...)` with renamed data.table\n- Lines 207-242: Error handling tests\n- No `.data.frame` tests\n\n---\n\n### 5. NAMESPACE Exports\n\n**All 7 generics are exported:**\n```\nexport(as_forecast_binary)\nexport(as_forecast_multivariate_sample)\nexport(as_forecast_nominal)\nexport(as_forecast_ordinal)\nexport(as_forecast_point)\nexport(as_forecast_quantile)\nexport(as_forecast_sample)\n```\n\n**All 7 `.default` methods are registered:**\n```\nS3method(as_forecast_binary,default)\nS3method(as_forecast_multivariate_sample,default)\nS3method(as_forecast_nominal,default)\nS3method(as_forecast_ordinal,default)\nS3method(as_forecast_point,default)\nS3method(as_forecast_quantile,default)\nS3method(as_forecast_sample,default)\n```\n\n**No `.data.frame` methods are registered** (which is the goal of issue #983).\n\n---\n\n### 6. Key Insights for Issue #983\n\n**Current Behavior:**\n1. All 7 `.default` methods silently accept **any type** of input\n2. They all delegate to `as_forecast_generic()` which calls `ensure_data.table()`\n3. `ensure_data.table()` uses `as.data.table()` for type coercion (works for data.frames, tibbles, lists, etc.)\n4. The `.default` methods are effectively `.data.frame` methods (they assume tabular input)\n\n**Why This is a Problem (per issue #983):**\n- The naming is misleading: `.default` suggests \"any type,\" but really only data.frame-like inputs work\n- No helpful error for unsupported types (e.g., vectors, matrices, non-tabular structures)\n- No explicit contract that data.frame input is expected\n\n**Required Changes (per ISSUE_TRIAGE.md):**\n> \"Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests.\"\n\n**What Needs to Happen:**\n1. For each of 7 types:\n   - Rename `as_forecast_TYPE.default` to `as_forecast_TYPE.data.frame`\n   - Create new `as_forecast_TYPE.default` that calls `cli_abort()` with helpful message\n   - Consider adding methods for `tibble` or other common types if desired\n\n2. Update NAMESPACE (7 S3method lines)\n\n3. Update tests to:\n   - Explicitly test `.data.frame` inputs (currently only quantile has one example)\n   - Test that `.default` methods error appropriately\n\n4. Update documentation to clarify that `.data.frame` method is the main entry point\n\n---\n\n### 7. Example of How to Implement\n\nHere's the pattern for one type (binary):\n\n**Current code (lines 34-56 of class-forecast-binary.R):**\n```r\nas_forecast_binary <- function(data, ...) {\n  UseMethod(\"as_forecast_binary\")\n}\n\nas_forecast_binary.default <- function(data, forecast_unit = NULL, \n                                       observed = NULL, predicted = NULL, ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed, predicted = predicted)\n  data <- new_forecast(data, \"forecast_binary\")\n  assert_forecast(data)\n  return(data)\n}\n```\n\n**After issue #983:**\n```r\nas_forecast_binary <- function(data, ...) {\n  UseMethod(\"as_forecast_binary\")\n}\n\nas_forecast_binary.data.frame <- function(data, forecast_unit = NULL, \n                                          observed = NULL, predicted = NULL, ...) {\n  data <- as_forecast_generic(data, forecast_unit, observed = observed, predicted = predicted)\n  data <- new_forecast(data, \"forecast_binary\")\n  assert_forecast(data)\n  return(data)\n}\n\nas_forecast_binary.default <- function(data, ...) {\n  cli_abort(\n    c(\n      \"!\" = \"{.arg data} must be a data.frame-like object (data.frame, data.table, tibble, etc.)\",\n      \"i\" = \"Got {.cls {class(data)}} instead.\",\n      \"i\" = \"Please convert to a data.frame or use {.fn as.data.frame} first.\"\n    )\n  )\n}\n```\n\n---\n\n### 8. Summary Table: What Each Method Expects\n\n| Method | Input Type | Key Columns | Special Handling |\n|--------|-----------|-------------|-----------------|\n| `.default` (all) | ANY | — | Currently accepts all; should error |\n| `.data.frame` (new) | data.frame/table/tibble | `observed`, `predicted` | Column renaming via parameters |\n| `binary.data.frame` | tabular | `observed` (factor, 2 levels), `predicted` (numeric, [0,1]) | None |\n| `point.data.frame` | tabular | `observed` (numeric), `predicted` (numeric) | None |\n| `quantile.data.frame` | tabular | `observed`, `predicted`, `quantile_level` | Rounding correction (lines 58-70) |\n| `sample.data.frame` | tabular | `observed`, `predicted`, `sample_id` | None |\n| `nominal.data.frame` | tabular | `observed` (unordered factor), `predicted` (numeric), `predicted_label` (unordered factor) | Completeness validation |\n| `ordinal.data.frame` | tabular | `observed` (ordered factor), `predicted` (numeric), `predicted_label` (ordered factor) | Level matching validation |\n| `multivariate_sample.data.frame` | tabular | `observed`, `predicted`, `sample_id`, + requires `joint_across` | Creates `.mv_group_id` grouping column |\n\n---\n\nThis research provides a complete foundation for implementing issue #983. The key insight is that all 7 existing `.default` methods assume data.frame-like input and should be renamed to `.data.frame` with appropriate new `.default` methods added to signal unsupported types clearly to users."}],"totalDurationMs":56214,"totalTokens":75827,"totalToolUseCount":27,"usage":{"input_tokens":2,"cache_creation_input_tokens":6956,"cache_read_input_tokens":64380,"output_tokens":4489,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":6956,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01HP3QFjz2NEn2de3EM4omD3","type":"message","role":"assistant","content":[{"type":"text","text":"Excellent. Now I have a thorough understanding of the codebase. Let me also quickly check a few specific existing test patterns to understand conventions."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5113,"cache_read_input_tokens":63609,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5113},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"ce87125d-16bd-45d6-ad29-8dc3c1dd3c22"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01HP3QFjz2NEn2de3EM4omD3","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01RzgRpYqcCUYCS76W42up1R","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-quantile.R","offset":85,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5113,"cache_read_input_tokens":63609,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5113},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"dc73ef36-64eb-47b8-a165-ec97fce81390"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RzgRpYqcCUYCS76W42up1R","type":"tool_result","content":"    85→    suppressMessages(suppressWarnings(as_forecast_quantile(\n    86→      data.table::copy(example_quantile)[, c(\"observed\", \"predicted\") := NULL]\n    87→    ))),\n    88→    \"Assertion on 'data' failed: Columns 'observed', 'predicted' not found in data.\"\n    89→  )\n    90→})\n    91→\n    92→test_that(\"as_forecast_quantile() works with a data.frame\", {\n    93→  expect_no_condition(as_forecast_quantile(example_quantile_df))\n    94→})\n    95→\n    96→test_that(\"as_forecast_quantiles works\", {\n    97→  samples <- data.frame(\n    98→    date = as.Date(\"2020-01-01\") + 1:10,\n    99→    model = \"model1\",\n   100→    observed = 1:10,\n   101→    predicted = c(rep(0, 10), 2:11, 3:12, 4:13, rep(100, 10)),\n   102→    sample_id = rep(1:5, each = 10),\n   103→    stringsAsFactors = FALSE\n   104→  ) |>\n   105→    as_forecast_sample()\n   106→\n   107→  quantile <- data.frame(\n   108→    date = rep(as.Date(\"2020-01-01\") + 1:10, each = 2),\n   109→    model = \"model1\",\n   110→    observed = rep(1:10, each = 2),\n   111→    quantile_level = c(0.25, 0.75),\n   112→    predicted = rep(2:11, each = 2) + c(0, 2),\n   113→    stringsAsFactors = FALSE\n   114→  )"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"57e7359a-fcb0-498f-9729-835ddeef21a7","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-quantile.R","content":"    suppressMessages(suppressWarnings(as_forecast_quantile(\n      data.table::copy(example_quantile)[, c(\"observed\", \"predicted\") := NULL]\n    ))),\n    \"Assertion on 'data' failed: Columns 'observed', 'predicted' not found in data.\"\n  )\n})\n\ntest_that(\"as_forecast_quantile() works with a data.frame\", {\n  expect_no_condition(as_forecast_quantile(example_quantile_df))\n})\n\ntest_that(\"as_forecast_quantiles works\", {\n  samples <- data.frame(\n    date = as.Date(\"2020-01-01\") + 1:10,\n    model = \"model1\",\n    observed = 1:10,\n    predicted = c(rep(0, 10), 2:11, 3:12, 4:13, rep(100, 10)),\n    sample_id = rep(1:5, each = 10),\n    stringsAsFactors = FALSE\n  ) |>\n    as_forecast_sample()\n\n  quantile <- data.frame(\n    date = rep(as.Date(\"2020-01-01\") + 1:10, each = 2),\n    model = \"model1\",\n    observed = rep(1:10, each = 2),\n    quantile_level = c(0.25, 0.75),\n    predicted = rep(2:11, each = 2) + c(0, 2),\n    stringsAsFactors = FALSE\n  )","numLines":30,"startLine":85,"totalLines":430}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01HP3QFjz2NEn2de3EM4omD3","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01StT8LWeQTKXDKA2UzVNqNS","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-binary.R","limit":50}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5113,"cache_read_input_tokens":63609,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5113},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"85de90ca-3398-4ecb-87a2-6c4a57790958"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01StT8LWeQTKXDKA2UzVNqNS","type":"tool_result","content":"     1→# ==============================================================================\n     2→# as_forecast_binary() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→test_that(\"output of as_forecast_binary() is accepted as input to score()\", {\n     5→  check <- suppressMessages(as_forecast_binary(example_binary))\n     6→  score_check <- expect_no_error(score(na.omit(check)))\n     7→  expect_identical(score_check, suppressMessages(score(as_forecast_binary(example_binary))))\n     8→})\n     9→\n    10→\n    11→# ==============================================================================\n    12→# is_forecast_binary() # nolint: commented_code_linter\n    13→# ==============================================================================\n    14→test_that(\"is_forecast_binary() works as expected\", {\n    15→  expect_true(is_forecast_binary(example_binary))\n    16→  expect_false(is_forecast_binary(example_point))\n    17→  expect_false(is_forecast_binary(example_quantile))\n    18→  expect_false(is_forecast_binary(example_sample_continuous))\n    19→  expect_false(is_forecast_binary(example_nominal))\n    20→})\n    21→\n    22→\n    23→# ==============================================================================\n    24→# assert_forecast.forecast_binary() # nolint: commented_code_linter\n    25→# ==============================================================================\n    26→test_that(\"assert_forecast.forecast_binary works as expected\", {\n    27→  test <- na.omit(as.data.table(example_binary))\n    28→  test[, \"sample_id\" := seq_len(nrow(test))]\n    29→\n    30→  # error if there is a superfluous sample_id column\n    31→  expect_error(\n    32→    as_forecast_binary(test),\n    33→    paste(\n    34→      \"Input looks like a binary forecast, but an additional column\",\n    35→      \"called `sample_id` or `quantile` was found.\"\n    36→    )\n    37→  )\n    38→\n    39→  # expect error if probabilties are not in [0, 1]\n    40→  test <- na.omit(as.data.table(example_binary))\n    41→  test[, \"predicted\" := predicted + 1]\n    42→  expect_error(\n    43→    as_forecast_binary(test),\n    44→    \"Input looks like a binary forecast, but found the following issue\"\n    45→  )\n    46→})\n    47→\n    48→\n    49→\n    50→# =============================================================================="}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"5c303b16-73e8-4d05-b382-44cf3138f6c2","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-class-forecast-binary.R","content":"# ==============================================================================\n# as_forecast_binary() # nolint: commented_code_linter\n# ==============================================================================\ntest_that(\"output of as_forecast_binary() is accepted as input to score()\", {\n  check <- suppressMessages(as_forecast_binary(example_binary))\n  score_check <- expect_no_error(score(na.omit(check)))\n  expect_identical(score_check, suppressMessages(score(as_forecast_binary(example_binary))))\n})\n\n\n# ==============================================================================\n# is_forecast_binary() # nolint: commented_code_linter\n# ==============================================================================\ntest_that(\"is_forecast_binary() works as expected\", {\n  expect_true(is_forecast_binary(example_binary))\n  expect_false(is_forecast_binary(example_point))\n  expect_false(is_forecast_binary(example_quantile))\n  expect_false(is_forecast_binary(example_sample_continuous))\n  expect_false(is_forecast_binary(example_nominal))\n})\n\n\n# ==============================================================================\n# assert_forecast.forecast_binary() # nolint: commented_code_linter\n# ==============================================================================\ntest_that(\"assert_forecast.forecast_binary works as expected\", {\n  test <- na.omit(as.data.table(example_binary))\n  test[, \"sample_id\" := seq_len(nrow(test))]\n\n  # error if there is a superfluous sample_id column\n  expect_error(\n    as_forecast_binary(test),\n    paste(\n      \"Input looks like a binary forecast, but an additional column\",\n      \"called `sample_id` or `quantile` was found.\"\n    )\n  )\n\n  # expect error if probabilties are not in [0, 1]\n  test <- na.omit(as.data.table(example_binary))\n  test[, \"predicted\" := predicted + 1]\n  expect_error(\n    as_forecast_binary(test),\n    \"Input looks like a binary forecast, but found the following issue\"\n  )\n})\n\n\n\n# ==============================================================================","numLines":50,"startLine":1,"totalLines":135}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017XQPExFbuNSC5WgCwpuphS","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have a clear picture. Let me also look at `ensure_data.table` to understand the R method dispatch behavior with data.table (since data.table inherits from data.frame)."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1410,"cache_read_input_tokens":68722,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1410},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"a9ca7c70-2102-4868-86b4-005de9000aef"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017XQPExFbuNSC5WgCwpuphS","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01J59ZrZS9zpE4RLBsaFGLS3","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/check-input-helpers.R","limit":20}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1410,"cache_read_input_tokens":68722,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1410},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"3efe64c0-14dc-429a-a256-c9739892c0f3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01J59ZrZS9zpE4RLBsaFGLS3","type":"tool_result","content":"     1→#' Ensure that an object is a `data.table`\n     2→#' @description\n     3→#' This function ensures that an object is a `data table`.\n     4→#' If the object is not a data table, it is converted to one. If the object\n     5→#' is a data table, a copy of the object is returned.\n     6→#' @param data An object to ensure is a data table.\n     7→#' @returns A data.table/a copy of an existing data.table.\n     8→#' @keywords internal\n     9→#' @importFrom data.table copy is.data.table as.data.table\n    10→ensure_data.table <- function(data) {\n    11→  if (is.data.table(data)) {\n    12→    data <- copy(data)\n    13→  } else {\n    14→    data <- as.data.table(data)\n    15→  }\n    16→  return(data)\n    17→}\n    18→\n    19→\n    20→#' @title Check whether an input is an atomic vector of mode 'numeric'"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"9f8cae6e-6a21-4356-9e51-ab19060df03e","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/R/check-input-helpers.R","content":"#' Ensure that an object is a `data.table`\n#' @description\n#' This function ensures that an object is a `data table`.\n#' If the object is not a data table, it is converted to one. If the object\n#' is a data table, a copy of the object is returned.\n#' @param data An object to ensure is a data table.\n#' @returns A data.table/a copy of an existing data.table.\n#' @keywords internal\n#' @importFrom data.table copy is.data.table as.data.table\nensure_data.table <- function(data) {\n  if (is.data.table(data)) {\n    data <- copy(data)\n  } else {\n    data <- as.data.table(data)\n  }\n  return(data)\n}\n\n\n#' @title Check whether an input is an atomic vector of mode 'numeric'","numLines":20,"startLine":1,"totalLines":122}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01C9m1m37yxjM18o6mcjnuan","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01W3t8Q7LA1ALU3fHqjXV7B7","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"in_progress","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":423,"cache_read_input_tokens":70132,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":423},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"be02182b-2676-4df5-b19f-a1964d0710bb"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01W3t8Q7LA1ALU3fHqjXV7B7","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8c9c0fa9-0201-4a88-877d-780de4b4f34f","tool_use_result":{"oldTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"in_progress","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"pending","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"in_progress","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Ew1U7eUUDyca3S6i5hKur4","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me design the test cases. Key considerations:\n\n1. **R's S3 dispatch**: `data.table` inherits from `data.frame`, so when we rename `.default` to `.data.frame`, passing a `data.table` will still dispatch to `.data.frame` (since data.table's class is `c(\"data.table\", \"data.frame\")`). Tibbles have class `c(\"tbl_df\", \"tbl\", \"data.frame\")`, so they'll also dispatch to `.data.frame`.\n\n2. **The fix**: Rename `.default` → `.data.frame`, create new `.default` that errors.\n\n3. **Tests needed**:\n   - `.data.frame` method works with data.frame input\n   - `.data.frame` method works with data.table input (via S3 inheritance)\n   - `.data.frame` method works with tibble input (via S3 inheritance)\n   - `.default` method errors with helpful message for unsupported types (e.g., character vector, list, numeric, matrix)\n   - Existing behavior is preserved (score pipeline still works)\n\nNow let me write the test specs file."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":282,"cache_read_input_tokens":70555,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":282},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"4a13c029-1c59-4138-83f5-be9ffbcce3ea"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Ew1U7eUUDyca3S6i5hKur4","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01KYumK25V8mcHBBqX7KqAHm","name":"Glob","input":{"pattern":".ralph/issues/983-*.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":282,"cache_read_input_tokens":70555,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":282},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"3243f8ca-f951-4739-8050-881930174334"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01KYumK25V8mcHBBqX7KqAHm","type":"tool_result","content":"No files found"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"56e2df16-1305-4841-a9ae-24e8b259598d","tool_use_result":{"filenames":[],"durationMs":291,"numFiles":0,"truncated":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01XUG6d6nEKc4hEnu7kg3QJ2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GQc4xpSCZQXzrLwDDeWnGR","name":"Write","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/issues/983-as-forecast-data-frame-methods.md","content":"# Issue #983: Create `.data.frame` methods for `as_forecast<type>`\n\n## Source\n\n- **GitHub**: https://github.com/epiforecasts/scoringutils/issues/983\n- **Action Summary**: Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests.\n- **Status Note**: Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside.\n\n## Test Specifications\n\n**Designed**: 2026-02-13\n**Test file**: `tests/testthat/test-class-forecast.R` (existing) and individual `test-class-forecast-*.R` files (existing)\n**Testing patterns observed**: testthat with data.table fixtures; `example_*` datasets from package; `suppressMessages()` wrappers around `as_forecast_*()` calls; `expect_error()`, `expect_no_condition()`, `expect_identical()` used extensively; `setup.R` pre-computes score objects.\n\n### Important S3 dispatch note\n\n`data.table` has class `c(\"data.table\", \"data.frame\")` and tibble has class `c(\"tbl_df\", \"tbl\", \"data.frame\")`. When `.default` is renamed to `.data.frame`, R's S3 dispatch will still route data.table and tibble inputs to the `.data.frame` method because both inherit from `data.frame`. The new `.default` will only fire for types that do NOT inherit from `data.frame`.\n\n---\n\n### Test 1: test_that(\"as_forecast_binary.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Confirms that the renamed `.data.frame` method works with plain data.frame input (same behavior as the old `.default` method).\n**File**: `tests/testthat/test-class-forecast-binary.R`\n**Setup**:\n- Create a plain `data.frame` from `example_binary`: `df <- as.data.frame(na.omit(example_binary))`\n**Action**:\n- Call `as_forecast_binary(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_binary(df))` — no errors or warnings\n- `expect_s3_class(result, \"forecast_binary\")` — returns proper class\n- The result should be scoreable: `expect_no_error(score(result))`\n**Fails now because**:\n- This test should PASS with current code (`.default` handles it) and continue to PASS after the fix. It's a regression guard.\n\n### Test 2: test_that(\"as_forecast_quantile.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Confirms `.data.frame` method works for quantile forecasts with a data.frame (partially tested already at line 92-94 of test-class-forecast-quantile.R, but this is more thorough).\n**File**: `tests/testthat/test-class-forecast-quantile.R`\n**Setup**:\n- Use `example_quantile_df` (already defined in `setup.R` as `as.data.frame(na.omit(example_quantile))`)\n**Action**:\n- Call `as_forecast_quantile(example_quantile_df)`\n**Expected**:\n- `expect_no_condition(as_forecast_quantile(example_quantile_df))`\n- `expect_s3_class(result, \"forecast_quantile\")`\n**Fails now because**:\n- This test should PASS both before and after. It's a regression guard confirming the existing data.frame test (line 93) keeps working.\n\n### Test 3: test_that(\"as_forecast_sample.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies sample forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-sample.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_sample_continuous))`\n**Action**:\n- Call `as_forecast_sample(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_sample(df))`\n- `expect_s3_class(result, \"forecast_sample\")`\n- `expect_no_error(score(result))`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 4: test_that(\"as_forecast_point.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies point forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-point.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_point))`\n**Action**:\n- Call `as_forecast_point(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_point(df))`\n- `expect_s3_class(result, \"forecast_point\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 5: test_that(\"as_forecast_nominal.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies nominal forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-nominal.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_nominal))`\n**Action**:\n- Call `as_forecast_nominal(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_nominal(df))`\n- `expect_s3_class(result, \"forecast_nominal\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 6: test_that(\"as_forecast_ordinal.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies ordinal forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-ordinal.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_ordinal))`\n**Action**:\n- Call `as_forecast_ordinal(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_ordinal(df))`\n- `expect_s3_class(result, \"forecast_ordinal\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 7: test_that(\"as_forecast_multivariate_sample.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies multivariate sample forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-multivariate-sample.R`\n**Setup**:\n- Create a suitable data.frame from `example_sample_continuous` with a subset that has multiple locations:\n  ```\n  df <- as.data.frame(example_sample_continuous[\n    target_type == \"Cases\" &\n    forecast_date == \"2021-05-03\" &\n    target_end_date == \"2021-05-15\" &\n    horizon == 2 &\n    model == \"EuroCOVIDhub-ensemble\"\n  ])\n  ```\n**Action**:\n- Call `as_forecast_multivariate_sample(df, c(\"location\", \"location_name\"))`\n**Expected**:\n- `expect_no_condition(as_forecast_multivariate_sample(df, c(\"location\", \"location_name\")))`\n- `expect_s3_class(result, \"forecast_sample_multivariate\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 8: test_that(\"as_forecast_*.data.frame methods work with data.table input via S3 dispatch\", { ... })\n\n**Reproduces**: Confirms that data.table input still dispatches correctly to the `.data.frame` method after the rename (since data.table inherits from data.frame).\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Use the pre-existing `example_*` objects which are all data.tables\n**Action**:\n- Call each `as_forecast_*()` with its corresponding example dataset (which is a data.table)\n**Expected**:\n- All 7 calls succeed without error, produce the correct forecast class\n- Specifically: `expect_s3_class(as_forecast_binary(example_binary), \"forecast_binary\")` and similarly for the other 6 types\n**Fails now because**:\n- Should PASS both before and after. Critical regression test ensuring data.table dispatch still works.\n\n### Test 9: test_that(\"as_forecast_*.default errors with helpful message for non-data.frame input\", { ... })\n\n**Reproduces**: The core bug — currently `.default` silently accepts any type. After the fix, `.default` should error for non-data.frame types.\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Create several non-data.frame objects: `vec <- 1:10`, `mat <- matrix(1:12, nrow = 3)`, `lst <- list(a = 1, b = 2)`, `chr <- \"not a data.frame\"`\n**Action**:\n- Call `as_forecast_binary(vec)`, `as_forecast_binary(mat)`, `as_forecast_binary(lst)`, `as_forecast_binary(chr)`\n- Also test at least one other type to confirm pattern: `as_forecast_sample(vec)`, `as_forecast_quantile(mat)`\n**Expected**:\n- Each call should produce an error with `expect_error()`\n- Error message should mention that input must be a data.frame-like object: `expect_error(as_forecast_binary(vec), \"data.frame\")` (match on \"data.frame\" in the error message)\n- Error message should indicate the type that was actually passed: the error should contain class info about the unsupported type\n**Fails now because**:\n- Currently `.default` accepts any input and tries `as.data.table()` coercion. For some inputs (like numeric vectors), this silently succeeds but produces nonsensical results. For others it produces a confusing error deep inside validation. After the fix, `.default` will immediately error with a clear message.\n\n### Test 10: test_that(\"as_forecast_*.default errors for matrix input\", { ... })\n\n**Reproduces**: Edge case — matrix does NOT inherit from data.frame, so it should hit the new `.default` error.\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- `mat <- matrix(1:20, nrow = 4, dimnames = list(NULL, c(\"observed\", \"predicted\", \"model\", \"date\", \"sample_id\")))`\n**Action**:\n- Call `as_forecast_sample(mat)`\n**Expected**:\n- `expect_error(as_forecast_sample(mat), \"data.frame\")` — should error with clear message rather than silently coercing\n**Fails now because**:\n- Currently `as.data.table(mat)` succeeds and coerces the matrix, but the intent is to only accept data.frame-like objects.\n\n### Test 11: test_that(\"as_forecast_*.data.frame methods preserve column renaming functionality\", { ... })\n\n**Reproduces**: Ensures the column renaming feature (e.g., `observed = \"obs\"`) works correctly after the method rename.\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Create a data.frame with non-standard column names:\n  ```\n  df <- data.frame(\n    model = rep(\"m1\", 10),\n    date = as.Date(\"2020-01-01\") + 1:10,\n    obs = 1:10,\n    pred = rnorm(10),\n    sid = rep(1:2, each = 5)\n  )\n  ```\n**Action**:\n- Call `as_forecast_sample(df, observed = \"obs\", predicted = \"pred\", sample_id = \"sid\")`\n**Expected**:\n- `expect_no_error(result)` — method handles column renaming correctly\n- `expect_true(\"observed\" %in% colnames(result))` — column was renamed\n- `expect_true(\"predicted\" %in% colnames(result))` — column was renamed\n- `expect_true(\"sample_id\" %in% colnames(result))` — column was renamed\n**Fails now because**:\n- Should PASS both before and after. Guards that the renaming feature works after the method rename.\n\n### Test 12: test_that(\"as_forecast_*.data.frame methods work with tibble input\", { ... })\n\n**Reproduces**: Confirms tibble input dispatches to `.data.frame` method correctly (tibble inherits from data.frame).\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Requires `tibble` package (check with `skip_if_not_installed(\"tibble\")`)\n- Create a tibble: `tbl <- tibble::as_tibble(na.omit(as.data.frame(example_binary)))`\n**Action**:\n- Call `as_forecast_binary(tbl)`\n**Expected**:\n- `expect_no_error(as_forecast_binary(tbl))`\n- `expect_s3_class(result, \"forecast_binary\")`\n**Fails now because**:\n- Should PASS both before and after. But it's an important edge case because tibble is a common input type and has class `c(\"tbl_df\", \"tbl\", \"data.frame\")`.\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":342,"cache_read_input_tokens":70837,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":342},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"1a52ec48-257d-4d22-bf49-5ae8c5a2e063"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GQc4xpSCZQXzrLwDDeWnGR","type":"tool_result","content":"File created successfully at: /Users/nikos/github-synced/scoringutils-pipeline/.ralph/issues/983-as-forecast-data-frame-methods.md"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8afeaa35-d48b-4f9f-a30b-2955948a4339","tool_use_result":{"type":"create","filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/issues/983-as-forecast-data-frame-methods.md","content":"# Issue #983: Create `.data.frame` methods for `as_forecast<type>`\n\n## Source\n\n- **GitHub**: https://github.com/epiforecasts/scoringutils/issues/983\n- **Action Summary**: Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests.\n- **Status Note**: Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside.\n\n## Test Specifications\n\n**Designed**: 2026-02-13\n**Test file**: `tests/testthat/test-class-forecast.R` (existing) and individual `test-class-forecast-*.R` files (existing)\n**Testing patterns observed**: testthat with data.table fixtures; `example_*` datasets from package; `suppressMessages()` wrappers around `as_forecast_*()` calls; `expect_error()`, `expect_no_condition()`, `expect_identical()` used extensively; `setup.R` pre-computes score objects.\n\n### Important S3 dispatch note\n\n`data.table` has class `c(\"data.table\", \"data.frame\")` and tibble has class `c(\"tbl_df\", \"tbl\", \"data.frame\")`. When `.default` is renamed to `.data.frame`, R's S3 dispatch will still route data.table and tibble inputs to the `.data.frame` method because both inherit from `data.frame`. The new `.default` will only fire for types that do NOT inherit from `data.frame`.\n\n---\n\n### Test 1: test_that(\"as_forecast_binary.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Confirms that the renamed `.data.frame` method works with plain data.frame input (same behavior as the old `.default` method).\n**File**: `tests/testthat/test-class-forecast-binary.R`\n**Setup**:\n- Create a plain `data.frame` from `example_binary`: `df <- as.data.frame(na.omit(example_binary))`\n**Action**:\n- Call `as_forecast_binary(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_binary(df))` — no errors or warnings\n- `expect_s3_class(result, \"forecast_binary\")` — returns proper class\n- The result should be scoreable: `expect_no_error(score(result))`\n**Fails now because**:\n- This test should PASS with current code (`.default` handles it) and continue to PASS after the fix. It's a regression guard.\n\n### Test 2: test_that(\"as_forecast_quantile.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Confirms `.data.frame` method works for quantile forecasts with a data.frame (partially tested already at line 92-94 of test-class-forecast-quantile.R, but this is more thorough).\n**File**: `tests/testthat/test-class-forecast-quantile.R`\n**Setup**:\n- Use `example_quantile_df` (already defined in `setup.R` as `as.data.frame(na.omit(example_quantile))`)\n**Action**:\n- Call `as_forecast_quantile(example_quantile_df)`\n**Expected**:\n- `expect_no_condition(as_forecast_quantile(example_quantile_df))`\n- `expect_s3_class(result, \"forecast_quantile\")`\n**Fails now because**:\n- This test should PASS both before and after. It's a regression guard confirming the existing data.frame test (line 93) keeps working.\n\n### Test 3: test_that(\"as_forecast_sample.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies sample forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-sample.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_sample_continuous))`\n**Action**:\n- Call `as_forecast_sample(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_sample(df))`\n- `expect_s3_class(result, \"forecast_sample\")`\n- `expect_no_error(score(result))`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 4: test_that(\"as_forecast_point.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies point forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-point.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_point))`\n**Action**:\n- Call `as_forecast_point(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_point(df))`\n- `expect_s3_class(result, \"forecast_point\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 5: test_that(\"as_forecast_nominal.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies nominal forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-nominal.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_nominal))`\n**Action**:\n- Call `as_forecast_nominal(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_nominal(df))`\n- `expect_s3_class(result, \"forecast_nominal\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 6: test_that(\"as_forecast_ordinal.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies ordinal forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-ordinal.R`\n**Setup**:\n- `df <- as.data.frame(na.omit(example_ordinal))`\n**Action**:\n- Call `as_forecast_ordinal(df)`\n**Expected**:\n- `expect_no_condition(as_forecast_ordinal(df))`\n- `expect_s3_class(result, \"forecast_ordinal\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 7: test_that(\"as_forecast_multivariate_sample.data.frame works with a plain data.frame\", { ... })\n\n**Reproduces**: Verifies multivariate sample forecast creation from a plain data.frame.\n**File**: `tests/testthat/test-class-forecast-multivariate-sample.R`\n**Setup**:\n- Create a suitable data.frame from `example_sample_continuous` with a subset that has multiple locations:\n  ```\n  df <- as.data.frame(example_sample_continuous[\n    target_type == \"Cases\" &\n    forecast_date == \"2021-05-03\" &\n    target_end_date == \"2021-05-15\" &\n    horizon == 2 &\n    model == \"EuroCOVIDhub-ensemble\"\n  ])\n  ```\n**Action**:\n- Call `as_forecast_multivariate_sample(df, c(\"location\", \"location_name\"))`\n**Expected**:\n- `expect_no_condition(as_forecast_multivariate_sample(df, c(\"location\", \"location_name\")))`\n- `expect_s3_class(result, \"forecast_sample_multivariate\")`\n**Fails now because**:\n- Should PASS both before and after. Regression guard.\n\n### Test 8: test_that(\"as_forecast_*.data.frame methods work with data.table input via S3 dispatch\", { ... })\n\n**Reproduces**: Confirms that data.table input still dispatches correctly to the `.data.frame` method after the rename (since data.table inherits from data.frame).\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Use the pre-existing `example_*` objects which are all data.tables\n**Action**:\n- Call each `as_forecast_*()` with its corresponding example dataset (which is a data.table)\n**Expected**:\n- All 7 calls succeed without error, produce the correct forecast class\n- Specifically: `expect_s3_class(as_forecast_binary(example_binary), \"forecast_binary\")` and similarly for the other 6 types\n**Fails now because**:\n- Should PASS both before and after. Critical regression test ensuring data.table dispatch still works.\n\n### Test 9: test_that(\"as_forecast_*.default errors with helpful message for non-data.frame input\", { ... })\n\n**Reproduces**: The core bug — currently `.default` silently accepts any type. After the fix, `.default` should error for non-data.frame types.\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Create several non-data.frame objects: `vec <- 1:10`, `mat <- matrix(1:12, nrow = 3)`, `lst <- list(a = 1, b = 2)`, `chr <- \"not a data.frame\"`\n**Action**:\n- Call `as_forecast_binary(vec)`, `as_forecast_binary(mat)`, `as_forecast_binary(lst)`, `as_forecast_binary(chr)`\n- Also test at least one other type to confirm pattern: `as_forecast_sample(vec)`, `as_forecast_quantile(mat)`\n**Expected**:\n- Each call should produce an error with `expect_error()`\n- Error message should mention that input must be a data.frame-like object: `expect_error(as_forecast_binary(vec), \"data.frame\")` (match on \"data.frame\" in the error message)\n- Error message should indicate the type that was actually passed: the error should contain class info about the unsupported type\n**Fails now because**:\n- Currently `.default` accepts any input and tries `as.data.table()` coercion. For some inputs (like numeric vectors), this silently succeeds but produces nonsensical results. For others it produces a confusing error deep inside validation. After the fix, `.default` will immediately error with a clear message.\n\n### Test 10: test_that(\"as_forecast_*.default errors for matrix input\", { ... })\n\n**Reproduces**: Edge case — matrix does NOT inherit from data.frame, so it should hit the new `.default` error.\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- `mat <- matrix(1:20, nrow = 4, dimnames = list(NULL, c(\"observed\", \"predicted\", \"model\", \"date\", \"sample_id\")))`\n**Action**:\n- Call `as_forecast_sample(mat)`\n**Expected**:\n- `expect_error(as_forecast_sample(mat), \"data.frame\")` — should error with clear message rather than silently coercing\n**Fails now because**:\n- Currently `as.data.table(mat)` succeeds and coerces the matrix, but the intent is to only accept data.frame-like objects.\n\n### Test 11: test_that(\"as_forecast_*.data.frame methods preserve column renaming functionality\", { ... })\n\n**Reproduces**: Ensures the column renaming feature (e.g., `observed = \"obs\"`) works correctly after the method rename.\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Create a data.frame with non-standard column names:\n  ```\n  df <- data.frame(\n    model = rep(\"m1\", 10),\n    date = as.Date(\"2020-01-01\") + 1:10,\n    obs = 1:10,\n    pred = rnorm(10),\n    sid = rep(1:2, each = 5)\n  )\n  ```\n**Action**:\n- Call `as_forecast_sample(df, observed = \"obs\", predicted = \"pred\", sample_id = \"sid\")`\n**Expected**:\n- `expect_no_error(result)` — method handles column renaming correctly\n- `expect_true(\"observed\" %in% colnames(result))` — column was renamed\n- `expect_true(\"predicted\" %in% colnames(result))` — column was renamed\n- `expect_true(\"sample_id\" %in% colnames(result))` — column was renamed\n**Fails now because**:\n- Should PASS both before and after. Guards that the renaming feature works after the method rename.\n\n### Test 12: test_that(\"as_forecast_*.data.frame methods work with tibble input\", { ... })\n\n**Reproduces**: Confirms tibble input dispatches to `.data.frame` method correctly (tibble inherits from data.frame).\n**File**: `tests/testthat/test-class-forecast.R`\n**Setup**:\n- Requires `tibble` package (check with `skip_if_not_installed(\"tibble\")`)\n- Create a tibble: `tbl <- tibble::as_tibble(na.omit(as.data.frame(example_binary)))`\n**Action**:\n- Call `as_forecast_binary(tbl)`\n**Expected**:\n- `expect_no_error(as_forecast_binary(tbl))`\n- `expect_s3_class(result, \"forecast_binary\")`\n**Fails now because**:\n- Should PASS both before and after. But it's an important edge case because tibble is a common input type and has class `c(\"tbl_df\", \"tbl\", \"data.frame\")`.\n","structuredPatch":[],"originalFile":null}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_0173ThtaAzjA2TDUN5bTHM3g","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BeVfchgTqAUy41vcD2gAFN","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"in_progress","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3612,"cache_read_input_tokens":71179,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3612},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"3662dedc-e368-4277-ba9b-57497fed5f7e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BeVfchgTqAUy41vcD2gAFN","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"78c5b28e-18eb-4084-bcf1-304f6a707295","tool_use_result":{"oldTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"in_progress","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"pending","activeForm":"Updating the triage table"}],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"in_progress","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01NCZU6GTia8DtT9LaRwcakJ","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me update the triage table. I need to change Has Tests from `[ ]` to `[x]` for issue #983."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":282,"cache_read_input_tokens":74791,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":282},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"e914af0f-0045-4011-8697-a24cc62e7c61"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01NCZU6GTia8DtT9LaRwcakJ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BEm6rGhQuiudauSk8yaTSu","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","offset":35,"limit":1}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":282,"cache_read_input_tokens":74791,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":282},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"f6f54b58-28d4-46bf-a8fd-49557ddb9efc"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BEm6rGhQuiudauSk8yaTSu","type":"tool_result","content":"    35→| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"3d7ff891-5d15-42f7-9169-baaa0c2e47e0","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","content":"| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |","numLines":1,"startLine":35,"totalLines":131}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01JPrNi2bHKXuYdoCWnGUfF6","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01KPt5kSH2Ce1FzwkHJsK84n","name":"Edit","input":{"replace_all":false,"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","old_string":"| Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |","new_string":"| Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":357,"cache_read_input_tokens":75073,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":357},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"1e010986-113c-4839-82cd-13d96377fac7"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01KPt5kSH2Ce1FzwkHJsK84n","type":"tool_result","content":"The file /Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"57b54abc-71cf-48f7-a233-45c8930ffb04","tool_use_result":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","oldString":"| Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |","newString":"| Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |","originalFile":"# scoringutils Issue Triage\n\n> Auto-generated by Ralph triage loop. Do not edit manually during the loop.\n\n## Summary\n\n| Metric | Count |\n|--------|-------|\n| Total open issues | 106 |\n| Triaged | 106 |\n| Remaining | 0 |\n\n## Pipeline\n\n- [ ] **Test design** — Test specs written for automatable issues\n- [ ] **Implement** — Fixes implemented, PRs created\n- [ ] **Review** — All PRs independently reviewed\n- [ ] **Summary** — Human summary generated\n\n## Issues\n\n| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n|---|-------|--------|--------|----------|----------|------------|-------------|-------------|--------------|----------------|-----------|-------------|-----|----------|\n| 1065 | Release reminder - February 2026 | | 2026-02-02 | meta | medium | medium | partially | Automated reminder. Dev version (2.1.2.9000) has multivariate feature + bug fix accumulated over 161 days. No blocking bugs. Two open PRs (#1060, #1062) are infrastructure changes. | Maintainers should decide whether to merge #1060/#1062 before releasing. | Decide whether to cut a new release (likely v2.2.0 given multivariate feature). If yes, run `usethis::use_release_issue()` and proceed with CRAN submission. If not, close the issue. | N/A | N/A | — | N/A |\n| 1064 | Pre-ranking for multivariate calibration assessment | | 2026-01-30 | feature | medium | large | no | No pre-ranking or multivariate calibration diagnostics exist in the codebase. The `forecast_sample_multivariate` class and energy score are implemented and stable. Design decision needed: integrate into `score()` (Option A) or standalone `get_multivariate_ranks()` function (Option B). MST method may need `igraph` dependency. | None blocking. Builds on existing `forecast_sample_multivariate` infrastructure. | Implement pre-ranking methods (multivariate rank, optionally MST rank) from Gneiting et al. (2008) as calibration diagnostics for multivariate forecasts, after resolving the API design choice with maintainers. | N/A | N/A | — | N/A |\n| 1057 | Revise README to remove news about the package rewrite | documentation | 2026-01-29 | documentation | low | medium | partially | Open, no assignee. The rewrite messaging and Google Drive paper links appear in 4 locations in README.Rmd. Mechanical edits are straightforward but replacement text requires editorial decisions about where to point users for documentation. | Loosely related to #810 (pkgdown for dev docs) and #759 (future of scoringutils.Rmd vignette). | Edit `README.Rmd` to remove/soften the \"major re-write\" paragraph and replace Google Drive paper links with pointers to package vignettes/docs, then re-render README.md. Confirm preferred documentation hierarchy with maintainers first. | N/A | N/A | — | N/A |\n| 1055 | Model confidence sets | feature request | 2026-01-19 | feature | medium | large | no | Design phase — methodology not yet agreed upon. 6 comments with active debate. Naive pairwise p-value approach acknowledged as incomplete. Three alternatives discussed: Hansen et al. MCS, novel extension for incomplete sets, Bayesian approach. No consensus reached. | Relates to #750 (p-value computation refactor), #769 (pairwise comparison arguments), #37 (Diebold-Mariano test), #1022 (two-model pairwise). | Resolve design consensus on which MCS methodology to implement (Hansen et al. on complete sets, novel incomplete-set extension, or Bayesian), then implement `get_model_confidence_set()` building on existing pairwise comparison infrastructure. | N/A | N/A | — | N/A |\n| 1046 | multivariate scoring in scoring rules vignette | documentation | 2025-11-04 | documentation | low | small | partially | Open, agreed upon by maintainers. The scoring-rules vignette has zero mentions of multivariate scoring. A dedicated multivariate vignette already exists. Only a cross-reference and brief section are missing. | None. All underlying code and the dedicated multivariate vignette already exist. | Add a short section to `vignettes/scoring-rules.Rmd` describing the energy score for multivariate forecasts and cross-linking to the existing `scoring-multivariate-forecasts` vignette. No code changes needed. | N/A | N/A | — | N/A |\n| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |\n| 1043 | Update print method for multivariate forecasts to show `joint_across` | | 2025-09-21 | enhancement | low | small | yes | Still relevant. No `print.forecast_sample_multivariate` method exists; the generic `print.forecast` handles all types. `joint_across` is not stored as an attribute — `set_grouping()` computes `.mv_group_id` but discards the original `joint_across` value. | Builds on multivariate infrastructure from merged PR #998. | Add a `print.forecast_sample_multivariate()` method that displays `joint_across` columns. Requires either storing `joint_across` as an attribute during object construction or computing it as the set difference between forecast unit and `get_grouping()` output. Update test snapshots. | [x] | [ ] | — | [ ] |\n| 1026 | as_forecast_sample.forecast_quantile | enhancement | 2025-07-13 | enhancement | medium | medium | partially | Still relevant. The reverse conversion (`as_forecast_quantile.forecast_sample`) exists, but `as_forecast_sample.forecast_quantile` does not. No distribution-fitting dependencies in DESCRIPTION. Author plans to borrow from hubverse approach. | Related to #557 (sample_to_quantile.numeric). May introduce new dependency (e.g., `distfromq`). | Implement `as_forecast_sample.forecast_quantile()` S3 method that fits a distribution or uses interpolation to approximate the predictive CDF from quantiles and draws samples. Requires design decision on approximation method and potential new package dependency. | N/A | N/A | — | N/A |\n| 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |\n| 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |\n| 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |\n| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [ ] | [ ] | — | [ ] |\n| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [ ] | [ ] | — | [ ] |\n| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [ ] | [ ] | — | [ ] |\n| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [ ] | [ ] | — | [ ] |\n| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n| 880 | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()? | | 2024-07-30 | enhancement | low | small | yes | Still relevant. Function hard-codes `0.5` on two lines (107, 115) in `R/class-forecast-quantile.R`. Design decisions already resolved in PR #876 discussion: parameter name `quantile_level`, default `0.5`, passed via `...`. No work done since issue opened. | None. Self-contained enhancement. | Add a `quantile_level` parameter (default `0.5`) to `as_forecast_point.forecast_quantile()`, replacing the two hard-coded `0.5` references, then add tests and documentation. | [ ] | [ ] | — | [ ] |\n| 872 | Add a reference for CRPS decomposition approach. | | 2024-07-25 | documentation | low | small | partially | Still relevant. `crps_sample()` in `R/metrics-sample.R` has only a `scoringRules` package reference, not one for the decomposition approach. `wis()` already properly cites Bracher et al. 2021. Minor decision needed on which paper(s) to cite: Bracher et al. 2021 and/or Arnold et al. 2023. | Related to #870 (CRPS/WIS decomposition refactor). Could be addressed in same PR. | Add `@references` entries to `crps_sample()` roxygen block citing paper(s) formalizing the overprediction/underprediction/dispersion decomposition (likely Bracher et al. 2021 and/or Arnold et al. 2023). | N/A | N/A | — | N/A |\n| 870 | Refactor CRPS and WIS decomposition | refactor | 2024-07-22 | refactor | low | medium | partially | Partially addressed. PR #985 (Feb 2025) simplified `crps_sample()` decomposition internally, but the structural duplication remains: `crps_sample()`, `wis()`, and `interval_score()` each independently implement `separate_results` branching, and 6 wrapper functions (3 sample + 3 quantile) repeat identical extraction patterns. | Related to #872 (CRPS decomposition reference). Filed during PR #854 review. On `scoringutils-2.x` milestone. | Extract the decomposition logic (`separate_results` branching and wrapper function boilerplate) into a shared internal helper or factory pattern so `crps_sample()`, `wis()`, and `interval_score()` delegate to the same mechanism instead of each implementing it independently. | N/A | N/A | — | N/A |\n| 852 | Design workflow for creating new forecast classes | enhancement | 2024-07-10 | enhancement | high | large | no | Still relevant. The building blocks exist (`new_forecast()` exported, `as_forecast_generic()` internal, pattern visible in 7 class files) but no documentation, vignette, or guide for third-party class creation exists. Three prerequisite S3 refactors (#889, #888, #740) remain open and block clean extensibility because `get_protected_columns()` and `get_duplicate_forecasts()` hard-code column names, preventing new classes from working seamlessly. `as_forecast_generic()`, `apply_metrics()`, and `as_scores()` are still unexported (#885). | #889 (S3 `get_protected_columns`), #888 (S3 `get_duplicate_forecasts`), #740 (`get_forecast_unit` customization), #885 (export decision for helper functions). | Complete prerequisite S3 refactors (#889, #888), decide export list for class-creation helpers (#885), then write a vignette or article documenting the step-by-step workflow for creating a custom `forecast_*` class (define `as_forecast_<type>`, `assert_forecast.<type>`, `score.<type>`, `get_metrics.<type>`, `get_protected_columns.<type>`, `get_duplicate_forecasts.<type>`). | N/A | N/A | — | N/A |\n| 840 | Clean up input checks | | 2024-06-07 | refactor | low | medium | partially | Still relevant. Input checks are applied inconsistently: `assert_forecast.forecast_binary` and `assert_forecast.forecast_point` call `check_input_*` wrappers (double-validating with scoring functions), while sample/quantile/ordinal/nominal types skip this. `check-input-helpers.R` has a comment marking `check_try()` and wrappers for future deletion. Author leans toward pushing all checks to the scoring-function level. No work done. | Related to #939 (remove `check_input_*` functions), #839, #922. On `scoringutils-2.x` milestone. | Decide where input validation should live (scoring-function level vs. `assert_forecast` level vs. both-consistently). Then either remove `check_input_binary`/`check_input_point` calls from `assert_forecast` methods and delete wrappers + `check_try()`, or add missing checks to sample/quantile `assert_forecast` methods for consistency. | N/A | N/A | — | N/A |\n| 839 | Remove `test_columns_not_present` | | 2024-06-07 | cleanup | low | small | yes | Still relevant. `test_columns_not_present()` is a trivial 1-line wrapper around `checkmate::test_names(..., disjunct.from = ...)` with exactly 1 production call site (`R/class-forecast-binary.R:68`) and 1 test block. `checkmate::test_names` is already imported. Straightforward removal. | Related to #922 (remove `check_columns_present`) and #840 (clean up input checks). Can be batched but has no blocking dependency. | Replace the single call in `R/class-forecast-binary.R:68` with `test_names(colnames(forecast), disjunct.from = ...)`, delete the function definition in `R/check-input-helpers.R`, delete the test block, and remove the generated man page. Optionally bundle with #922. | [ ] | [ ] | — | [ ] |\n| 826 | Update `plot_heatmap()` to be a table instead of a plot | | 2024-05-19 | enhancement | low | medium | partially | Still relevant. Accessibility concern from @Bisaloo is valid: current ggplot2 heatmap renders values as image text, inaccessible to screen readers. Conversation stalled — Bisaloo never replied to follow-up about output format (HTML/LaTeX). `kableExtra` is already in `Suggests`. This is a breaking change: function returns `ggplot` object, used with `+ facet_wrap()` in `inst/manuscript/`. Not used in any current vignette. Design decision needed on migration strategy. | None blocking. Thematically adjacent to #713 (visualization vignette). | Resolve stalled design conversation on output format and migration strategy (deprecate + new function vs. breaking change). Implement colored-table version using `kableExtra` (already in Suggests) or `gt` (new dep), update manuscript, replace test/snapshot, and deprecate or replace `plot_heatmap()`. | N/A | N/A | — | N/A |\n| 825 | Package scaffolding | | 2024-05-19 | refactor | low | small | yes | Still relevant. Neither recommendation acted upon: (1) `Metrics` package still imported for 3 trivial functions (`ae`, `se`, `ape`) used only in `R/class-forecast-point.R` — these are one-line base-R replacements. `Metrics` did receive a 2025 CRAN update, reducing urgency. (2) `matrixStats` not adopted; 3 `apply()` calls in `R/metrics-sample.R` could benefit. Suggest splitting into two sub-tasks, prioritizing `Metrics` removal as a quick win. | None blocking. Removing `Metrics` requires updating vignettes (`scoring-rules.Rmd`) and tests (`test-metrics-binary.R`) that reference `Metrics::` functions. | (1) Replace `Metrics::ae/se/ape` with internal one-liners in `R/class-forecast-point.R`, remove from DESCRIPTION/NAMESPACE, update vignettes and tests. (2) Optionally evaluate adding `matrixStats` for 3 `apply()` calls — lower value, separate decision. | [ ] | [ ] | — | [ ] |\n| 823 | Reduce number of `try()`, `suppressMessages()`, `suppressWarnings()` used in the package | | 2024-05-19 | refactor | medium | medium | no | Still relevant. 8 direct `try()` calls + 6 indirect via `check_try()` + 1 `suppressWarnings(suppressMessages(...))` in R/ code. Usage has slightly increased since filing due to new `[.forecast` subsetting operators (PR #884). Each `try()` exists for a reason; removal requires architectural rethinking of validation flow, not simple find-and-replace. | Deeply linked to #935 (print on column change), #840 (input check cleanup), #939 (remove `check_input_*`), #885 (export decisions), #811 (batch warnings). | Address incrementally: (1) Replace blanket `try()` in subsetting operators with targeted `tryCatch()`, (2) add `verbose` params to internal validation paths to eliminate `suppress*()`, (3) replace `check_try()` with proper `check_*()` functions as part of #840. | N/A | N/A | — | N/A |\n| 811 | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? | enhancement | 2024-05-07 | enhancement | low | small | yes | Still relevant. `run_safely()` in `R/score.R:210` uses `try()` + immediate `cli_warn()` per failed metric. No batching/deferring mechanism exists. Failed metrics silently dropped from `scores` object via `as_scores()` filtering. Blog post on deferred errors referenced in issue. Only 1 comment from maintainer (\"I like this idea\"). | None blocking. Tangentially related to #823 (reduce `try()`) and #382 (metric/column name clash warnings). | Modify `apply_metrics()`/`run_safely()` to collect error messages in a list during the `lapply` loop, then emit a single batched `cli_warn()` summarizing all failed metrics at the end. Update test expectations in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 810 | Clean up readme by using `pkgdown` to render dev version docs | | 2024-05-05 | documentation | low | small | yes | CLOSED (completed 2026-02-09). PR #1060 merged today, replacing R-based `packageVersion()` detection with pkgdown selective HTML (`.pkgdown-devel`/`.pkgdown-release` fenced divs). `_pkgdown.yml` already had `development: mode: auto`. Dev docs deployed at `/dev/` on gh-pages. | None. Fully resolved. | No action needed — issue closed via merged PR #1060. | N/A | N/A | — | N/A |\n| 808 | Run lintr everywhere | | 2024-05-05 | cleanup | low | medium | partially | Active work in progress. PR #1062 (\"enable lintr for tests, inst, and vignettes\") is open but not yet merged. `.lintr` excludes all `tests/`, `inst/`, and `vignettes/*.R` files. CI only lints changed files on PRs. Main scope is `tests/` (~5,400 lines across 35 files). `inst/` has only 2 R files. `vignettes/*.R` not tracked in git (auto-generated tangles). ~90 `nolint` annotations already exist in `R/` source. | #807 (cli nolint cleanup) should be done first to establish pattern. #513 (vignette .R scripts) related to vignette exclusion. PR #1062 is in flight. | Merge or rebase PR #1062 once reviews pass. May need to fix remaining lint violations or add targeted `# nolint` annotations. Address `tests/` directory-wide lint cleanup separately from `inst/` and `vignettes/`. | N/A | N/A | — | N/A |\n| 807 | Clean up linter statements around `cli` conditions | good first issue, implementation-ready | 2024-05-05 | cleanup | low | medium | yes | Still relevant. ~43 `#nolint start: keyword_quote_linter` blocks across R/ wrap `cli_abort()`/`cli_warn()` calls that use `\"!\" =`, `\"i\" =` named vector syntax. Only 12 instances use Hugo's backtick fix (`\\`!\\` =`). The fix is mechanical: replace `\"!\" =` with `` `!` = `` inside cli condition calls, then remove the `nolint` annotations. Additional `object_usage_linter` suppressions (14 blocks) exist for `.data[...]` patterns in data.table code — these need separate handling. No work done on this issue since filing. | Prerequisite for #808 (run lintr everywhere). Related to PR #583 (which introduced the cli pattern). | Replace all `\"!\" =`, `\"i\" =`, `\"x\" =`, `\"v\" =` with backtick-quoted equivalents (`` `!` = ``, etc.) in `cli_abort()`/`cli_warn()` calls across ~20 R/ files, then delete the corresponding `#nolint start`/`#nolint end` blocks. Handle `object_usage_linter` suppressions separately. | [ ] | [ ] | — | [ ] |\n| 793 | How should we document methods that have different arguments than the default method? | | 2024-04-19 | documentation | low | small | yes | Still relevant. Consensus reached (separate pages per @seabbs), but not implemented. Three generics currently lump methods with differing arguments onto one `@rdname` page: `as_forecast_quantile` (`.forecast_sample` has different args), `as_forecast_point` (`.forecast_quantile`), and `get_pit_histogram` (`.forecast_sample` has extra args). Renders a confusing flat argument list. | None. | Remove shared `@rdname` tags for S3 methods whose arguments differ from the generic (`as_forecast_quantile.forecast_sample`, `as_forecast_point.forecast_quantile`, `get_pit_histogram.forecast_sample`) and give each its own documentation page with proper title and description. | [ ] | [ ] | — | [ ] |\n| 789 | Add documentation about how to customise scores | documentation | 2024-04-13 | documentation | medium | medium | partially | Partially addressed. The `score()` roxygen docs now have a \"Customising metrics\" section with `purrr::partial()` examples (PR #874). But the core ask — a vignette walkthrough — remains undone. No vignette covers `get_metrics()` → `select_metrics()` → `purrr::partial()` → `score()` workflow. On `scoringutils-2.x` milestone. | Related to #546 (vignette restructuring — decides where customization docs go), #666 (manuscript update). | Create a vignette section (or standalone vignette) walking users through the full metric customization workflow: `get_metrics()`, `select_metrics()`, `purrr::partial()`, custom metric functions, and passing to `score()`. Coordinate with vignette restructuring in #546. | N/A | N/A | — | N/A |\n| 785 | Small updates to the scoring rules Vignette | documentation | 2024-04-10 | documentation | low | small | yes | Still relevant. None of the 7 suggestions from @seabbs's review of PR #762 have been implemented: author field still present, Google Drive link still used, no citations for proper scoring rules or transformed scores paper, `get_metrics` wording not improved for website readers, no introductory paragraph on different probabilistic forecast types. Nearly 2 years open with no progress on these items. | Loosely related to #666 (manuscript update re: Google Drive link), #758 (scoring rules vignette parent issue), #1046 (multivariate scoring in same vignette). | Implement the 7 small documentation edits in `vignettes/scoring-rules.Rmd`: remove author field, replace Google Drive link with proper reference, add citations for proper scoring rules and transformed scores, improve `get_metrics` wording, clean whitespace, and add intro paragraph on probabilistic forecast types. | [ ] | [ ] | — | [ ] |\n| 780 | Add a `verbose` argument to `as_forecast()` | | 2024-04-05 | enhancement | low | small | yes | Still relevant. `verbose` already exists on `assert_forecast()` and `assert_forecast_generic()` (PR #778) but has not been threaded through to any `as_forecast_<type>()` constructor. ~11 test lines and 1 internal R source line use `suppressMessages`/`suppressWarnings` wrappers around `as_forecast_*` calls, confirming ongoing need. Author acknowledged low urgency but wants it for test cleanup. | None. PR #778 (verbose on `assert_forecast`) already merged; plumbing is in place. | Add `verbose = TRUE` parameter to each `as_forecast_<type>.default()` method, pass it to their `assert_forecast()` calls, guard direct `cli_warn()`/`cli_inform()` calls behind the flag, and replace `suppressMessages`/`suppressWarnings` wrappers in tests and internal code with `verbose = FALSE`. | [ ] | [ ] | — | [ ] |\n| 769 | Rethink the way arguments are passed in `get_pairwise_comparisons()` | package improvement | 2024-04-03 | refactor | low | small | yes | Still relevant. `test_type`, `one_sided`, and `n_permutations` are still passed via `...` through 3 levels (`get_pairwise_comparisons` → `pairwise_comparison_one_group` → `compare_forecasts`). Users must read internal `compare_forecasts` docs to discover options. PR #994 added `test_type = NULL` but didn't address the `...` pattern. #750 (split out p-values) was deferred. | Loosely related to #750 (if p-values split out, this becomes moot — but #750 deferred indefinitely). No hard blockers. | Promote `test_type`, `one_sided`, and `n_permutations` from `...` to explicit named arguments in `get_pairwise_comparisons()` and `add_relative_skill()`, improving API discoverability. Update roxygen `@param` tags accordingly. | [ ] | [ ] | — | [ ] |\n| 763 | We should check for consistency when scoring binary forecasts | feature request, high-priority | 2024-03-30 | enhancement | medium | small | yes | Still relevant. No code warns when 0/1 factor levels are in counterintuitive order (e.g., `levels = c(1, 0)` making `0` the reference level). `assert_input_binary()` only validates factor with 2 levels and predicted in [0,1]. No helper for safe factor conversion exists. @seabbs questioned whether this is must-have for 2.0 or nice-to-have. | Loosely related to #840 (input check cleanup) and #939 (remove `check_input_*` functions). | Add a check in `assert_input_binary()` that warns when observed values are \"0\"/\"1\" and level order is `c(1, 0)`. Optionally create `as_binary_observed()` helper with a `reference_level` argument for safe factor conversion. | [ ] | [ ] | — | [ ] |\n| 759 | Future of the current `scoringutils.Rmd` vignette | documentation, question | 2024-03-27 | documentation | medium | medium | partially | Partially addressed. The outdated `scoringutils.Rmd` was removed in PR #836 (May 2024). However, the promised replacement \"Get started\" vignette was never written — @seabbs volunteered June 2024 but no PR materialized. Package has no introductory vignette. README also has dead links to deleted vignettes (`metric-details`, `scoring-forecasts-directly`). | Related to #546 (simplify getting started). Completing this would likely close #546. Dead README links should be fixed regardless. | Write a new \"Get started\" / quick-start vignette to replace the deleted `scoringutils.Rmd`. Fix dead vignette links in README.Rmd. Consider merging #546 into this issue. | N/A | N/A | — | N/A |\n| 758 | Create a vignette that explains scoring rules | documentation | 2024-03-27 | documentation | medium | large | partially | Partially addressed. Part 1 (explain each scoring rule) is covered by the existing `scoring-rules.Rmd` vignette. Part 2 (practical guidance on choosing a scoring rule — convergence, overconfidence, locality, scale sensitivity) remains unaddressed. The issue body contains a detailed vignette stub. Three illustration PNGs exist in `vignettes/` but are orphaned (not referenced by any vignette). | Parent issue for #929, #930, #933, #934 (illustration issues). Related to #785 (scoring rules vignette updates), #1046 (multivariate content). | Narrow scope to creating a \"Choosing a scoring rule\" vignette covering comparative guidance (convergence, overconfidence, locality, scale sensitivity). Coordinate with #929/#930/#933/#934 to incorporate the orphaned illustrations. | N/A | N/A | — | N/A |\n| 752 | Split up `plot_pit()` into a method for vectors and one for data.frames | | 2024-03-27 | refactor | low | small | yes | Fully resolved. `plot_pit()` was removed entirely in PRs #940 and #949. Replaced by `get_pit_histogram()` (S3 generic with methods for `forecast_sample` and `forecast_quantile`) and `pit_histogram_sample()` for low-level computation. Plotting is now via standard ggplot2 calls. The dual-purpose if/else concern no longer exists. | None. | Close as completed. The issue's concern was superseded by a complete removal of `plot_pit()` in favor of `get_pit_histogram()` with proper S3 dispatch. | N/A | N/A | — | N/A |\n| 750 | Move computation of p-values out of `get_pairwise_comparisons()`? | question | 2024-03-26 | discussion | low | medium | no | Partially addressed. PR #994 added `test_type = NULL` to skip p-value computation, resolving the main pain point (spurious warnings from `add_relative_skill()`). The broader architectural question of separating p-value from score ratio computation remains open but contributor consensus was option 1 (do nothing). | #769 (explicit test_type argument) should be considered together. #1022 (two-model case) tangentially related. | Consider closing as won't-fix or converting to long-term design discussion. The practical problem was solved by PR #994. If kept open, the most impactful remaining change would be defaulting `add_relative_skill()` to `test_type = NULL` since it discards p-values anyway. | N/A | N/A | — | N/A |\n| 740 | Make `get_forecast_unit()` and friends customisable such that they can work with new forecast classes | enhancement | 2024-03-24 | refactor | medium | medium | partially | Still relevant. No implementation work done. `get_protected_columns()` remains a plain function with hard-coded column lists (not S3). `get_duplicate_forecasts()` still hard-codes type-specific columns. Related S3 conversions (e.g., `get_forecast_type`, `get_pit`, `as_forecast`) establish the pattern. On `scoringutils-2.x` milestone. | Near-duplicate of #889 (S3 `get_protected_columns`) and #888 (S3 `get_duplicate_forecasts`). Blocks #852 (custom forecast class workflow). | Convert `get_protected_columns()` into an S3 generic with per-class methods (addressing #889), similarly make `get_duplicate_forecasts()` S3 (#888), so each forecast class declares its own columns. Follow established pattern from completed S3 conversions. | N/A | N/A | — | N/A |\n| 731 | Should `wis()` warn about missing values? | question | 2024-03-23 | discussion | low | small | no | Still relevant. Current behavior is correct — `as_forecast()` emits a vague NA message, `clean_forecast()` strips NA rows before `wis()` sees them. Discussion converged on improving the `as_forecast()` message (#537) as the primary fix, with `wis()`-specific warning being secondary. No consensus reached on implementation. | Related to #537 (high-priority: improve NA-removal message). Related to #542 (debug `score()`). Resolving #537 first would clarify what remains here. | Resolve #537 first (improve NA-removal message in `as_forecast()` to report affected row count). Then decide whether `wis()` needs its own warning for direct (non-`score()` pipeline) usage. May be closeable once #537 is done. | N/A | N/A | — | N/A |\n| 718 | Testing fails locally when done repeatedly | enhancement | 2024-03-20 | bug | low | small | yes | Still relevant and unfixed. Root cause: `cli_warn()` with `.frequency = \"once\"` and `.frequency_id = \"small_interval_range\"` suppresses warnings after first test run in same R session. 4 instances of `.frequency = \"once\"` in codebase. Tests pass on CI (fresh process) and on first local run. | No blocking dependencies. Tangentially related to #935 (surprising output behavior). | Add `rlang::reset_message_verbosity(\"small_interval_range\")` before the `expect_warning()` call in `test-metrics-interval-range.R`. Audit the other 3 `.frequency = \"once\"` sites to ensure their tests are similarly resilient. | [ ] | [ ] | — | [ ] |\n| 713 | Create vignette that showcases different ways to visualise forecasts and scores | documentation | 2024-03-11 | documentation | medium | medium | no | Still relevant. Package has 7 exported plot functions (`plot_wis`, `plot_heatmap`, `plot_interval_coverage`, `plot_quantile_coverage`, `plot_pairwise_comparisons`, `plot_forecast_counts`, `plot_correlations`) plus `theme_scoringutils()` but no dedicated visualization vignette. Only a deprecated-visualisations vignette exists. Issue requests updated `plot_score_table()` and mention of `hubVis`. | Related to #826 (plot_heatmap accessibility), #49 (calibration plotting), #942 (discrimination plots), #356 (interactions vignette). Does not block other issues. | Create a new `vignettes/visualising-scores.Rmd` with worked examples for all exported plot functions using example data. Include mention of `hubVis` as alternative for forecast visualization (per closed #812), and an updated `plot_score_table()` equivalent. | N/A | N/A | — | N/A |\n| 702 | Update docs to conform to tidyverse styleguide | good first issue | 2024-03-08 | documentation | low | small | partially | Still relevant. ~85% of `@param` tags already conform to sentence case. ~21 clearly non-conforming (lowercase start) and ~21 special-character-starting descriptions across ~15 R files. Blocker PR #705 (file reorg) was merged March 2024. Assigned to @jamesmbaazam but no PR submitted. | No blocking dependencies. Standalone cleanup task. PR #705 dependency resolved. | Fix ~21 `@param` descriptions across ~15 R files to start with capital letter and end with full stop per tidyverse style guide. Also audit ~21 special-character-starting descriptions and all `@return`/`@returns` tags for same conformance. | N/A | N/A | — | N/A |\n| 686 | Improve code coverage | | 2024-03-06 | meta | low | small | partially | Largely resolved. Coverage is at 98%. The specific sub-issue (#703, misnamed test file) was fixed in PR #704. Only `theme_scoringutils()` (18 lines) lacks any test coverage. The remaining 2% is edge-case branches. Could be closed as-is. | Sub-issue #703 resolved. No blockers. | Optionally add a test for `theme_scoringutils()` and a `.codecov.yml` with a coverage target to prevent regressions, then close this issue given the excellent 98% coverage level. | N/A | N/A | — | N/A |\n| 685 | Suppress printing in tests | enhancement, implementation-ready | 2024-03-06 | cleanup | medium | medium | partially | Still relevant. 29 `suppressMessages()` and 16 `suppressWarnings()` remain across 13 test files, silently swallowing conditions instead of asserting them. Partially addressed: 80+ `expect_no_condition()` calls already exist. Root causes include `as_forecast_*()` verbose messaging and the #935 print bug. | Partially depends on #935 (unwanted printing on `:=`). Overlaps significantly with #823 (reduce `suppress*()` in package). | Audit all 29 `suppressMessages()` and 16 `suppressWarnings()` in tests: replace with `expect_message()`/`expect_warning()` for intentional conditions, pass `verbose = FALSE` where available, or use `expect_no_condition()` after fixing the source. Also review `setup.R` score calls. | N/A | N/A | — | N/A |\n| 684 | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` | | 2024-03-06 | cleanup | low | small | yes | Still relevant but fully superseded by #939, which covers all 5 `check_input_*` functions. These 3 functions are dead code: never called in production, not exported, only tested. A comment in `check-input-helpers.R` explicitly marks them for future deletion. Should be closed as duplicate of #939. | Superseded by #939. Related to #840 (input check cleanup) and #823 (reduce `try()`). | Close as duplicate of #939. If handled independently, delete the 3 function definitions, their man pages, roxygen cross-references, and corresponding test blocks. | [ ] | [ ] | — | [ ] |\n| 668 | Add argument to control what kind of coverage is computed by `get_coverage()` | feature request | 2024-02-29 | enhancement | low | small | yes | Still relevant and unimplemented. `get_coverage()` always computes all 4 coverage metrics with no user control. The issue body contains a near-complete code proposal (with a minor bug to fix). The function structure already cleanly separates interval and quantile computation paths. On `scoringutils-2.x` milestone. | No blocking dependencies. Standalone enhancement. | Add a `type = c(\"quantile\", \"interval\")` argument to `get_coverage()` that conditionally computes interval coverage, quantile coverage, or both. Update downstream column handling, summarisation, and tests. | [ ] | [ ] | — | [ ] |\n| 667 | Create print method for `scores()` objects | feature request | 2024-02-29 | feature | low | small | partially | Still relevant but effectively blocked by #935. No `print.scores` method exists; scores objects fall through to `print.data.table`. Adding a custom print method would exacerbate the unwanted-printing-on-`:=` bug documented in #935. Both maintainers expressed lukewarm interest, deferring to post-release. | Blocked by #935 (print on column change bug). Related to #975 (link to score definitions in print). | Implement `print.scores()` S3 method displaying computed metric names (from `metrics` attribute) before delegating to `NextMethod()`, following the `print.forecast` pattern. Wait until #935 is resolved first to avoid spurious printing on `:=` operations. | N/A | N/A | — | N/A |\n| 666 | Update the scoringutils paper that accompanies the package | question | 2024-02-29 | documentation | medium | large | no | Still relevant but partially stale. Only 2 of ~15 checklist items done. Manuscript in `inst/manuscript/` still references `magrittr`, CITATION still points to 2022 arXiv preprint. README links to Google Drive PDF. Some checklist items reference functions no longer exported. Should be broken into smaller actionable issues. | Related to #759, #789, #546, #758, #1057. | Audit the to-do list against current API to remove stale items, break remaining unchecked items into individual issues, and decide whether to prioritize the \"make a vignette out of the paper\" item before the next CRAN release. Requires human authorship decisions. | N/A | N/A | — | N/A |\n| 646 | Stop converting errors to warnings in `score()`? | help wanted, question, Breaking change | 2024-02-23 | refactor | medium | medium | partially | Still relevant. `run_safely()` in `R/score.R` still converts all metric errors to warnings via `try()` + `cli_warn()` and returns `NULL`, silently dropping failed metrics. Deferred to `scoringutils-2.x` milestone. No consensus on approach: let errors propagate (breaking), add `on_error` parameter (compatible), or batch warnings (#811). | Related to #811 (batch warnings), #941 (`score()` redesign), #823 (reduce `try()`). | Design decision needed: (a) let errors propagate as errors, (b) add `on_error = c(\"warn\", \"error\")` param to `score()`, or (c) combine with #811's batch-warning approach. Implementation touches `run_safely()` in `R/score.R` and test expectations. | N/A | N/A | — | N/A |\n| 625 | Implement a `get_unique_values()` function | implementation-ready | 2024-02-10 | enhancement | low | small | yes | Still relevant. No implementation work done. Function does not exist. Would compute `length(unique(values))` per forecast-unit column (via `get_forecast_unit()`), optionally grouped by model. Similar diagnostic purpose to `get_forecast_counts()` but different output. | Loosely related to #581 (NA in print), #935 (print bug), #621 (closed, prettier print). No hard blockers for standalone function. | Implement a new exported `get_unique_values()` function that takes a forecast object, calls `get_forecast_unit()` to identify relevant columns, and returns a summary table of unique value counts per column. Optionally integrate into `print.forecast()`. | [ ] | [ ] | — | [ ] |\n| 608 | Define input format for categorical forecasts | implementation-ready | 2024-01-17 | discussion | low | small | no | Largely resolved. Input format fully defined and implemented in `forecast_nominal` (PR #837) and `forecast_ordinal` (PR #977). Column named `predicted_label`. Strict completeness enforced. Only unimplemented item is a helper function to auto-expand incomplete forecasts with zero-probability rows, which has no dedicated tracking issue. Can be closed. | #604 (parent, closed), #607 (naming, closed). No blockers. | Close this issue as completed. Optionally open a new targeted issue for the helper function that auto-expands incomplete forecasts with zero-probability rows if still desired. | N/A | N/A | — | N/A |\n| 581 | Add the number of `NA` values to the planned print method for `forecast_*` objects, rather than throw a message? | feature request | 2024-01-05 | enhancement | medium | medium | partially | Still relevant. None of the three requested features implemented: `print.forecast()` shows no NA info, NA message in `assert_forecast_generic()` is still vague (no row count), no `get_NA()` helper exists. Issue #537 is a near-duplicate and both should be addressed together. | Near-duplicate of #537 (should be resolved together). Tangentially related to #731, #542, #935 (print bug). | Enhance `print.forecast()` to display NA row count, improve the message in `assert_forecast_generic()` to report exact number of NA rows, and consider adding a `get_na_summary()` helper. Consolidate with #537. | N/A | N/A | — | N/A |\n| 557 | sample_to_quantile.numeric method | low-priority | 2024-01-05 | enhancement | low | small | yes | Effectively resolved. `sample_to_quantile()` was replaced by `as_forecast_quantile.forecast_sample()` in PR #876. The motivating code (`interval_coverage_sample()`) was deleted entirely. No current code needs a `.numeric` method. Can be closed. | Tangentially related to #1026 (reverse conversion). No blockers. | Close this issue as resolved. The refactoring in PR #876 and deletion of `interval_coverage_sample()` eliminated the code pattern that motivated this request. | N/A | N/A | — | N/A |\n| 546 | Simplify \"Getting Started\" \\| Split into smaller chunks | documentation | 2024-01-05 | documentation | medium | medium | no | Still relevant. The old `scoringutils.Rmd` was deleted (PR #836) but the promised replacement \"Get started\" vignette was never written. Package currently has no introductory vignette at all. README has a \"Quick start\" section but no pkgdown \"Get started\" page. @seabbs volunteered in #759 but no PR materialized. | Tightly coupled with #759 (future of scoringutils.Rmd) — should be resolved together. Related to #789 (metric customization docs). | Write a new introductory \"Getting Started\" vignette (as `vignettes/scoringutils.Rmd` for pkgdown) providing a concise end-to-end walkthrough. Can close both #546 and #759 simultaneously. Requires editorial decisions on scope and content. | N/A | N/A | — | N/A |\n| 542 | Improve ways to debug `score()` | question | 2024-01-04 | enhancement | medium | medium | partially | Still relevant. Neither action item is done: (1) no vignette documents `debugonce()` for scoring rule debugging, (2) `run_safely()` still only reports the metric name in warnings, not which forecast unit/row caused the failure. The `{cli}` migration (PR #583) improved formatting but not informativeness. | Closely related to #646 (stop converting errors to warnings — open, breaking change). Related to closed #391, #418, #543, #583. | Two tasks remain: (1) add documentation showing `debugonce()` usage for debugging scoring failures, and (2) enhance `run_safely()`/`apply_metrics()` to include forecast-unit context in warning messages. Coordinate error-vs-warning design with #646. | N/A | N/A | — | N/A |\n| 537 | Inform users about the number of rows that have NA values | good first issue, question, implementation-ready, high-priority | 2024-01-03 | enhancement | medium | medium | partially | Still relevant. The message in `assert_forecast_generic()` (line 150 of `R/class-forecast.R`) still says the generic \"Some rows containing NA values may be removed\" without a row count. No NA info in `print.forecast`, no `get_na()` helper. The `anyNA()` check and `na.omit()` call are already in place, so computing the count is trivial. | Near-duplicate of #581 (NA count in print method) — should be resolved together. Related to #731 (wis NA warnings). | Minimal fix: add NA row count to the existing `cli_inform` message in `assert_forecast_generic()`. For a more complete resolution, combine with #581 to also add NA summary to `print.forecast` and potentially create a `get_na()` diagnostic helper. | N/A | N/A | — | N/A |\n| 522 | Create a way to test user-provided scoring functions | feature request | 2023-12-06 | feature | medium | medium | partially | Still relevant. No progress made. `validate_metrics()` only checks that items are functions (via `checkmate::check_function`), not signature compatibility, output shape, or runtime behavior. Custom metric errors are silently converted to warnings by `run_safely()`, making debugging hard. Each of the 6+ forecast types passes different arguments to metrics. | Loosely related to #362 (non-standard metric I/O), #823 (reduce `try()`). | Implement an exported `test_metrics()` function that runs each metric on a small representative subset of a forecast object and reports informative diagnostics: signature compatibility, runtime errors, and output shape validation. Needs design decision on API. | N/A | N/A | — | N/A |\n| 520 | Create documentation/vignette for the naming scheme of our functions | help wanted, question | 2023-11-30 | documentation | low | medium | partially | Still relevant. The issue body already describes the naming scheme in detail but no vignette or formal documentation exists. CONTRIBUTING.md only references tidyverse style guide. Issue #948 (closed) debated `get_` prefix but decided to keep it. 65 exported functions follow conventions consistently. ~2 years old with no implementation. | Related to closed #948, #610, #476, #357. No blockers. | Create a developer-facing vignette or CONTRIBUTING.md section formalizing the naming conventions (scoring rules: `<name>_<type>`, `get_*`, `as_forecast_*`, `is_forecast_*`, `plot_*`, `add_*`, `assert_`/`check_`/`test_`). Update to reflect current classes including `forecast_nominal`, `forecast_ordinal`, `forecast_sample_multivariate`. | N/A | N/A | — | N/A |\n| 513 | Investigate R scripts that get generated based on our vignettes | | 2023-11-28 | cleanup | low | small | yes | Already resolved. The `.R` and `.html` vignette artifacts (generated by `knitr` tangle) are gitignored via `vignettes/.gitignore`, excluded from linting via `.lintr`, and not tracked in git. All three mitigations from the discussion are in place. Issue can be closed. | None. | Close this issue — all requested investigation and mitigations (gitignore, buildignore, lint-ignore for vignette-generated `.R` files) are already implemented. No code changes needed. | N/A | N/A | — | N/A |\n| 492 | Expose `get_range_from_quantile()` to the user | question | 2023-11-22 | enhancement | low | small | yes | Still relevant. Function exists at `R/helper-quantile-interval-range.R:185` as `@keywords internal`, not exported. Maintainer consensus reached: rename to `get_interval_range()` and create `add_interval_range()` wrapper. Neither implemented. Original motivating function `plot_ranges()` was deleted, reducing urgency. 3 internal call sites. | Parent meta-issue #484. Tangentially related to #885 (export decisions). | Rename `get_range_from_quantile()` to `get_interval_range()`, export it, and create a new exported `add_interval_range()` that adds an `interval_range` column to a quantile-format data.table. Update 3 internal call sites, NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n| 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |\n| 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |\n| 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |\n| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |\n| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n\nTRIAGE_COMPLETE","structuredPatch":[{"oldStart":32,"oldLines":7,"newStart":32,"newLines":7,"lines":[" | 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |"," | 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |"," | 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |","-| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |","+| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |"," | 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [ ] | [ ] | — | [ ] |"," | 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |"," | 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [ ] | [ ] | — | [ ] |"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01K3DV8YJNHTsYuof6UdDKWu","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check if all automatable issues with Has Tests not `N/A` are now `[x]`. Let me scan through all rows with `Automatable = yes`:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":318,"cache_read_input_tokens":75430,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":318},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"b3dfb332-3267-4cb6-beef-24cbca198285"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01K3DV8YJNHTsYuof6UdDKWu","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01ErN4ZE4qoVghu917zGKWWv","name":"Grep","input":{"pattern":"\\| yes \\|","path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":318,"cache_read_input_tokens":75430,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":318},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"78c03ca7-e031-4fe2-a652-3da202c8af78"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01ErN4ZE4qoVghu917zGKWWv","type":"tool_result","content":"29:[Omitted long matching line]\n30:[Omitted long matching line]\n32:[Omitted long matching line]\n33:[Omitted long matching line]\n35:[Omitted long matching line]\n36:[Omitted long matching line]\n38:[Omitted long matching line]\n48:[Omitted long matching line]\n49:| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n52:[Omitted long matching line]\n56:[Omitted long matching line]\n61:[Omitted long matching line]\n63:[Omitted long matching line]\n65:[Omitted long matching line]\n66:[Omitted long matching line]\n68:[Omitted long matching line]\n69:[Omitted long matching line]\n71:[Omitted long matching line]\n72:[Omitted long matching line]\n73:[Omitted long matching line]\n74:[Omitted long matching line]\n77:[Omitted long matching line]\n81:[Omitted long matching line]\n86:[Omitted long matching line]\n87:[Omitted long matching line]\n91:[Omitted long matching line]\n94:[Omitted long matching line]\n100:[Omitted long matching line]\n101:[Omitted long matching line]\n105:[Omitted long matching line]\n108:[Omitted long matching line]\n112:[Omitted long matching line]\n113:[Omitted long matching line]\n114:[Omitted long matching line]\n116:[Omitted long matching line]\n117:[Omitted long matching line]\n119:[Omitted long matching line]\n127:[Omitted long matching line]"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"b032b66f-5952-4ca6-bbcf-97c04bd75bb0","tool_use_result":{"mode":"content","numFiles":0,"filenames":[],"content":"29:[Omitted long matching line]\n30:[Omitted long matching line]\n32:[Omitted long matching line]\n33:[Omitted long matching line]\n35:[Omitted long matching line]\n36:[Omitted long matching line]\n38:[Omitted long matching line]\n48:[Omitted long matching line]\n49:| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n52:[Omitted long matching line]\n56:[Omitted long matching line]\n61:[Omitted long matching line]\n63:[Omitted long matching line]\n65:[Omitted long matching line]\n66:[Omitted long matching line]\n68:[Omitted long matching line]\n69:[Omitted long matching line]\n71:[Omitted long matching line]\n72:[Omitted long matching line]\n73:[Omitted long matching line]\n74:[Omitted long matching line]\n77:[Omitted long matching line]\n81:[Omitted long matching line]\n86:[Omitted long matching line]\n87:[Omitted long matching line]\n91:[Omitted long matching line]\n94:[Omitted long matching line]\n100:[Omitted long matching line]\n101:[Omitted long matching line]\n105:[Omitted long matching line]\n108:[Omitted long matching line]\n112:[Omitted long matching line]\n113:[Omitted long matching line]\n114:[Omitted long matching line]\n116:[Omitted long matching line]\n117:[Omitted long matching line]\n119:[Omitted long matching line]\n127:[Omitted long matching line]","numLines":38}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Dx4P9P6U7ve8aeyW9Gyjjo","type":"message","role":"assistant","content":[{"type":"text","text":"I need to check each automatable=yes row for Has Tests status. Let me look at the ones that aren't N/A more carefully."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":662,"cache_read_input_tokens":75748,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":662},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"c41d2638-870c-4fad-91f7-f54282e79c9c"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Dx4P9P6U7ve8aeyW9Gyjjo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019La6Q7dmhpgdW3duLg8ViD","name":"Task","input":{"description":"Check Has Tests column status","prompt":"Read the file /Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md and find all rows where:\n1. The Automatable column is \"yes\"\n2. The Has Tests column is NOT \"N/A\"\n\nFor each such row, report the issue number, the Has Tests value (either \"[ ]\" or \"[x]\"), and the issue title.\n\nThe table format is:\n| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n\nSo Has Tests is the 12th column.","subagent_type":"Explore","model":"haiku"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":662,"cache_read_input_tokens":75748,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":662},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"a5f014b4-b03b-42ca-8fb7-7bdece98a6a2"}
{"type":"user","message":{"role":"user","content":[{"type":"text","text":"Read the file /Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md and find all rows where:\n1. The Automatable column is \"yes\"\n2. The Has Tests column is NOT \"N/A\"\n\nFor each such row, report the issue number, the Has Tests value (either \"[ ]\" or \"[x]\"), and the issue title.\n\nThe table format is:\n| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n\nSo Has Tests is the 12th column."}]},"parent_tool_use_id":"toolu_019La6Q7dmhpgdW3duLg8ViD","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"fd73547e-343b-451f-af9f-d628bfd3c612"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_016GxN225rhqJTSbYLw9CMH9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01JQa4kgZEyu41JmsyVrqT2V","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":471,"cache_read_input_tokens":10318,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":471},"output_tokens":7,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_019La6Q7dmhpgdW3duLg8ViD","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8f6a0478-e3c6-485d-a9c0-3c67b0d08e24"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01JQa4kgZEyu41JmsyVrqT2V","type":"tool_result","content":"     1→# scoringutils Issue Triage\n     2→\n     3→> Auto-generated by Ralph triage loop. Do not edit manually during the loop.\n     4→\n     5→## Summary\n     6→\n     7→| Metric | Count |\n     8→|--------|-------|\n     9→| Total open issues | 106 |\n    10→| Triaged | 106 |\n    11→| Remaining | 0 |\n    12→\n    13→## Pipeline\n    14→\n    15→- [ ] **Test design** — Test specs written for automatable issues\n    16→- [ ] **Implement** — Fixes implemented, PRs created\n    17→- [ ] **Review** — All PRs independently reviewed\n    18→- [ ] **Summary** — Human summary generated\n    19→\n    20→## Issues\n    21→\n    22→| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n    23→|---|-------|--------|--------|----------|----------|------------|-------------|-------------|--------------|----------------|-----------|-------------|-----|----------|\n    24→| 1065 | Release reminder - February 2026 | | 2026-02-02 | meta | medium | medium | partially | Automated reminder. Dev version (2.1.2.9000) has multivariate feature + bug fix accumulated over 161 days. No blocking bugs. Two open PRs (#1060, #1062) are infrastructure changes. | Maintainers should decide whether to merge #1060/#1062 before releasing. | Decide whether to cut a new release (likely v2.2.0 given multivariate feature). If yes, run `usethis::use_release_issue()` and proceed with CRAN submission. If not, close the issue. | N/A | N/A | — | N/A |\n    25→| 1064 | Pre-ranking for multivariate calibration assessment | | 2026-01-30 | feature | medium | large | no | No pre-ranking or multivariate calibration diagnostics exist in the codebase. The `forecast_sample_multivariate` class and energy score are implemented and stable. Design decision needed: integrate into `score()` (Option A) or standalone `get_multivariate_ranks()` function (Option B). MST method may need `igraph` dependency. | None blocking. Builds on existing `forecast_sample_multivariate` infrastructure. | Implement pre-ranking methods (multivariate rank, optionally MST rank) from Gneiting et al. (2008) as calibration diagnostics for multivariate forecasts, after resolving the API design choice with maintainers. | N/A | N/A | — | N/A |\n    26→| 1057 | Revise README to remove news about the package rewrite | documentation | 2026-01-29 | documentation | low | medium | partially | Open, no assignee. The rewrite messaging and Google Drive paper links appear in 4 locations in README.Rmd. Mechanical edits are straightforward but replacement text requires editorial decisions about where to point users for documentation. | Loosely related to #810 (pkgdown for dev docs) and #759 (future of scoringutils.Rmd vignette). | Edit `README.Rmd` to remove/soften the \"major re-write\" paragraph and replace Google Drive paper links with pointers to package vignettes/docs, then re-render README.md. Confirm preferred documentation hierarchy with maintainers first. | N/A | N/A | — | N/A |\n    27→| 1055 | Model confidence sets | feature request | 2026-01-19 | feature | medium | large | no | Design phase — methodology not yet agreed upon. 6 comments with active debate. Naive pairwise p-value approach acknowledged as incomplete. Three alternatives discussed: Hansen et al. MCS, novel extension for incomplete sets, Bayesian approach. No consensus reached. | Relates to #750 (p-value computation refactor), #769 (pairwise comparison arguments), #37 (Diebold-Mariano test), #1022 (two-model pairwise). | Resolve design consensus on which MCS methodology to implement (Hansen et al. on complete sets, novel incomplete-set extension, or Bayesian), then implement `get_model_confidence_set()` building on existing pairwise comparison infrastructure. | N/A | N/A | — | N/A |\n    28→| 1046 | multivariate scoring in scoring rules vignette | documentation | 2025-11-04 | documentation | low | small | partially | Open, agreed upon by maintainers. The scoring-rules vignette has zero mentions of multivariate scoring. A dedicated multivariate vignette already exists. Only a cross-reference and brief section are missing. | None. All underlying code and the dedicated multivariate vignette already exist. | Add a short section to `vignettes/scoring-rules.Rmd` describing the energy score for multivariate forecasts and cross-linking to the existing `scoring-multivariate-forecasts` vignette. No code changes needed. | N/A | N/A | — | N/A |\n    29→| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |\n    30→| 1043 | Update print method for multivariate forecasts to show `joint_across` | | 2025-09-21 | enhancement | low | small | yes | Still relevant. No `print.forecast_sample_multivariate` method exists; the generic `print.forecast` handles all types. `joint_across` is not stored as an attribute — `set_grouping()` computes `.mv_group_id` but discards the original `joint_across` value. | Builds on multivariate infrastructure from merged PR #998. | Add a `print.forecast_sample_multivariate()` method that displays `joint_across` columns. Requires either storing `joint_across` as an attribute during object construction or computing it as the set difference between forecast unit and `get_grouping()` output. Update test snapshots. | [x] | [ ] | — | [ ] |\n    31→| 1026 | as_forecast_sample.forecast_quantile | enhancement | 2025-07-13 | enhancement | medium | medium | partially | Still relevant. The reverse conversion (`as_forecast_quantile.forecast_sample`) exists, but `as_forecast_sample.forecast_quantile` does not. No distribution-fitting dependencies in DESCRIPTION. Author plans to borrow from hubverse approach. | Related to #557 (sample_to_quantile.numeric). May introduce new dependency (e.g., `distfromq`). | Implement `as_forecast_sample.forecast_quantile()` S3 method that fits a distribution or uses interpolation to approximate the predictive CDF from quantiles and draws samples. Requires design decision on approximation method and potential new package dependency. | N/A | N/A | — | N/A |\n    32→| 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |\n    33→| 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |\n    34→| 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |\n    35→| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |\n    36→| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [ ] | [ ] | — | [ ] |\n    37→| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n    38→| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [ ] | [ ] | — | [ ] |\n    39→| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n    40→| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n    41→| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n    42→| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n    43→| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n    44→| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n    45→| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n    46→| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n    47→| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n    48→| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [ ] | [ ] | — | [ ] |\n    49→| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n    50→| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n    51→| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n    52→| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [ ] | [ ] | — | [ ] |\n    53→| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n    54→| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n    55→| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n    56→| 880 | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()? | | 2024-07-30 | enhancement | low | small | yes | Still relevant. Function hard-codes `0.5` on two lines (107, 115) in `R/class-forecast-quantile.R`. Design decisions already resolved in PR #876 discussion: parameter name `quantile_level`, default `0.5`, passed via `...`. No work done since issue opened. | None. Self-contained enhancement. | Add a `quantile_level` parameter (default `0.5`) to `as_forecast_point.forecast_quantile()`, replacing the two hard-coded `0.5` references, then add tests and documentation. | [ ] | [ ] | — | [ ] |\n    57→| 872 | Add a reference for CRPS decomposition approach. | | 2024-07-25 | documentation | low | small | partially | Still relevant. `crps_sample()` in `R/metrics-sample.R` has only a `scoringRules` package reference, not one for the decomposition approach. `wis()` already properly cites Bracher et al. 2021. Minor decision needed on which paper(s) to cite: Bracher et al. 2021 and/or Arnold et al. 2023. | Related to #870 (CRPS/WIS decomposition refactor). Could be addressed in same PR. | Add `@references` entries to `crps_sample()` roxygen block citing paper(s) formalizing the overprediction/underprediction/dispersion decomposition (likely Bracher et al. 2021 and/or Arnold et al. 2023). | N/A | N/A | — | N/A |\n    58→| 870 | Refactor CRPS and WIS decomposition | refactor | 2024-07-22 | refactor | low | medium | partially | Partially addressed. PR #985 (Feb 2025) simplified `crps_sample()` decomposition internally, but the structural duplication remains: `crps_sample()`, `wis()`, and `interval_score()` each independently implement `separate_results` branching, and 6 wrapper functions (3 sample + 3 quantile) repeat identical extraction patterns. | Related to #872 (CRPS decomposition reference). Filed during PR #854 review. On `scoringutils-2.x` milestone. | Extract the decomposition logic (`separate_results` branching and wrapper function boilerplate) into a shared internal helper or factory pattern so `crps_sample()`, `wis()`, and `interval_score()` delegate to the same mechanism instead of each implementing it independently. | N/A | N/A | — | N/A |\n    59→| 852 | Design workflow for creating new forecast classes | enhancement | 2024-07-10 | enhancement | high | large | no | Still relevant. The building blocks exist (`new_forecast()` exported, `as_forecast_generic()` internal, pattern visible in 7 class files) but no documentation, vignette, or guide for third-party class creation exists. Three prerequisite S3 refactors (#889, #888, #740) remain open and block clean extensibility because `get_protected_columns()` and `get_duplicate_forecasts()` hard-code column names, preventing new classes from working seamlessly. `as_forecast_generic()`, `apply_metrics()`, and `as_scores()` are still unexported (#885). | #889 (S3 `get_protected_columns`), #888 (S3 `get_duplicate_forecasts`), #740 (`get_forecast_unit` customization), #885 (export decision for helper functions). | Complete prerequisite S3 refactors (#889, #888), decide export list for class-creation helpers (#885), then write a vignette or article documenting the step-by-step workflow for creating a custom `forecast_*` class (define `as_forecast_<type>`, `assert_forecast.<type>`, `score.<type>`, `get_metrics.<type>`, `get_protected_columns.<type>`, `get_duplicate_forecasts.<type>`). | N/A | N/A | — | N/A |\n    60→| 840 | Clean up input checks | | 2024-06-07 | refactor | low | medium | partially | Still relevant. Input checks are applied inconsistently: `assert_forecast.forecast_binary` and `assert_forecast.forecast_point` call `check_input_*` wrappers (double-validating with scoring functions), while sample/quantile/ordinal/nominal types skip this. `check-input-helpers.R` has a comment marking `check_try()` and wrappers for future deletion. Author leans toward pushing all checks to the scoring-function level. No work done. | Related to #939 (remove `check_input_*` functions), #839, #922. On `scoringutils-2.x` milestone. | Decide where input validation should live (scoring-function level vs. `assert_forecast` level vs. both-consistently). Then either remove `check_input_binary`/`check_input_point` calls from `assert_forecast` methods and delete wrappers + `check_try()`, or add missing checks to sample/quantile `assert_forecast` methods for consistency. | N/A | N/A | — | N/A |\n    61→| 839 | Remove `test_columns_not_present` | | 2024-06-07 | cleanup | low | small | yes | Still relevant. `test_columns_not_present()` is a trivial 1-line wrapper around `checkmate::test_names(..., disjunct.from = ...)` with exactly 1 production call site (`R/class-forecast-binary.R:68`) and 1 test block. `checkmate::test_names` is already imported. Straightforward removal. | Related to #922 (remove `check_columns_present`) and #840 (clean up input checks). Can be batched but has no blocking dependency. | Replace the single call in `R/class-forecast-binary.R:68` with `test_names(colnames(forecast), disjunct.from = ...)`, delete the function definition in `R/check-input-helpers.R`, delete the test block, and remove the generated man page. Optionally bundle with #922. | [ ] | [ ] | — | [ ] |\n    62→| 826 | Update `plot_heatmap()` to be a table instead of a plot | | 2024-05-19 | enhancement | low | medium | partially | Still relevant. Accessibility concern from @Bisaloo is valid: current ggplot2 heatmap renders values as image text, inaccessible to screen readers. Conversation stalled — Bisaloo never replied to follow-up about output format (HTML/LaTeX). `kableExtra` is already in `Suggests`. This is a breaking change: function returns `ggplot` object, used with `+ facet_wrap()` in `inst/manuscript/`. Not used in any current vignette. Design decision needed on migration strategy. | None blocking. Thematically adjacent to #713 (visualization vignette). | Resolve stalled design conversation on output format and migration strategy (deprecate + new function vs. breaking change). Implement colored-table version using `kableExtra` (already in Suggests) or `gt` (new dep), update manuscript, replace test/snapshot, and deprecate or replace `plot_heatmap()`. | N/A | N/A | — | N/A |\n    63→| 825 | Package scaffolding | | 2024-05-19 | refactor | low | small | yes | Still relevant. Neither recommendation acted upon: (1) `Metrics` package still imported for 3 trivial functions (`ae`, `se`, `ape`) used only in `R/class-forecast-point.R` — these are one-line base-R replacements. `Metrics` did receive a 2025 CRAN update, reducing urgency. (2) `matrixStats` not adopted; 3 `apply()` calls in `R/metrics-sample.R` could benefit. Suggest splitting into two sub-tasks, prioritizing `Metrics` removal as a quick win. | None blocking. Removing `Metrics` requires updating vignettes (`scoring-rules.Rmd`) and tests (`test-metrics-binary.R`) that reference `Metrics::` functions. | (1) Replace `Metrics::ae/se/ape` with internal one-liners in `R/class-forecast-point.R`, remove from DESCRIPTION/NAMESPACE, update vignettes and tests. (2) Optionally evaluate adding `matrixStats` for 3 `apply()` calls — lower value, separate decision. | [ ] | [ ] | — | [ ] |\n    64→| 823 | Reduce number of `try()`, `suppressMessages()`, `suppressWarnings()` used in the package | | 2024-05-19 | refactor | medium | medium | no | Still relevant. 8 direct `try()` calls + 6 indirect via `check_try()` + 1 `suppressWarnings(suppressMessages(...))` in R/ code. Usage has slightly increased since filing due to new `[.forecast` subsetting operators (PR #884). Each `try()` exists for a reason; removal requires architectural rethinking of validation flow, not simple find-and-replace. | Deeply linked to #935 (print on column change), #840 (input check cleanup), #939 (remove `check_input_*`), #885 (export decisions), #811 (batch warnings). | Address incrementally: (1) Replace blanket `try()` in subsetting operators with targeted `tryCatch()`, (2) add `verbose` params to internal validation paths to eliminate `suppress*()`, (3) replace `check_try()` with proper `check_*()` functions as part of #840. | N/A | N/A | — | N/A |\n    65→| 811 | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? | enhancement | 2024-05-07 | enhancement | low | small | yes | Still relevant. `run_safely()` in `R/score.R:210` uses `try()` + immediate `cli_warn()` per failed metric. No batching/deferring mechanism exists. Failed metrics silently dropped from `scores` object via `as_scores()` filtering. Blog post on deferred errors referenced in issue. Only 1 comment from maintainer (\"I like this idea\"). | None blocking. Tangentially related to #823 (reduce `try()`) and #382 (metric/column name clash warnings). | Modify `apply_metrics()`/`run_safely()` to collect error messages in a list during the `lapply` loop, then emit a single batched `cli_warn()` summarizing all failed metrics at the end. Update test expectations in `test-score.R`. | [ ] | [ ] | — | [ ] |\n    66→| 810 | Clean up readme by using `pkgdown` to render dev version docs | | 2024-05-05 | documentation | low | small | yes | CLOSED (completed 2026-02-09). PR #1060 merged today, replacing R-based `packageVersion()` detection with pkgdown selective HTML (`.pkgdown-devel`/`.pkgdown-release` fenced divs). `_pkgdown.yml` already had `development: mode: auto`. Dev docs deployed at `/dev/` on gh-pages. | None. Fully resolved. | No action needed — issue closed via merged PR #1060. | N/A | N/A | — | N/A |\n    67→| 808 | Run lintr everywhere | | 2024-05-05 | cleanup | low | medium | partially | Active work in progress. PR #1062 (\"enable lintr for tests, inst, and vignettes\") is open but not yet merged. `.lintr` excludes all `tests/`, `inst/`, and `vignettes/*.R` files. CI only lints changed files on PRs. Main scope is `tests/` (~5,400 lines across 35 files). `inst/` has only 2 R files. `vignettes/*.R` not tracked in git (auto-generated tangles). ~90 `nolint` annotations already exist in `R/` source. | #807 (cli nolint cleanup) should be done first to establish pattern. #513 (vignette .R scripts) related to vignette exclusion. PR #1062 is in flight. | Merge or rebase PR #1062 once reviews pass. May need to fix remaining lint violations or add targeted `# nolint` annotations. Address `tests/` directory-wide lint cleanup separately from `inst/` and `vignettes/`. | N/A | N/A | — | N/A |\n    68→| 807 | Clean up linter statements around `cli` conditions | good first issue, implementation-ready | 2024-05-05 | cleanup | low | medium | yes | Still relevant. ~43 `#nolint start: keyword_quote_linter` blocks across R/ wrap `cli_abort()`/`cli_warn()` calls that use `\"!\" =`, `\"i\" =` named vector syntax. Only 12 instances use Hugo's backtick fix (`\\`!\\` =`). The fix is mechanical: replace `\"!\" =` with `` `!` = `` inside cli condition calls, then remove the `nolint` annotations. Additional `object_usage_linter` suppressions (14 blocks) exist for `.data[...]` patterns in data.table code — these need separate handling. No work done on this issue since filing. | Prerequisite for #808 (run lintr everywhere). Related to PR #583 (which introduced the cli pattern). | Replace all `\"!\" =`, `\"i\" =`, `\"x\" =`, `\"v\" =` with backtick-quoted equivalents (`` `!` = ``, etc.) in `cli_abort()`/`cli_warn()` calls across ~20 R/ files, then delete the corresponding `#nolint start`/`#nolint end` blocks. Handle `object_usage_linter` suppressions separately. | [ ] | [ ] | — | [ ] |\n    69→| 793 | How should we document methods that have different arguments than the default method? | | 2024-04-19 | documentation | low | small | yes | Still relevant. Consensus reached (separate pages per @seabbs), but not implemented. Three generics currently lump methods with differing arguments onto one `@rdname` page: `as_forecast_quantile` (`.forecast_sample` has different args), `as_forecast_point` (`.forecast_quantile`), and `get_pit_histogram` (`.forecast_sample` has extra args). Renders a confusing flat argument list. | None. | Remove shared `@rdname` tags for S3 methods whose arguments differ from the generic (`as_forecast_quantile.forecast_sample`, `as_forecast_point.forecast_quantile`, `get_pit_histogram.forecast_sample`) and give each its own documentation page with proper title and description. | [ ] | [ ] | — | [ ] |\n    70→| 789 | Add documentation about how to customise scores | documentation | 2024-04-13 | documentation | medium | medium | partially | Partially addressed. The `score()` roxygen docs now have a \"Customising metrics\" section with `purrr::partial()` examples (PR #874). But the core ask — a vignette walkthrough — remains undone. No vignette covers `get_metrics()` → `select_metrics()` → `purrr::partial()` → `score()` workflow. On `scoringutils-2.x` milestone. | Related to #546 (vignette restructuring — decides where customization docs go), #666 (manuscript update). | Create a vignette section (or standalone vignette) walking users through the full metric customization workflow: `get_metrics()`, `select_metrics()`, `purrr::partial()`, custom metric functions, and passing to `score()`. Coordinate with vignette restructuring in #546. | N/A | N/A | — | N/A |\n    71→| 785 | Small updates to the scoring rules Vignette | documentation | 2024-04-10 | documentation | low | small | yes | Still relevant. None of the 7 suggestions from @seabbs's review of PR #762 have been implemented: author field still present, Google Drive link still used, no citations for proper scoring rules or transformed scores paper, `get_metrics` wording not improved for website readers, no introductory paragraph on different probabilistic forecast types. Nearly 2 years open with no progress on these items. | Loosely related to #666 (manuscript update re: Google Drive link), #758 (scoring rules vignette parent issue), #1046 (multivariate scoring in same vignette). | Implement the 7 small documentation edits in `vignettes/scoring-rules.Rmd`: remove author field, replace Google Drive link with proper reference, add citations for proper scoring rules and transformed scores, improve `get_metrics` wording, clean whitespace, and add intro paragraph on probabilistic forecast types. | [ ] | [ ] | — | [ ] |\n    72→| 780 | Add a `verbose` argument to `as_forecast()` | | 2024-04-05 | enhancement | low | small | yes | Still relevant. `verbose` already exists on `assert_forecast()` and `assert_forecast_generic()` (PR #778) but has not been threaded through to any `as_forecast_<type>()` constructor. ~11 test lines and 1 internal R source line use `suppressMessages`/`suppressWarnings` wrappers around `as_forecast_*` calls, confirming ongoing need. Author acknowledged low urgency but wants it for test cleanup. | None. PR #778 (verbose on `assert_forecast`) already merged; plumbing is in place. | Add `verbose = TRUE` parameter to each `as_forecast_<type>.default()` method, pass it to their `assert_forecast()` calls, guard direct `cli_warn()`/`cli_inform()` calls behind the flag, and replace `suppressMessages`/`suppressWarnings` wrappers in tests and internal code with `verbose = FALSE`. | [ ] | [ ] | — | [ ] |\n    73→| 769 | Rethink the way arguments are passed in `get_pairwise_comparisons()` | package improvement | 2024-04-03 | refactor | low | small | yes | Still relevant. `test_type`, `one_sided`, and `n_permutations` are still passed via `...` through 3 levels (`get_pairwise_comparisons` → `pairwise_comparison_one_group` → `compare_forecasts`). Users must read internal `compare_forecasts` docs to discover options. PR #994 added `test_type = NULL` but didn't address the `...` pattern. #750 (split out p-values) was deferred. | Loosely related to #750 (if p-values split out, this becomes moot — but #750 deferred indefinitely). No hard blockers. | Promote `test_type`, `one_sided`, and `n_permutations` from `...` to explicit named arguments in `get_pairwise_comparisons()` and `add_relative_skill()`, improving API discoverability. Update roxygen `@param` tags accordingly. | [ ] | [ ] | — | [ ] |\n    74→| 763 | We should check for consistency when scoring binary forecasts | feature request, high-priority | 2024-03-30 | enhancement | medium | small | yes | Still relevant. No code warns when 0/1 factor levels are in counterintuitive order (e.g., `levels = c(1, 0)` making `0` the reference level). `assert_input_binary()` only validates factor with 2 levels and predicted in [0,1]. No helper for safe factor conversion exists. @seabbs questioned whether this is must-have for 2.0 or nice-to-have. | Loosely related to #840 (input check cleanup) and #939 (remove `check_input_*` functions). | Add a check in `assert_input_binary()` that warns when observed values are \"0\"/\"1\" and level order is `c(1, 0)`. Optionally create `as_binary_observed()` helper with a `reference_level` argument for safe factor conversion. | [ ] | [ ] | — | [ ] |\n    75→| 759 | Future of the current `scoringutils.Rmd` vignette | documentation, question | 2024-03-27 | documentation | medium | medium | partially | Partially addressed. The outdated `scoringutils.Rmd` was removed in PR #836 (May 2024). However, the promised replacement \"Get started\" vignette was never written — @seabbs volunteered June 2024 but no PR materialized. Package has no introductory vignette. README also has dead links to deleted vignettes (`metric-details`, `scoring-forecasts-directly`). | Related to #546 (simplify getting started). Completing this would likely close #546. Dead README links should be fixed regardless. | Write a new \"Get started\" / quick-start vignette to replace the deleted `scoringutils.Rmd`. Fix dead vignette links in README.Rmd. Consider merging #546 into this issue. | N/A | N/A | — | N/A |\n    76→| 758 | Create a vignette that explains scoring rules | documentation | 2024-03-27 | documentation | medium | large | partially | Partially addressed. Part 1 (explain each scoring rule) is covered by the existing `scoring-rules.Rmd` vignette. Part 2 (practical guidance on choosing a scoring rule — convergence, overconfidence, locality, scale sensitivity) remains unaddressed. The issue body contains a detailed vignette stub. Three illustration PNGs exist in `vignettes/` but are orphaned (not referenced by any vignette). | Parent issue for #929, #930, #933, #934 (illustration issues). Related to #785 (scoring rules vignette updates), #1046 (multivariate content). | Narrow scope to creating a \"Choosing a scoring rule\" vignette covering comparative guidance (convergence, overconfidence, locality, scale sensitivity). Coordinate with #929/#930/#933/#934 to incorporate the orphaned illustrations. | N/A | N/A | — | N/A |\n    77→| 752 | Split up `plot_pit()` into a method for vectors and one for data.frames | | 2024-03-27 | refactor | low | small | yes | Fully resolved. `plot_pit()` was removed entirely in PRs #940 and #949. Replaced by `get_pit_histogram()` (S3 generic with methods for `forecast_sample` and `forecast_quantile`) and `pit_histogram_sample()` for low-level computation. Plotting is now via standard ggplot2 calls. The dual-purpose if/else concern no longer exists. | None. | Close as completed. The issue's concern was superseded by a complete removal of `plot_pit()` in favor of `get_pit_histogram()` with proper S3 dispatch. | N/A | N/A | — | N/A |\n    78→| 750 | Move computation of p-values out of `get_pairwise_comparisons()`? | question | 2024-03-26 | discussion | low | medium | no | Partially addressed. PR #994 added `test_type = NULL` to skip p-value computation, resolving the main pain point (spurious warnings from `add_relative_skill()`). The broader architectural question of separating p-value from score ratio computation remains open but contributor consensus was option 1 (do nothing). | #769 (explicit test_type argument) should be considered together. #1022 (two-model case) tangentially related. | Consider closing as won't-fix or converting to long-term design discussion. The practical problem was solved by PR #994. If kept open, the most impactful remaining change would be defaulting `add_relative_skill()` to `test_type = NULL` since it discards p-values anyway. | N/A | N/A | — | N/A |\n    79→| 740 | Make `get_forecast_unit()` and friends customisable such that they can work with new forecast classes | enhancement | 2024-03-24 | refactor | medium | medium | partially | Still relevant. No implementation work done. `get_protected_columns()` remains a plain function with hard-coded column lists (not S3). `get_duplicate_forecasts()` still hard-codes type-specific columns. Related S3 conversions (e.g., `get_forecast_type`, `get_pit`, `as_forecast`) establish the pattern. On `scoringutils-2.x` milestone. | Near-duplicate of #889 (S3 `get_protected_columns`) and #888 (S3 `get_duplicate_forecasts`). Blocks #852 (custom forecast class workflow). | Convert `get_protected_columns()` into an S3 generic with per-class methods (addressing #889), similarly make `get_duplicate_forecasts()` S3 (#888), so each forecast class declares its own columns. Follow established pattern from completed S3 conversions. | N/A | N/A | — | N/A |\n    80→| 731 | Should `wis()` warn about missing values? | question | 2024-03-23 | discussion | low | small | no | Still relevant. Current behavior is correct — `as_forecast()` emits a vague NA message, `clean_forecast()` strips NA rows before `wis()` sees them. Discussion converged on improving the `as_forecast()` message (#537) as the primary fix, with `wis()`-specific warning being secondary. No consensus reached on implementation. | Related to #537 (high-priority: improve NA-removal message). Related to #542 (debug `score()`). Resolving #537 first would clarify what remains here. | Resolve #537 first (improve NA-removal message in `as_forecast()` to report affected row count). Then decide whether `wis()` needs its own warning for direct (non-`score()` pipeline) usage. May be closeable once #537 is done. | N/A | N/A | — | N/A |\n    81→| 718 | Testing fails locally when done repeatedly | enhancement | 2024-03-20 | bug | low | small | yes | Still relevant and unfixed. Root cause: `cli_warn()` with `.frequency = \"once\"` and `.frequency_id = \"small_interval_range\"` suppresses warnings after first test run in same R session. 4 instances of `.frequency = \"once\"` in codebase. Tests pass on CI (fresh process) and on first local run. | No blocking dependencies. Tangentially related to #935 (surprising output behavior). | Add `rlang::reset_message_verbosity(\"small_interval_range\")` before the `expect_warning()` call in `test-metrics-interval-range.R`. Audit the other 3 `.frequency = \"once\"` sites to ensure their tests are similarly resilient. | [ ] | [ ] | — | [ ] |\n    82→| 713 | Create vignette that showcases different ways to visualise forecasts and scores | documentation | 2024-03-11 | documentation | medium | medium | no | Still relevant. Package has 7 exported plot functions (`plot_wis`, `plot_heatmap`, `plot_interval_coverage`, `plot_quantile_coverage`, `plot_pairwise_comparisons`, `plot_forecast_counts`, `plot_correlations`) plus `theme_scoringutils()` but no dedicated visualization vignette. Only a deprecated-visualisations vignette exists. Issue requests updated `plot_score_table()` and mention of `hubVis`. | Related to #826 (plot_heatmap accessibility), #49 (calibration plotting), #942 (discrimination plots), #356 (interactions vignette). Does not block other issues. | Create a new `vignettes/visualising-scores.Rmd` with worked examples for all exported plot functions using example data. Include mention of `hubVis` as alternative for forecast visualization (per closed #812), and an updated `plot_score_table()` equivalent. | N/A | N/A | — | N/A |\n    83→| 702 | Update docs to conform to tidyverse styleguide | good first issue | 2024-03-08 | documentation | low | small | partially | Still relevant. ~85% of `@param` tags already conform to sentence case. ~21 clearly non-conforming (lowercase start) and ~21 special-character-starting descriptions across ~15 R files. Blocker PR #705 (file reorg) was merged March 2024. Assigned to @jamesmbaazam but no PR submitted. | No blocking dependencies. Standalone cleanup task. PR #705 dependency resolved. | Fix ~21 `@param` descriptions across ~15 R files to start with capital letter and end with full stop per tidyverse style guide. Also audit ~21 special-character-starting descriptions and all `@return`/`@returns` tags for same conformance. | N/A | N/A | — | N/A |\n    84→| 686 | Improve code coverage | | 2024-03-06 | meta | low | small | partially | Largely resolved. Coverage is at 98%. The specific sub-issue (#703, misnamed test file) was fixed in PR #704. Only `theme_scoringutils()` (18 lines) lacks any test coverage. The remaining 2% is edge-case branches. Could be closed as-is. | Sub-issue #703 resolved. No blockers. | Optionally add a test for `theme_scoringutils()` and a `.codecov.yml` with a coverage target to prevent regressions, then close this issue given the excellent 98% coverage level. | N/A | N/A | — | N/A |\n    85→| 685 | Suppress printing in tests | enhancement, implementation-ready | 2024-03-06 | cleanup | medium | medium | partially | Still relevant. 29 `suppressMessages()` and 16 `suppressWarnings()` remain across 13 test files, silently swallowing conditions instead of asserting them. Partially addressed: 80+ `expect_no_condition()` calls already exist. Root causes include `as_forecast_*()` verbose messaging and the #935 print bug. | Partially depends on #935 (unwanted printing on `:=`). Overlaps significantly with #823 (reduce `suppress*()` in package). | Audit all 29 `suppressMessages()` and 16 `suppressWarnings()` in tests: replace with `expect_message()`/`expect_warning()` for intentional conditions, pass `verbose = FALSE` where available, or use `expect_no_condition()` after fixing the source. Also review `setup.R` score calls. | N/A | N/A | — | N/A |\n    86→| 684 | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` | | 2024-03-06 | cleanup | low | small | yes | Still relevant but fully superseded by #939, which covers all 5 `check_input_*` functions. These 3 functions are dead code: never called in production, not exported, only tested. A comment in `check-input-helpers.R` explicitly marks them for future deletion. Should be closed as duplicate of #939. | Superseded by #939. Related to #840 (input check cleanup) and #823 (reduce `try()`). | Close as duplicate of #939. If handled independently, delete the 3 function definitions, their man pages, roxygen cross-references, and corresponding test blocks. | [ ] | [ ] | — | [ ] |\n    87→| 668 | Add argument to control what kind of coverage is computed by `get_coverage()` | feature request | 2024-02-29 | enhancement | low | small | yes | Still relevant and unimplemented. `get_coverage()` always computes all 4 coverage metrics with no user control. The issue body contains a near-complete code proposal (with a minor bug to fix). The function structure already cleanly separates interval and quantile computation paths. On `scoringutils-2.x` milestone. | No blocking dependencies. Standalone enhancement. | Add a `type = c(\"quantile\", \"interval\")` argument to `get_coverage()` that conditionally computes interval coverage, quantile coverage, or both. Update downstream column handling, summarisation, and tests. | [ ] | [ ] | — | [ ] |\n    88→| 667 | Create print method for `scores()` objects | feature request | 2024-02-29 | feature | low | small | partially | Still relevant but effectively blocked by #935. No `print.scores` method exists; scores objects fall through to `print.data.table`. Adding a custom print method would exacerbate the unwanted-printing-on-`:=` bug documented in #935. Both maintainers expressed lukewarm interest, deferring to post-release. | Blocked by #935 (print on column change bug). Related to #975 (link to score definitions in print). | Implement `print.scores()` S3 method displaying computed metric names (from `metrics` attribute) before delegating to `NextMethod()`, following the `print.forecast` pattern. Wait until #935 is resolved first to avoid spurious printing on `:=` operations. | N/A | N/A | — | N/A |\n    89→| 666 | Update the scoringutils paper that accompanies the package | question | 2024-02-29 | documentation | medium | large | no | Still relevant but partially stale. Only 2 of ~15 checklist items done. Manuscript in `inst/manuscript/` still references `magrittr`, CITATION still points to 2022 arXiv preprint. README links to Google Drive PDF. Some checklist items reference functions no longer exported. Should be broken into smaller actionable issues. | Related to #759, #789, #546, #758, #1057. | Audit the to-do list against current API to remove stale items, break remaining unchecked items into individual issues, and decide whether to prioritize the \"make a vignette out of the paper\" item before the next CRAN release. Requires human authorship decisions. | N/A | N/A | — | N/A |\n    90→| 646 | Stop converting errors to warnings in `score()`? | help wanted, question, Breaking change | 2024-02-23 | refactor | medium | medium | partially | Still relevant. `run_safely()` in `R/score.R` still converts all metric errors to warnings via `try()` + `cli_warn()` and returns `NULL`, silently dropping failed metrics. Deferred to `scoringutils-2.x` milestone. No consensus on approach: let errors propagate (breaking), add `on_error` parameter (compatible), or batch warnings (#811). | Related to #811 (batch warnings), #941 (`score()` redesign), #823 (reduce `try()`). | Design decision needed: (a) let errors propagate as errors, (b) add `on_error = c(\"warn\", \"error\")` param to `score()`, or (c) combine with #811's batch-warning approach. Implementation touches `run_safely()` in `R/score.R` and test expectations. | N/A | N/A | — | N/A |\n    91→| 625 | Implement a `get_unique_values()` function | implementation-ready | 2024-02-10 | enhancement | low | small | yes | Still relevant. No implementation work done. Function does not exist. Would compute `length(unique(values))` per forecast-unit column (via `get_forecast_unit()`), optionally grouped by model. Similar diagnostic purpose to `get_forecast_counts()` but different output. | Loosely related to #581 (NA in print), #935 (print bug), #621 (closed, prettier print). No hard blockers for standalone function. | Implement a new exported `get_unique_values()` function that takes a forecast object, calls `get_forecast_unit()` to identify relevant columns, and returns a summary table of unique value counts per column. Optionally integrate into `print.forecast()`. | [ ] | [ ] | — | [ ] |\n    92→| 608 | Define input format for categorical forecasts | implementation-ready | 2024-01-17 | discussion | low | small | no | Largely resolved. Input format fully defined and implemented in `forecast_nominal` (PR #837) and `forecast_ordinal` (PR #977). Column named `predicted_label`. Strict completeness enforced. Only unimplemented item is a helper function to auto-expand incomplete forecasts with zero-probability rows, which has no dedicated tracking issue. Can be closed. | #604 (parent, closed), #607 (naming, closed). No blockers. | Close this issue as completed. Optionally open a new targeted issue for the helper function that auto-expands incomplete forecasts with zero-probability rows if still desired. | N/A | N/A | — | N/A |\n    93→| 581 | Add the number of `NA` values to the planned print method for `forecast_*` objects, rather than throw a message? | feature request | 2024-01-05 | enhancement | medium | medium | partially | Still relevant. None of the three requested features implemented: `print.forecast()` shows no NA info, NA message in `assert_forecast_generic()` is still vague (no row count), no `get_NA()` helper exists. Issue #537 is a near-duplicate and both should be addressed together. | Near-duplicate of #537 (should be resolved together). Tangentially related to #731, #542, #935 (print bug). | Enhance `print.forecast()` to display NA row count, improve the message in `assert_forecast_generic()` to report exact number of NA rows, and consider adding a `get_na_summary()` helper. Consolidate with #537. | N/A | N/A | — | N/A |\n    94→| 557 | sample_to_quantile.numeric method | low-priority | 2024-01-05 | enhancement | low | small | yes | Effectively resolved. `sample_to_quantile()` was replaced by `as_forecast_quantile.forecast_sample()` in PR #876. The motivating code (`interval_coverage_sample()`) was deleted entirely. No current code needs a `.numeric` method. Can be closed. | Tangentially related to #1026 (reverse conversion). No blockers. | Close this issue as resolved. The refactoring in PR #876 and deletion of `interval_coverage_sample()` eliminated the code pattern that motivated this request. | N/A | N/A | — | N/A |\n    95→| 546 | Simplify \"Getting Started\" \\| Split into smaller chunks | documentation | 2024-01-05 | documentation | medium | medium | no | Still relevant. The old `scoringutils.Rmd` was deleted (PR #836) but the promised replacement \"Get started\" vignette was never written. Package currently has no introductory vignette at all. README has a \"Quick start\" section but no pkgdown \"Get started\" page. @seabbs volunteered in #759 but no PR materialized. | Tightly coupled with #759 (future of scoringutils.Rmd) — should be resolved together. Related to #789 (metric customization docs). | Write a new introductory \"Getting Started\" vignette (as `vignettes/scoringutils.Rmd` for pkgdown) providing a concise end-to-end walkthrough. Can close both #546 and #759 simultaneously. Requires editorial decisions on scope and content. | N/A | N/A | — | N/A |\n    96→| 542 | Improve ways to debug `score()` | question | 2024-01-04 | enhancement | medium | medium | partially | Still relevant. Neither action item is done: (1) no vignette documents `debugonce()` for scoring rule debugging, (2) `run_safely()` still only reports the metric name in warnings, not which forecast unit/row caused the failure. The `{cli}` migration (PR #583) improved formatting but not informativeness. | Closely related to #646 (stop converting errors to warnings — open, breaking change). Related to closed #391, #418, #543, #583. | Two tasks remain: (1) add documentation showing `debugonce()` usage for debugging scoring failures, and (2) enhance `run_safely()`/`apply_metrics()` to include forecast-unit context in warning messages. Coordinate error-vs-warning design with #646. | N/A | N/A | — | N/A |\n    97→| 537 | Inform users about the number of rows that have NA values | good first issue, question, implementation-ready, high-priority | 2024-01-03 | enhancement | medium | medium | partially | Still relevant. The message in `assert_forecast_generic()` (line 150 of `R/class-forecast.R`) still says the generic \"Some rows containing NA values may be removed\" without a row count. No NA info in `print.forecast`, no `get_na()` helper. The `anyNA()` check and `na.omit()` call are already in place, so computing the count is trivial. | Near-duplicate of #581 (NA count in print method) — should be resolved together. Related to #731 (wis NA warnings). | Minimal fix: add NA row count to the existing `cli_inform` message in `assert_forecast_generic()`. For a more complete resolution, combine with #581 to also add NA summary to `print.forecast` and potentially create a `get_na()` diagnostic helper. | N/A | N/A | — | N/A |\n    98→| 522 | Create a way to test user-provided scoring functions | feature request | 2023-12-06 | feature | medium | medium | partially | Still relevant. No progress made. `validate_metrics()` only checks that items are functions (via `checkmate::check_function`), not signature compatibility, output shape, or runtime behavior. Custom metric errors are silently converted to warnings by `run_safely()`, making debugging hard. Each of the 6+ forecast types passes different arguments to metrics. | Loosely related to #362 (non-standard metric I/O), #823 (reduce `try()`). | Implement an exported `test_metrics()` function that runs each metric on a small representative subset of a forecast object and reports informative diagnostics: signature compatibility, runtime errors, and output shape validation. Needs design decision on API. | N/A | N/A | — | N/A |\n    99→| 520 | Create documentation/vignette for the naming scheme of our functions | help wanted, question | 2023-11-30 | documentation | low | medium | partially | Still relevant. The issue body already describes the naming scheme in detail but no vignette or formal documentation exists. CONTRIBUTING.md only references tidyverse style guide. Issue #948 (closed) debated `get_` prefix but decided to keep it. 65 exported functions follow conventions consistently. ~2 years old with no implementation. | Related to closed #948, #610, #476, #357. No blockers. | Create a developer-facing vignette or CONTRIBUTING.md section formalizing the naming conventions (scoring rules: `<name>_<type>`, `get_*`, `as_forecast_*`, `is_forecast_*`, `plot_*`, `add_*`, `assert_`/`check_`/`test_`). Update to reflect current classes including `forecast_nominal`, `forecast_ordinal`, `forecast_sample_multivariate`. | N/A | N/A | — | N/A |\n   100→| 513 | Investigate R scripts that get generated based on our vignettes | | 2023-11-28 | cleanup | low | small | yes | Already resolved. The `.R` and `.html` vignette artifacts (generated by `knitr` tangle) are gitignored via `vignettes/.gitignore`, excluded from linting via `.lintr`, and not tracked in git. All three mitigations from the discussion are in place. Issue can be closed. | None. | Close this issue — all requested investigation and mitigations (gitignore, buildignore, lint-ignore for vignette-generated `.R` files) are already implemented. No code changes needed. | N/A | N/A | — | N/A |\n   101→| 492 | Expose `get_range_from_quantile()` to the user | question | 2023-11-22 | enhancement | low | small | yes | Still relevant. Function exists at `R/helper-quantile-interval-range.R:185` as `@keywords internal`, not exported. Maintainer consensus reached: rename to `get_interval_range()` and create `add_interval_range()` wrapper. Neither implemented. Original motivating function `plot_ranges()` was deleted, reducing urgency. 3 internal call sites. | Parent meta-issue #484. Tangentially related to #885 (export decisions). | Rename `get_range_from_quantile()` to `get_interval_range()`, export it, and create a new exported `add_interval_range()` that adds an `interval_range` column to a quantile-format data.table. Update 3 internal call sites, NAMESPACE, docs, and tests. | [ ] | [ ] | — | [ ] |\n   102→| 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |\n   103→| 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |\n   104→| 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |\n   105→| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |\n   106→| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n   107→| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n   108→| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n   109→| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n   110→| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n   111→| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n   112→| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n   113→| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n   114→| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n   115→| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n   116→| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n   117→| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n   118→| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n   119→| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n   120→| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n   121→| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n   122→| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n   123→| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n   124→| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n   125→| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n   126→| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n   127→| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n   128→| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n   129→| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n   130→\n   131→TRIAGE_COMPLETE"}]},"parent_tool_use_id":"toolu_019La6Q7dmhpgdW3duLg8ViD","session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"5a9e6c37-39e5-4012-bce5-4b06fcc15f7f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019La6Q7dmhpgdW3duLg8ViD","type":"tool_result","content":[{"type":"text","text":"Perfect! I've read the file. Now let me extract the rows where:\n1. **Automatable** column (column 8) = \"yes\"\n2. **Has Tests** column (column 12) ≠ \"N/A\"\n\nLet me parse through the table data and identify the matching rows:\n\nLooking at the table, I can identify the following rows that meet both criteria:\n\n1. **Issue 1044**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Remove the commented out vignette code.\"\n\n2. **Issue 1043**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"Update print method for multivariate forecasts to show `joint_across`\"\n\n3. **Issue 1022**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"Let `get_pairwise_comparison()` run with two models?\"\n\n4. **Issue 1002**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"LaTeX not rendering in function reference\"\n\n5. **Issue 983**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"Create `.data.frame` methods for `as_forecast<type>`\"\n\n6. **Issue 975**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Add link to score definitions in `summarise_score()`\"\n\n7. **Issue 942**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Discrimination plots for binary forecasts\"\n\n8. **Issue 922**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"remove `check_columns_present`\"\n\n9. **Issue 895**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Check quantile prediction order helper function\"\n\n10. **Issue 880**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Allow users to specify the quantile in `as_forecast_point.forecast_quantile()`?\"\n\n11. **Issue 839**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Remove `test_columns_not_present`\"\n\n12. **Issue 825**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Package scaffolding\"\n\n13. **Issue 811**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Should `score()`/`apply_metrics()` return warning about failed metrics in a batch?\"\n\n14. **Issue 807**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Clean up linter statements around `cli` conditions\"\n\n15. **Issue 793**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"How should we document methods that have different arguments than the default method?\"\n\n16. **Issue 785**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Small updates to the scoring rules Vignette\"\n\n17. **Issue 780**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add a `verbose` argument to `as_forecast()`\"\n\n18. **Issue 769**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Rethink the way arguments are passed in `get_pairwise_comparisons()`\"\n\n19. **Issue 763**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"We should check for consistency when scoring binary forecasts\"\n\n20. **Issue 718**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Testing fails locally when done repeatedly\"\n\n21. **Issue 684**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()`\"\n\n22. **Issue 668**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add argument to control what kind of coverage is computed by `get_coverage()`\"\n\n23. **Issue 625**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Implement a `get_unique_values()` function\"\n\n24. **Issue 557**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"sample_to_quantile.numeric method\"\n\n25. **Issue 492**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Expose `get_range_from_quantile()` to the user\"\n\n26. **Issue 454**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()`\"\n\n27. **Issue 445**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Cleanup: delete calls that use `::` or `:::` and add to imports instead\"\n\n28. **Issue 412**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add check to `validate_forecast()` that predictions must not decrease with quantile\"\n\n29. **Issue 406**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Maybe expose `get_type()`.\"\n\n30. **Issue 382**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Feature: Input check to notify about clashes with metrics and column names\"\n\n31. **Issue 360**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Create a wrapper around log score to warn about its use for integer-valued forecasts\"\n\n32. **Issue 363**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Discussion: design interface for external functions with different input/output formats\"\n\n33. **Issue 193**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add more metrics to assess sharpness of forecasts in a sample format\"\n\n## Summary\n\nHere are all the rows where **Automatable = \"yes\"** AND **Has Tests ≠ \"N/A\"**:\n\n| Issue # | Has Tests | Title |\n|---------|-----------|-------|\n| 1044 | `[ ]` | Remove the commented out vignette code. |\n| 1043 | `[x]` | Update print method for multivariate forecasts to show `joint_across` |\n| 1022 | `[x]` | Let `get_pairwise_comparison()` run with two models? |\n| 1002 | `[x]` | LaTeX not rendering in function reference |\n| 983 | `[x]` | Create `.data.frame` methods for `as_forecast<type>` |\n| 975 | `[ ]` | Add link to score definitions in `summarise_score()` |\n| 942 | `[ ]` | Discrimination plots for binary forecasts |\n| 922 | `[ ]` | remove `check_columns_present` |\n| 895 | `[ ]` | Check quantile prediction order helper function |\n| 880 | `[ ]` | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()`? |\n| 839 | `[ ]` | Remove `test_columns_not_present` |\n| 825 | `[ ]` | Package scaffolding |\n| 811 | `[ ]` | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? |\n| 807 | `[ ]` | Clean up linter statements around `cli` conditions |\n| 793 | `[ ]` | How should we document methods that have different arguments than the default method? |\n| 785 | `[ ]` | Small updates to the scoring rules Vignette |\n| 780 | `[ ]` | Add a `verbose` argument to `as_forecast()` |\n| 769 | `[ ]` | Rethink the way arguments are passed in `get_pairwise_comparisons()` |\n| 763 | `[ ]` | We should check for consistency when scoring binary forecasts |\n| 718 | `[ ]` | Testing fails locally when done repeatedly |\n| 684 | `[ ]` | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` |\n| 668 | `[ ]` | Add argument to control what kind of coverage is computed by `get_coverage()` |\n| 625 | `[ ]` | Implement a `get_unique_values()` function |\n| 557 | `[ ]` | sample_to_quantile.numeric method |\n| 492 | `[ ]` | Expose `get_range_from_quantile()` to the user |\n| 454 | `[ ]` | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` |\n| 445 | `[ ]` | Cleanup: delete calls that use `::` or `:::` and add to imports instead |\n| 412 | `[ ]` | Add check to `validate_forecast()` that predictions must not decrease with quantile |\n| 406 | `[ ]` | Maybe expose `get_type()`. |\n| 382 | `[ ]` | Feature: Input check to notify about clashes with metrics and column names |\n| 360 | `[ ]` | Create a wrapper around log score to warn about its use for integer-valued forecasts |\n| 363 | `[ ]` | Discussion: design interface for external functions with different input/output formats |\n| 193 | `[ ]` | Add more metrics to assess sharpness of forecasts in a sample format |\n\n**Total: 33 issues** matching the criteria. Of these, 4 have tests already implemented (`[x]`), and 29 do not have tests yet (`[ ]`)."},{"type":"text","text":"agentId: af539df (for resuming to continue this agent's work if needed)\n<usage>total_tokens: 38648\ntool_uses: 1\nduration_ms: 17625</usage>"}]}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"fdfd5454-b16d-4c20-89d8-3dfec7944d52","tool_use_result":{"status":"completed","prompt":"Read the file /Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md and find all rows where:\n1. The Automatable column is \"yes\"\n2. The Has Tests column is NOT \"N/A\"\n\nFor each such row, report the issue number, the Has Tests value (either \"[ ]\" or \"[x]\"), and the issue title.\n\nThe table format is:\n| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n\nSo Has Tests is the 12th column.","agentId":"af539df","content":[{"type":"text","text":"Perfect! I've read the file. Now let me extract the rows where:\n1. **Automatable** column (column 8) = \"yes\"\n2. **Has Tests** column (column 12) ≠ \"N/A\"\n\nLet me parse through the table data and identify the matching rows:\n\nLooking at the table, I can identify the following rows that meet both criteria:\n\n1. **Issue 1044**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Remove the commented out vignette code.\"\n\n2. **Issue 1043**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"Update print method for multivariate forecasts to show `joint_across`\"\n\n3. **Issue 1022**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"Let `get_pairwise_comparison()` run with two models?\"\n\n4. **Issue 1002**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"LaTeX not rendering in function reference\"\n\n5. **Issue 983**: Has Tests = `[x]`, Automatable = \"yes\"\n   - Title: \"Create `.data.frame` methods for `as_forecast<type>`\"\n\n6. **Issue 975**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Add link to score definitions in `summarise_score()`\"\n\n7. **Issue 942**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Discrimination plots for binary forecasts\"\n\n8. **Issue 922**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"remove `check_columns_present`\"\n\n9. **Issue 895**: Has Tests = `[ ]`, Automatable = \"yes\"\n   - Title: \"Check quantile prediction order helper function\"\n\n10. **Issue 880**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Allow users to specify the quantile in `as_forecast_point.forecast_quantile()`?\"\n\n11. **Issue 839**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Remove `test_columns_not_present`\"\n\n12. **Issue 825**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Package scaffolding\"\n\n13. **Issue 811**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Should `score()`/`apply_metrics()` return warning about failed metrics in a batch?\"\n\n14. **Issue 807**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Clean up linter statements around `cli` conditions\"\n\n15. **Issue 793**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"How should we document methods that have different arguments than the default method?\"\n\n16. **Issue 785**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Small updates to the scoring rules Vignette\"\n\n17. **Issue 780**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add a `verbose` argument to `as_forecast()`\"\n\n18. **Issue 769**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Rethink the way arguments are passed in `get_pairwise_comparisons()`\"\n\n19. **Issue 763**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"We should check for consistency when scoring binary forecasts\"\n\n20. **Issue 718**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Testing fails locally when done repeatedly\"\n\n21. **Issue 684**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()`\"\n\n22. **Issue 668**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add argument to control what kind of coverage is computed by `get_coverage()`\"\n\n23. **Issue 625**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Implement a `get_unique_values()` function\"\n\n24. **Issue 557**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"sample_to_quantile.numeric method\"\n\n25. **Issue 492**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Expose `get_range_from_quantile()` to the user\"\n\n26. **Issue 454**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()`\"\n\n27. **Issue 445**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Cleanup: delete calls that use `::` or `:::` and add to imports instead\"\n\n28. **Issue 412**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add check to `validate_forecast()` that predictions must not decrease with quantile\"\n\n29. **Issue 406**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Maybe expose `get_type()`.\"\n\n30. **Issue 382**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Feature: Input check to notify about clashes with metrics and column names\"\n\n31. **Issue 360**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Create a wrapper around log score to warn about its use for integer-valued forecasts\"\n\n32. **Issue 363**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Discussion: design interface for external functions with different input/output formats\"\n\n33. **Issue 193**: Has Tests = `[ ]`, Automatable = \"yes\"\n    - Title: \"Add more metrics to assess sharpness of forecasts in a sample format\"\n\n## Summary\n\nHere are all the rows where **Automatable = \"yes\"** AND **Has Tests ≠ \"N/A\"**:\n\n| Issue # | Has Tests | Title |\n|---------|-----------|-------|\n| 1044 | `[ ]` | Remove the commented out vignette code. |\n| 1043 | `[x]` | Update print method for multivariate forecasts to show `joint_across` |\n| 1022 | `[x]` | Let `get_pairwise_comparison()` run with two models? |\n| 1002 | `[x]` | LaTeX not rendering in function reference |\n| 983 | `[x]` | Create `.data.frame` methods for `as_forecast<type>` |\n| 975 | `[ ]` | Add link to score definitions in `summarise_score()` |\n| 942 | `[ ]` | Discrimination plots for binary forecasts |\n| 922 | `[ ]` | remove `check_columns_present` |\n| 895 | `[ ]` | Check quantile prediction order helper function |\n| 880 | `[ ]` | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()`? |\n| 839 | `[ ]` | Remove `test_columns_not_present` |\n| 825 | `[ ]` | Package scaffolding |\n| 811 | `[ ]` | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? |\n| 807 | `[ ]` | Clean up linter statements around `cli` conditions |\n| 793 | `[ ]` | How should we document methods that have different arguments than the default method? |\n| 785 | `[ ]` | Small updates to the scoring rules Vignette |\n| 780 | `[ ]` | Add a `verbose` argument to `as_forecast()` |\n| 769 | `[ ]` | Rethink the way arguments are passed in `get_pairwise_comparisons()` |\n| 763 | `[ ]` | We should check for consistency when scoring binary forecasts |\n| 718 | `[ ]` | Testing fails locally when done repeatedly |\n| 684 | `[ ]` | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` |\n| 668 | `[ ]` | Add argument to control what kind of coverage is computed by `get_coverage()` |\n| 625 | `[ ]` | Implement a `get_unique_values()` function |\n| 557 | `[ ]` | sample_to_quantile.numeric method |\n| 492 | `[ ]` | Expose `get_range_from_quantile()` to the user |\n| 454 | `[ ]` | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` |\n| 445 | `[ ]` | Cleanup: delete calls that use `::` or `:::` and add to imports instead |\n| 412 | `[ ]` | Add check to `validate_forecast()` that predictions must not decrease with quantile |\n| 406 | `[ ]` | Maybe expose `get_type()`. |\n| 382 | `[ ]` | Feature: Input check to notify about clashes with metrics and column names |\n| 360 | `[ ]` | Create a wrapper around log score to warn about its use for integer-valued forecasts |\n| 363 | `[ ]` | Discussion: design interface for external functions with different input/output formats |\n| 193 | `[ ]` | Add more metrics to assess sharpness of forecasts in a sample format |\n\n**Total: 33 issues** matching the criteria. Of these, 4 have tests already implemented (`[x]`), and 29 do not have tests yet (`[ ]`)."}],"totalDurationMs":17625,"totalTokens":38648,"totalToolUseCount":1,"usage":{"input_tokens":5,"cache_creation_input_tokens":25400,"cache_read_input_tokens":10789,"output_tokens":2454,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":25400,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017nUiswYo2J5y912mpfJJFt","type":"message","role":"assistant","content":[{"type":"text","text":"There are still many automatable issues with `Has Tests = [ ]`, so I should NOT set the `- [x] **Test design**` pipeline checkpoint. The work is not complete yet."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2803,"cache_read_input_tokens":76410,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2803},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"bbfda794-deed-4f05-b93a-2a13e7269d4f"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_017nUiswYo2J5y912mpfJJFt","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01B3rLDb6QG6Z5Ci4LHbA3NA","name":"TodoWrite","input":{"todos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"completed","activeForm":"Updating the triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2803,"cache_read_input_tokens":76410,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2803},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"a0b68411-9285-4e55-b476-dccc01b89aef"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01B3rLDb6QG6Z5Ci4LHbA3NA","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"d911f71c-54a8-42a1-bf43-728bd25431c6","tool_use_result":{"oldTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"in_progress","activeForm":"Updating the triage table"}],"newTodos":[{"content":"Find next automatable issue without test cases","status":"completed","activeForm":"Finding next automatable issue"},{"content":"Read full issue context for #983 (create .data.frame methods)","status":"completed","activeForm":"Reading issue #983 context"},{"content":"Understand the test landscape for as_forecast functions","status":"completed","activeForm":"Understanding the test landscape"},{"content":"Design test cases for #983","status":"completed","activeForm":"Designing test cases"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs"},{"content":"Update the triage table","status":"completed","activeForm":"Updating the triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01CnfszUK9jNrYsGt1CeoYEN","type":"message","role":"assistant","content":[{"type":"text","text":"Done. Here's a summary of what was completed:\n\n**Issue processed**: #983 — Create `.data.frame` methods for `as_forecast<type>`\n\n**Test specs written to**: `.ralph/issues/983-as-forecast-data-frame-methods.md`\n\n**12 test cases designed** covering:\n1. **Tests 1-7**: One test per forecast type (binary, quantile, sample, point, nominal, ordinal, multivariate_sample) verifying `.data.frame` method works with plain `data.frame` input — regression guards\n2. **Test 8**: Confirms `data.table` input still dispatches correctly via S3 inheritance to `.data.frame` method\n3. **Tests 9-10**: Core bug tests — the new `.default` method should error with a helpful message for non-data.frame types (vectors, matrices, lists, character strings)\n4. **Test 11**: Column renaming functionality preserved after method rename\n5. **Test 12**: Tibble input dispatches correctly to `.data.frame` method\n\n**Triage table updated**: Has Tests for #983 changed from `[ ]` to `[x]`. Pipeline checkpoint NOT set (29 other automatable issues still need test design)."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":322,"cache_read_input_tokens":79213,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":322},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","uuid":"8ebe649f-7ed0-4713-b0f2-1fde17c64c86"}
{"type":"result","subtype":"success","is_error":false,"duration_ms":292104,"duration_api_ms":299934,"num_turns":37,"result":"Done. Here's a summary of what was completed:\n\n**Issue processed**: #983 — Create `.data.frame` methods for `as_forecast<type>`\n\n**Test specs written to**: `.ralph/issues/983-as-forecast-data-frame-methods.md`\n\n**12 test cases designed** covering:\n1. **Tests 1-7**: One test per forecast type (binary, quantile, sample, point, nominal, ordinal, multivariate_sample) verifying `.data.frame` method works with plain `data.frame` input — regression guards\n2. **Test 8**: Confirms `data.table` input still dispatches correctly via S3 inheritance to `.data.frame` method\n3. **Tests 9-10**: Core bug tests — the new `.default` method should error with a helpful message for non-data.frame types (vectors, matrices, lists, character strings)\n4. **Test 11**: Column renaming functionality preserved after method rename\n5. **Test 12**: Tibble input dispatches correctly to `.data.frame` method\n\n**Triage table updated**: Has Tests for #983 changed from `[ ]` to `[x]`. Pipeline checkpoint NOT set (29 other automatable issues still need test design).","stop_reason":null,"session_id":"246e70b1-a882-48a6-b2f0-18ae4a988f98","total_cost_usd":1.78856635,"usage":{"input_tokens":32,"cache_creation_input_tokens":61531,"cache_read_input_tokens":1824415,"output_tokens":10427,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":61531,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]},"modelUsage":{"claude-opus-4-6":{"inputTokens":32,"outputTokens":10427,"cacheReadInputTokens":1824415,"cacheCreationInputTokens":61531,"webSearchRequests":0,"costUSD":1.5576112500000003,"contextWindow":200000,"maxOutputTokens":32000},"claude-haiku-4-5-20251001":{"inputTokens":13454,"outputTokens":10111,"cacheReadInputTokens":480536,"cacheCreationInputTokens":95114,"webSearchRequests":0,"costUSD":0.23095509999999997,"contextWindow":200000,"maxOutputTokens":64000}},"permission_denials":[],"uuid":"9dd43658-00ac-44fe-b5b3-013a5ae1bbd7"}
