{"type":"system","subtype":"init","cwd":"/Users/nikos/github-synced/scoringutils-pipeline","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","tools":["Task","TaskOutput","Bash","Glob","Grep","ExitPlanMode","Read","Edit","Write","NotebookEdit","WebFetch","TodoWrite","WebSearch","TaskStop","AskUserQuestion","Skill","EnterPlanMode","ToolSearch"],"mcp_servers":[],"model":"claude-opus-4-6","permissionMode":"bypassPermissions","slash_commands":["keybindings-help","debug","compact","context","cost","init","pr-comments","release-notes","review","security-review","insights"],"apiKeySource":"none","claude_code_version":"2.1.39","output_style":"default","agents":["Bash","general-purpose","statusline-setup","Explore","Plan","claude-code-guide"],"skills":["keybindings-help","debug"],"plugins":[],"uuid":"3721a92b-727a-4001-b2e8-98344b80691b","fast_mode_state":"off"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_015qjwCSQCQUoYRZ3YdLUcfX","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nI'll start by verifying the worktree and branch, then find the next automatable issue to design tests for."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2800,"cache_read_input_tokens":18004,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2800},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"b6dafac8-1122-4d73-bff6-bad5f4f3c28d"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_015qjwCSQCQUoYRZ3YdLUcfX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016KGuFf9718HJaviNUVXpdZ","name":"Bash","input":{"command":"git rev-parse --show-toplevel","description":"Verify git worktree root"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2800,"cache_read_input_tokens":18004,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2800},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"142377a3-a6a4-475f-b4fc-bffa97f20f52"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_015qjwCSQCQUoYRZ3YdLUcfX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01G3VNBPhpJ6GTC3ADTZwHH9","name":"Bash","input":{"command":"git branch --show-current","description":"Verify current branch"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2800,"cache_read_input_tokens":18004,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2800},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"2b576da1-316b-499a-ac0d-9d9d5cd3185c"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016KGuFf9718HJaviNUVXpdZ","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline","is_error":false}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"13723535-af89-48a0-9e0e-a60de7b31cd1","tool_use_result":{"stdout":"/Users/nikos/github-synced/scoringutils-pipeline","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01G3VNBPhpJ6GTC3ADTZwHH9","type":"tool_result","content":"ralph","is_error":false}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"461d27d5-0e00-4834-b81e-401b9f2ab89e","tool_use_result":{"stdout":"ralph","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01RgLQ3wYgrDRSUc2fB4kLx2","type":"message","role":"assistant","content":[{"type":"text","text":"Good — correct worktree and branch. Now let me fetch and merge main."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":250,"cache_read_input_tokens":20804,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":250},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"1a631c07-0ea1-401a-98f6-3ec24a9becfb"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01RgLQ3wYgrDRSUc2fB4kLx2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011eHgbngXwBLhF1SKsLF6G2","name":"Bash","input":{"command":"git fetch origin main && git merge origin/main --ff-only","description":"Update ralph branch with latest main"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":250,"cache_read_input_tokens":20804,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":250},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"a3dcd2e6-ba52-4e57-bd3b-ac731e216726"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011eHgbngXwBLhF1SKsLF6G2","type":"tool_result","content":"Already up to date.\nFrom https://github.com/epiforecasts/scoringutils\n * branch              main       -> FETCH_HEAD","is_error":false}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"651aec12-733c-4d9d-9b20-df08ef6302e0","tool_use_result":{"stdout":"Already up to date.\nFrom https://github.com/epiforecasts/scoringutils\n * branch              main       -> FETCH_HEAD","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01XmdR66v92HNaRa1UUaHbnC","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me read the triage table to find the next automatable issue without tests."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":151,"cache_read_input_tokens":21054,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":151},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"5d6298a9-bc4c-4437-a22e-0349f5114f40"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01XmdR66v92HNaRa1UUaHbnC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HD7HgVFCPxTkm8qEw9JHme","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":151,"cache_read_input_tokens":21054,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":151},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"b46a02e2-ddba-45f3-ac2e-c0b66a130550"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HD7HgVFCPxTkm8qEw9JHme","type":"tool_result","content":"     1→# scoringutils Issue Triage\n     2→\n     3→> Auto-generated by Ralph triage loop. Do not edit manually during the loop.\n     4→\n     5→## Summary\n     6→\n     7→| Metric | Count |\n     8→|--------|-------|\n     9→| Total open issues | 106 |\n    10→| Triaged | 106 |\n    11→| Remaining | 0 |\n    12→\n    13→## Pipeline\n    14→\n    15→- [ ] **Test design** — Test specs written for automatable issues\n    16→- [ ] **Implement** — Fixes implemented, PRs created\n    17→- [ ] **Review** — All PRs independently reviewed\n    18→- [ ] **Summary** — Human summary generated\n    19→\n    20→## Issues\n    21→\n    22→| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n    23→|---|-------|--------|--------|----------|----------|------------|-------------|-------------|--------------|----------------|-----------|-------------|-----|----------|\n    24→| 1065 | Release reminder - February 2026 | | 2026-02-02 | meta | medium | medium | partially | Automated reminder. Dev version (2.1.2.9000) has multivariate feature + bug fix accumulated over 161 days. No blocking bugs. Two open PRs (#1060, #1062) are infrastructure changes. | Maintainers should decide whether to merge #1060/#1062 before releasing. | Decide whether to cut a new release (likely v2.2.0 given multivariate feature). If yes, run `usethis::use_release_issue()` and proceed with CRAN submission. If not, close the issue. | N/A | N/A | — | N/A |\n    25→| 1064 | Pre-ranking for multivariate calibration assessment | | 2026-01-30 | feature | medium | large | no | No pre-ranking or multivariate calibration diagnostics exist in the codebase. The `forecast_sample_multivariate` class and energy score are implemented and stable. Design decision needed: integrate into `score()` (Option A) or standalone `get_multivariate_ranks()` function (Option B). MST method may need `igraph` dependency. | None blocking. Builds on existing `forecast_sample_multivariate` infrastructure. | Implement pre-ranking methods (multivariate rank, optionally MST rank) from Gneiting et al. (2008) as calibration diagnostics for multivariate forecasts, after resolving the API design choice with maintainers. | N/A | N/A | — | N/A |\n    26→| 1057 | Revise README to remove news about the package rewrite | documentation | 2026-01-29 | documentation | low | medium | partially | Open, no assignee. The rewrite messaging and Google Drive paper links appear in 4 locations in README.Rmd. Mechanical edits are straightforward but replacement text requires editorial decisions about where to point users for documentation. | Loosely related to #810 (pkgdown for dev docs) and #759 (future of scoringutils.Rmd vignette). | Edit `README.Rmd` to remove/soften the \"major re-write\" paragraph and replace Google Drive paper links with pointers to package vignettes/docs, then re-render README.md. Confirm preferred documentation hierarchy with maintainers first. | N/A | N/A | — | N/A |\n    27→| 1055 | Model confidence sets | feature request | 2026-01-19 | feature | medium | large | no | Design phase — methodology not yet agreed upon. 6 comments with active debate. Naive pairwise p-value approach acknowledged as incomplete. Three alternatives discussed: Hansen et al. MCS, novel extension for incomplete sets, Bayesian approach. No consensus reached. | Relates to #750 (p-value computation refactor), #769 (pairwise comparison arguments), #37 (Diebold-Mariano test), #1022 (two-model pairwise). | Resolve design consensus on which MCS methodology to implement (Hansen et al. on complete sets, novel incomplete-set extension, or Bayesian), then implement `get_model_confidence_set()` building on existing pairwise comparison infrastructure. | N/A | N/A | — | N/A |\n    28→| 1046 | multivariate scoring in scoring rules vignette | documentation | 2025-11-04 | documentation | low | small | partially | Open, agreed upon by maintainers. The scoring-rules vignette has zero mentions of multivariate scoring. A dedicated multivariate vignette already exists. Only a cross-reference and brief section are missing. | None. All underlying code and the dedicated multivariate vignette already exist. | Add a short section to `vignettes/scoring-rules.Rmd` describing the energy score for multivariate forecasts and cross-linking to the existing `scoring-multivariate-forecasts` vignette. No code changes needed. | N/A | N/A | — | N/A |\n    29→| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |\n    30→| 1043 | Update print method for multivariate forecasts to show `joint_across` | | 2025-09-21 | enhancement | low | small | yes | Still relevant. No `print.forecast_sample_multivariate` method exists; the generic `print.forecast` handles all types. `joint_across` is not stored as an attribute — `set_grouping()` computes `.mv_group_id` but discards the original `joint_across` value. | Builds on multivariate infrastructure from merged PR #998. | Add a `print.forecast_sample_multivariate()` method that displays `joint_across` columns. Requires either storing `joint_across` as an attribute during object construction or computing it as the set difference between forecast unit and `get_grouping()` output. Update test snapshots. | [x] | [ ] | — | [ ] |\n    31→| 1026 | as_forecast_sample.forecast_quantile | enhancement | 2025-07-13 | enhancement | medium | medium | partially | Still relevant. The reverse conversion (`as_forecast_quantile.forecast_sample`) exists, but `as_forecast_sample.forecast_quantile` does not. No distribution-fitting dependencies in DESCRIPTION. Author plans to borrow from hubverse approach. | Related to #557 (sample_to_quantile.numeric). May introduce new dependency (e.g., `distfromq`). | Implement `as_forecast_sample.forecast_quantile()` S3 method that fits a distribution or uses interpolation to approximate the predictive CDF from quantiles and draws samples. Requires design decision on approximation method and potential new package dependency. | N/A | N/A | — | N/A |\n    32→| 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |\n    33→| 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |\n    34→| 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |\n    35→| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |\n    36→| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [x] | [ ] | — | [ ] |\n    37→| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n    38→| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [x] | [ ] | — | [ ] |\n    39→| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n    40→| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n    41→| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n    42→| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n    43→| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n    44→| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n    45→| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n    46→| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n    47→| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n    48→| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [x] | [ ] | — | [ ] |\n    49→| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n    50→| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n    51→| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n    52→| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [x] | [ ] | — | [ ] |\n    53→| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n    54→| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n    55→| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n    56→| 880 | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()? | | 2024-07-30 | enhancement | low | small | yes | Still relevant. Function hard-codes `0.5` on two lines (107, 115) in `R/class-forecast-quantile.R`. Design decisions already resolved in PR #876 discussion: parameter name `quantile_level`, default `0.5`, passed via `...`. No work done since issue opened. | None. Self-contained enhancement. | Add a `quantile_level` parameter (default `0.5`) to `as_forecast_point.forecast_quantile()`, replacing the two hard-coded `0.5` references, then add tests and documentation. | [x] | [ ] | — | [ ] |\n    57→| 872 | Add a reference for CRPS decomposition approach. | | 2024-07-25 | documentation | low | small | partially | Still relevant. `crps_sample()` in `R/metrics-sample.R` has only a `scoringRules` package reference, not one for the decomposition approach. `wis()` already properly cites Bracher et al. 2021. Minor decision needed on which paper(s) to cite: Bracher et al. 2021 and/or Arnold et al. 2023. | Related to #870 (CRPS/WIS decomposition refactor). Could be addressed in same PR. | Add `@references` entries to `crps_sample()` roxygen block citing paper(s) formalizing the overprediction/underprediction/dispersion decomposition (likely Bracher et al. 2021 and/or Arnold et al. 2023). | N/A | N/A | — | N/A |\n    58→| 870 | Refactor CRPS and WIS decomposition | refactor | 2024-07-22 | refactor | low | medium | partially | Partially addressed. PR #985 (Feb 2025) simplified `crps_sample()` decomposition internally, but the structural duplication remains: `crps_sample()`, `wis()`, and `interval_score()` each independently implement `separate_results` branching, and 6 wrapper functions (3 sample + 3 quantile) repeat identical extraction patterns. | Related to #872 (CRPS decomposition reference). Filed during PR #854 review. On `scoringutils-2.x` milestone. | Extract the decomposition logic (`separate_results` branching and wrapper function boilerplate) into a shared internal helper or factory pattern so `crps_sample()`, `wis()`, and `interval_score()` delegate to the same mechanism instead of each implementing it independently. | N/A | N/A | — | N/A |\n    59→| 852 | Design workflow for creating new forecast classes | enhancement | 2024-07-10 | enhancement | high | large | no | Still relevant. The building blocks exist (`new_forecast()` exported, `as_forecast_generic()` internal, pattern visible in 7 class files) but no documentation, vignette, or guide for third-party class creation exists. Three prerequisite S3 refactors (#889, #888, #740) remain open and block clean extensibility because `get_protected_columns()` and `get_duplicate_forecasts()` hard-code column names, preventing new classes from working seamlessly. `as_forecast_generic()`, `apply_metrics()`, and `as_scores()` are still unexported (#885). | #889 (S3 `get_protected_columns`), #888 (S3 `get_duplicate_forecasts`), #740 (`get_forecast_unit` customization), #885 (export decision for helper functions). | Complete prerequisite S3 refactors (#889, #888), decide export list for class-creation helpers (#885), then write a vignette or article documenting the step-by-step workflow for creating a custom `forecast_*` class (define `as_forecast_<type>`, `assert_forecast.<type>`, `score.<type>`, `get_metrics.<type>`, `get_protected_columns.<type>`, `get_duplicate_forecasts.<type>`). | N/A | N/A | — | N/A |\n    60→| 840 | Clean up input checks | | 2024-06-07 | refactor | low | medium | partially | Still relevant. Input checks are applied inconsistently: `assert_forecast.forecast_binary` and `assert_forecast.forecast_point` call `check_input_*` wrappers (double-validating with scoring functions), while sample/quantile/ordinal/nominal types skip this. `check-input-helpers.R` has a comment marking `check_try()` and wrappers for future deletion. Author leans toward pushing all checks to the scoring-function level. No work done. | Related to #939 (remove `check_input_*` functions), #839, #922. On `scoringutils-2.x` milestone. | Decide where input validation should live (scoring-function level vs. `assert_forecast` level vs. both-consistently). Then either remove `check_input_binary`/`check_input_point` calls from `assert_forecast` methods and delete wrappers + `check_try()`, or add missing checks to sample/quantile `assert_forecast` methods for consistency. | N/A | N/A | — | N/A |\n    61→| 839 | Remove `test_columns_not_present` | | 2024-06-07 | cleanup | low | small | yes | Still relevant. `test_columns_not_present()` is a trivial 1-line wrapper around `checkmate::test_names(..., disjunct.from = ...)` with exactly 1 production call site (`R/class-forecast-binary.R:68`) and 1 test block. `checkmate::test_names` is already imported. Straightforward removal. | Related to #922 (remove `check_columns_present`) and #840 (clean up input checks). Can be batched but has no blocking dependency. | Replace the single call in `R/class-forecast-binary.R:68` with `test_names(colnames(forecast), disjunct.from = ...)`, delete the function definition in `R/check-input-helpers.R`, delete the test block, and remove the generated man page. Optionally bundle with #922. | [x] | [ ] | — | [ ] |\n    62→| 826 | Update `plot_heatmap()` to be a table instead of a plot | | 2024-05-19 | enhancement | low | medium | partially | Still relevant. Accessibility concern from @Bisaloo is valid: current ggplot2 heatmap renders values as image text, inaccessible to screen readers. Conversation stalled — Bisaloo never replied to follow-up about output format (HTML/LaTeX). `kableExtra` is already in `Suggests`. This is a breaking change: function returns `ggplot` object, used with `+ facet_wrap()` in `inst/manuscript/`. Not used in any current vignette. Design decision needed on migration strategy. | None blocking. Thematically adjacent to #713 (visualization vignette). | Resolve stalled design conversation on output format and migration strategy (deprecate + new function vs. breaking change). Implement colored-table version using `kableExtra` (already in Suggests) or `gt` (new dep), update manuscript, replace test/snapshot, and deprecate or replace `plot_heatmap()`. | N/A | N/A | — | N/A |\n    63→| 825 | Package scaffolding | | 2024-05-19 | refactor | low | small | yes | Still relevant. Neither recommendation acted upon: (1) `Metrics` package still imported for 3 trivial functions (`ae`, `se`, `ape`) used only in `R/class-forecast-point.R` — these are one-line base-R replacements. `Metrics` did receive a 2025 CRAN update, reducing urgency. (2) `matrixStats` not adopted; 3 `apply()` calls in `R/metrics-sample.R` could benefit. Suggest splitting into two sub-tasks, prioritizing `Metrics` removal as a quick win. | None blocking. Removing `Metrics` requires updating vignettes (`scoring-rules.Rmd`) and tests (`test-metrics-binary.R`) that reference `Metrics::` functions. | (1) Replace `Metrics::ae/se/ape` with internal one-liners in `R/class-forecast-point.R`, remove from DESCRIPTION/NAMESPACE, update vignettes and tests. (2) Optionally evaluate adding `matrixStats` for 3 `apply()` calls — lower value, separate decision. | [x] | [ ] | — | [ ] |\n    64→| 823 | Reduce number of `try()`, `suppressMessages()`, `suppressWarnings()` used in the package | | 2024-05-19 | refactor | medium | medium | no | Still relevant. 8 direct `try()` calls + 6 indirect via `check_try()` + 1 `suppressWarnings(suppressMessages(...))` in R/ code. Usage has slightly increased since filing due to new `[.forecast` subsetting operators (PR #884). Each `try()` exists for a reason; removal requires architectural rethinking of validation flow, not simple find-and-replace. | Deeply linked to #935 (print on column change), #840 (input check cleanup), #939 (remove `check_input_*`), #885 (export decisions), #811 (batch warnings). | Address incrementally: (1) Replace blanket `try()` in subsetting operators with targeted `tryCatch()`, (2) add `verbose` params to internal validation paths to eliminate `suppress*()`, (3) replace `check_try()` with proper `check_*()` functions as part of #840. | N/A | N/A | — | N/A |\n    65→| 811 | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? | enhancement | 2024-05-07 | enhancement | low | small | yes | Still relevant. `run_safely()` in `R/score.R:210` uses `try()` + immediate `cli_warn()` per failed metric. No batching/deferring mechanism exists. Failed metrics silently dropped from `scores` object via `as_scores()` filtering. Blog post on deferred errors referenced in issue. Only 1 comment from maintainer (\"I like this idea\"). | None blocking. Tangentially related to #823 (reduce `try()`) and #382 (metric/column name clash warnings). | Modify `apply_metrics()`/`run_safely()` to collect error messages in a list during the `lapply` loop, then emit a single batched `cli_warn()` summarizing all failed metrics at the end. Update test expectations in `test-score.R`. | [x] | [ ] | — | [ ] |\n    66→| 810 | Clean up readme by using `pkgdown` to render dev version docs | | 2024-05-05 | documentation | low | small | yes | CLOSED (completed 2026-02-09). PR #1060 merged today, replacing R-based `packageVersion()` detection with pkgdown selective HTML (`.pkgdown-devel`/`.pkgdown-release` fenced divs). `_pkgdown.yml` already had `development: mode: auto`. Dev docs deployed at `/dev/` on gh-pages. | None. Fully resolved. | No action needed — issue closed via merged PR #1060. | N/A | N/A | — | N/A |\n    67→| 808 | Run lintr everywhere | | 2024-05-05 | cleanup | low | medium | partially | Active work in progress. PR #1062 (\"enable lintr for tests, inst, and vignettes\") is open but not yet merged. `.lintr` excludes all `tests/`, `inst/`, and `vignettes/*.R` files. CI only lints changed files on PRs. Main scope is `tests/` (~5,400 lines across 35 files). `inst/` has only 2 R files. `vignettes/*.R` not tracked in git (auto-generated tangles). ~90 `nolint` annotations already exist in `R/` source. | #807 (cli nolint cleanup) should be done first to establish pattern. #513 (vignette .R scripts) related to vignette exclusion. PR #1062 is in flight. | Merge or rebase PR #1062 once reviews pass. May need to fix remaining lint violations or add targeted `# nolint` annotations. Address `tests/` directory-wide lint cleanup separately from `inst/` and `vignettes/`. | N/A | N/A | — | N/A |\n    68→| 807 | Clean up linter statements around `cli` conditions | good first issue, implementation-ready | 2024-05-05 | cleanup | low | medium | yes | Still relevant. ~43 `#nolint start: keyword_quote_linter` blocks across R/ wrap `cli_abort()`/`cli_warn()` calls that use `\"!\" =`, `\"i\" =` named vector syntax. Only 12 instances use Hugo's backtick fix (`\\`!\\` =`). The fix is mechanical: replace `\"!\" =` with `` `!` = `` inside cli condition calls, then remove the `nolint` annotations. Additional `object_usage_linter` suppressions (14 blocks) exist for `.data[...]` patterns in data.table code — these need separate handling. No work done on this issue since filing. | Prerequisite for #808 (run lintr everywhere). Related to PR #583 (which introduced the cli pattern). | Replace all `\"!\" =`, `\"i\" =`, `\"x\" =`, `\"v\" =` with backtick-quoted equivalents (`` `!` = ``, etc.) in `cli_abort()`/`cli_warn()` calls across ~20 R/ files, then delete the corresponding `#nolint start`/`#nolint end` blocks. Handle `object_usage_linter` suppressions separately. | [x] | [ ] | — | [ ] |\n    69→| 793 | How should we document methods that have different arguments than the default method? | | 2024-04-19 | documentation | low | small | yes | Still relevant. Consensus reached (separate pages per @seabbs), but not implemented. Three generics currently lump methods with differing arguments onto one `@rdname` page: `as_forecast_quantile` (`.forecast_sample` has different args), `as_forecast_point` (`.forecast_quantile`), and `get_pit_histogram` (`.forecast_sample` has extra args). Renders a confusing flat argument list. | None. | Remove shared `@rdname` tags for S3 methods whose arguments differ from the generic (`as_forecast_quantile.forecast_sample`, `as_forecast_point.forecast_quantile`, `get_pit_histogram.forecast_sample`) and give each its own documentation page with proper title and description. | [x] | [ ] | — | [ ] |\n    70→| 789 | Add documentation about how to customise scores | documentation | 2024-04-13 | documentation | medium | medium | partially | Partially addressed. The `score()` roxygen docs now have a \"Customising metrics\" section with `purrr::partial()` examples (PR #874). But the core ask — a vignette walkthrough — remains undone. No vignette covers `get_metrics()` → `select_metrics()` → `purrr::partial()` → `score()` workflow. On `scoringutils-2.x` milestone. | Related to #546 (vignette restructuring — decides where customization docs go), #666 (manuscript update). | Create a vignette section (or standalone vignette) walking users through the full metric customization workflow: `get_metrics()`, `select_metrics()`, `purrr::partial()`, custom metric functions, and passing to `score()`. Coordinate with vignette restructuring in #546. | N/A | N/A | — | N/A |\n    71→| 785 | Small updates to the scoring rules Vignette | documentation | 2024-04-10 | documentation | low | small | yes | Still relevant. None of the 7 suggestions from @seabbs's review of PR #762 have been implemented: author field still present, Google Drive link still used, no citations for proper scoring rules or transformed scores paper, `get_metrics` wording not improved for website readers, no introductory paragraph on different probabilistic forecast types. Nearly 2 years open with no progress on these items. | Loosely related to #666 (manuscript update re: Google Drive link), #758 (scoring rules vignette parent issue), #1046 (multivariate scoring in same vignette). | Implement the 7 small documentation edits in `vignettes/scoring-rules.Rmd`: remove author field, replace Google Drive link with proper reference, add citations for proper scoring rules and transformed scores, improve `get_metrics` wording, clean whitespace, and add intro paragraph on probabilistic forecast types. | [x] | [ ] | — | [ ] |\n    72→| 780 | Add a `verbose` argument to `as_forecast()` | | 2024-04-05 | enhancement | low | small | yes | Still relevant. `verbose` already exists on `assert_forecast()` and `assert_forecast_generic()` (PR #778) but has not been threaded through to any `as_forecast_<type>()` constructor. ~11 test lines and 1 internal R source line use `suppressMessages`/`suppressWarnings` wrappers around `as_forecast_*` calls, confirming ongoing need. Author acknowledged low urgency but wants it for test cleanup. | None. PR #778 (verbose on `assert_forecast`) already merged; plumbing is in place. | Add `verbose = TRUE` parameter to each `as_forecast_<type>.default()` method, pass it to their `assert_forecast()` calls, guard direct `cli_warn()`/`cli_inform()` calls behind the flag, and replace `suppressMessages`/`suppressWarnings` wrappers in tests and internal code with `verbose = FALSE`. | [x] | [ ] | — | [ ] |\n    73→| 769 | Rethink the way arguments are passed in `get_pairwise_comparisons()` | package improvement | 2024-04-03 | refactor | low | small | yes | Still relevant. `test_type`, `one_sided`, and `n_permutations` are still passed via `...` through 3 levels (`get_pairwise_comparisons` → `pairwise_comparison_one_group` → `compare_forecasts`). Users must read internal `compare_forecasts` docs to discover options. PR #994 added `test_type = NULL` but didn't address the `...` pattern. #750 (split out p-values) was deferred. | Loosely related to #750 (if p-values split out, this becomes moot — but #750 deferred indefinitely). No hard blockers. | Promote `test_type`, `one_sided`, and `n_permutations` from `...` to explicit named arguments in `get_pairwise_comparisons()` and `add_relative_skill()`, improving API discoverability. Update roxygen `@param` tags accordingly. | [x] | [ ] | — | [ ] |\n    74→| 763 | We should check for consistency when scoring binary forecasts | feature request, high-priority | 2024-03-30 | enhancement | medium | small | yes | Still relevant. No code warns when 0/1 factor levels are in counterintuitive order (e.g., `levels = c(1, 0)` making `0` the reference level). `assert_input_binary()` only validates factor with 2 levels and predicted in [0,1]. No helper for safe factor conversion exists. @seabbs questioned whether this is must-have for 2.0 or nice-to-have. | Loosely related to #840 (input check cleanup) and #939 (remove `check_input_*` functions). | Add a check in `assert_input_binary()` that warns when observed values are \"0\"/\"1\" and level order is `c(1, 0)`. Optionally create `as_binary_observed()` helper with a `reference_level` argument for safe factor conversion. | [x] | [ ] | — | [ ] |\n    75→| 759 | Future of the current `scoringutils.Rmd` vignette | documentation, question | 2024-03-27 | documentation | medium | medium | partially | Partially addressed. The outdated `scoringutils.Rmd` was removed in PR #836 (May 2024). However, the promised replacement \"Get started\" vignette was never written — @seabbs volunteered June 2024 but no PR materialized. Package has no introductory vignette. README also has dead links to deleted vignettes (`metric-details`, `scoring-forecasts-directly`). | Related to #546 (simplify getting started). Completing this would likely close #546. Dead README links should be fixed regardless. | Write a new \"Get started\" / quick-start vignette to replace the deleted `scoringutils.Rmd`. Fix dead vignette links in README.Rmd. Consider merging #546 into this issue. | N/A | N/A | — | N/A |\n    76→| 758 | Create a vignette that explains scoring rules | documentation | 2024-03-27 | documentation | medium | large | partially | Partially addressed. Part 1 (explain each scoring rule) is covered by the existing `scoring-rules.Rmd` vignette. Part 2 (practical guidance on choosing a scoring rule — convergence, overconfidence, locality, scale sensitivity) remains unaddressed. The issue body contains a detailed vignette stub. Three illustration PNGs exist in `vignettes/` but are orphaned (not referenced by any vignette). | Parent issue for #929, #930, #933, #934 (illustration issues). Related to #785 (scoring rules vignette updates), #1046 (multivariate content). | Narrow scope to creating a \"Choosing a scoring rule\" vignette covering comparative guidance (convergence, overconfidence, locality, scale sensitivity). Coordinate with #929/#930/#933/#934 to incorporate the orphaned illustrations. | N/A | N/A | — | N/A |\n    77→| 752 | Split up `plot_pit()` into a method for vectors and one for data.frames | | 2024-03-27 | refactor | low | small | yes | Fully resolved. `plot_pit()` was removed entirely in PRs #940 and #949. Replaced by `get_pit_histogram()` (S3 generic with methods for `forecast_sample` and `forecast_quantile`) and `pit_histogram_sample()` for low-level computation. Plotting is now via standard ggplot2 calls. The dual-purpose if/else concern no longer exists. | None. | Close as completed. The issue's concern was superseded by a complete removal of `plot_pit()` in favor of `get_pit_histogram()` with proper S3 dispatch. | N/A | N/A | — | N/A |\n    78→| 750 | Move computation of p-values out of `get_pairwise_comparisons()`? | question | 2024-03-26 | discussion | low | medium | no | Partially addressed. PR #994 added `test_type = NULL` to skip p-value computation, resolving the main pain point (spurious warnings from `add_relative_skill()`). The broader architectural question of separating p-value from score ratio computation remains open but contributor consensus was option 1 (do nothing). | #769 (explicit test_type argument) should be considered together. #1022 (two-model case) tangentially related. | Consider closing as won't-fix or converting to long-term design discussion. The practical problem was solved by PR #994. If kept open, the most impactful remaining change would be defaulting `add_relative_skill()` to `test_type = NULL` since it discards p-values anyway. | N/A | N/A | — | N/A |\n    79→| 740 | Make `get_forecast_unit()` and friends customisable such that they can work with new forecast classes | enhancement | 2024-03-24 | refactor | medium | medium | partially | Still relevant. No implementation work done. `get_protected_columns()` remains a plain function with hard-coded column lists (not S3). `get_duplicate_forecasts()` still hard-codes type-specific columns. Related S3 conversions (e.g., `get_forecast_type`, `get_pit`, `as_forecast`) establish the pattern. On `scoringutils-2.x` milestone. | Near-duplicate of #889 (S3 `get_protected_columns`) and #888 (S3 `get_duplicate_forecasts`). Blocks #852 (custom forecast class workflow). | Convert `get_protected_columns()` into an S3 generic with per-class methods (addressing #889), similarly make `get_duplicate_forecasts()` S3 (#888), so each forecast class declares its own columns. Follow established pattern from completed S3 conversions. | N/A | N/A | — | N/A |\n    80→| 731 | Should `wis()` warn about missing values? | question | 2024-03-23 | discussion | low | small | no | Still relevant. Current behavior is correct — `as_forecast()` emits a vague NA message, `clean_forecast()` strips NA rows before `wis()` sees them. Discussion converged on improving the `as_forecast()` message (#537) as the primary fix, with `wis()`-specific warning being secondary. No consensus reached on implementation. | Related to #537 (high-priority: improve NA-removal message). Related to #542 (debug `score()`). Resolving #537 first would clarify what remains here. | Resolve #537 first (improve NA-removal message in `as_forecast()` to report affected row count). Then decide whether `wis()` needs its own warning for direct (non-`score()` pipeline) usage. May be closeable once #537 is done. | N/A | N/A | — | N/A |\n    81→| 718 | Testing fails locally when done repeatedly | enhancement | 2024-03-20 | bug | low | small | yes | Still relevant and unfixed. Root cause: `cli_warn()` with `.frequency = \"once\"` and `.frequency_id = \"small_interval_range\"` suppresses warnings after first test run in same R session. 4 instances of `.frequency = \"once\"` in codebase. Tests pass on CI (fresh process) and on first local run. | No blocking dependencies. Tangentially related to #935 (surprising output behavior). | Add `rlang::reset_message_verbosity(\"small_interval_range\")` before the `expect_warning()` call in `test-metrics-interval-range.R`. Audit the other 3 `.frequency = \"once\"` sites to ensure their tests are similarly resilient. | [x] | [ ] | — | [ ] |\n    82→| 713 | Create vignette that showcases different ways to visualise forecasts and scores | documentation | 2024-03-11 | documentation | medium | medium | no | Still relevant. Package has 7 exported plot functions (`plot_wis`, `plot_heatmap`, `plot_interval_coverage`, `plot_quantile_coverage`, `plot_pairwise_comparisons`, `plot_forecast_counts`, `plot_correlations`) plus `theme_scoringutils()` but no dedicated visualization vignette. Only a deprecated-visualisations vignette exists. Issue requests updated `plot_score_table()` and mention of `hubVis`. | Related to #826 (plot_heatmap accessibility), #49 (calibration plotting), #942 (discrimination plots), #356 (interactions vignette). Does not block other issues. | Create a new `vignettes/visualising-scores.Rmd` with worked examples for all exported plot functions using example data. Include mention of `hubVis` as alternative for forecast visualization (per closed #812), and an updated `plot_score_table()` equivalent. | N/A | N/A | — | N/A |\n    83→| 702 | Update docs to conform to tidyverse styleguide | good first issue | 2024-03-08 | documentation | low | small | partially | Still relevant. ~85% of `@param` tags already conform to sentence case. ~21 clearly non-conforming (lowercase start) and ~21 special-character-starting descriptions across ~15 R files. Blocker PR #705 (file reorg) was merged March 2024. Assigned to @jamesmbaazam but no PR submitted. | No blocking dependencies. Standalone cleanup task. PR #705 dependency resolved. | Fix ~21 `@param` descriptions across ~15 R files to start with capital letter and end with full stop per tidyverse style guide. Also audit ~21 special-character-starting descriptions and all `@return`/`@returns` tags for same conformance. | N/A | N/A | — | N/A |\n    84→| 686 | Improve code coverage | | 2024-03-06 | meta | low | small | partially | Largely resolved. Coverage is at 98%. The specific sub-issue (#703, misnamed test file) was fixed in PR #704. Only `theme_scoringutils()` (18 lines) lacks any test coverage. The remaining 2% is edge-case branches. Could be closed as-is. | Sub-issue #703 resolved. No blockers. | Optionally add a test for `theme_scoringutils()` and a `.codecov.yml` with a coverage target to prevent regressions, then close this issue given the excellent 98% coverage level. | N/A | N/A | — | N/A |\n    85→| 685 | Suppress printing in tests | enhancement, implementation-ready | 2024-03-06 | cleanup | medium | medium | partially | Still relevant. 29 `suppressMessages()` and 16 `suppressWarnings()` remain across 13 test files, silently swallowing conditions instead of asserting them. Partially addressed: 80+ `expect_no_condition()` calls already exist. Root causes include `as_forecast_*()` verbose messaging and the #935 print bug. | Partially depends on #935 (unwanted printing on `:=`). Overlaps significantly with #823 (reduce `suppress*()` in package). | Audit all 29 `suppressMessages()` and 16 `suppressWarnings()` in tests: replace with `expect_message()`/`expect_warning()` for intentional conditions, pass `verbose = FALSE` where available, or use `expect_no_condition()` after fixing the source. Also review `setup.R` score calls. | N/A | N/A | — | N/A |\n    86→| 684 | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` | | 2024-03-06 | cleanup | low | small | yes | Still relevant but fully superseded by #939, which covers all 5 `check_input_*` functions. These 3 functions are dead code: never called in production, not exported, only tested. A comment in `check-input-helpers.R` explicitly marks them for future deletion. Should be closed as duplicate of #939. | Superseded by #939. Related to #840 (input check cleanup) and #823 (reduce `try()`). | Close as duplicate of #939. If handled independently, delete the 3 function definitions, their man pages, roxygen cross-references, and corresponding test blocks. | [x] | [ ] | — | [ ] |\n    87→| 668 | Add argument to control what kind of coverage is computed by `get_coverage()` | feature request | 2024-02-29 | enhancement | low | small | yes | Still relevant and unimplemented. `get_coverage()` always computes all 4 coverage metrics with no user control. The issue body contains a near-complete code proposal (with a minor bug to fix). The function structure already cleanly separates interval and quantile computation paths. On `scoringutils-2.x` milestone. | No blocking dependencies. Standalone enhancement. | Add a `type = c(\"quantile\", \"interval\")` argument to `get_coverage()` that conditionally computes interval coverage, quantile coverage, or both. Update downstream column handling, summarisation, and tests. | [x] | [ ] | — | [ ] |\n    88→| 667 | Create print method for `scores()` objects | feature request | 2024-02-29 | feature | low | small | partially | Still relevant but effectively blocked by #935. No `print.scores` method exists; scores objects fall through to `print.data.table`. Adding a custom print method would exacerbate the unwanted-printing-on-`:=` bug documented in #935. Both maintainers expressed lukewarm interest, deferring to post-release. | Blocked by #935 (print on column change bug). Related to #975 (link to score definitions in print). | Implement `print.scores()` S3 method displaying computed metric names (from `metrics` attribute) before delegating to `NextMethod()`, following the `print.forecast` pattern. Wait until #935 is resolved first to avoid spurious printing on `:=` operations. | N/A | N/A | — | N/A |\n    89→| 666 | Update the scoringutils paper that accompanies the package | question | 2024-02-29 | documentation | medium | large | no | Still relevant but partially stale. Only 2 of ~15 checklist items done. Manuscript in `inst/manuscript/` still references `magrittr`, CITATION still points to 2022 arXiv preprint. README links to Google Drive PDF. Some checklist items reference functions no longer exported. Should be broken into smaller actionable issues. | Related to #759, #789, #546, #758, #1057. | Audit the to-do list against current API to remove stale items, break remaining unchecked items into individual issues, and decide whether to prioritize the \"make a vignette out of the paper\" item before the next CRAN release. Requires human authorship decisions. | N/A | N/A | — | N/A |\n    90→| 646 | Stop converting errors to warnings in `score()`? | help wanted, question, Breaking change | 2024-02-23 | refactor | medium | medium | partially | Still relevant. `run_safely()` in `R/score.R` still converts all metric errors to warnings via `try()` + `cli_warn()` and returns `NULL`, silently dropping failed metrics. Deferred to `scoringutils-2.x` milestone. No consensus on approach: let errors propagate (breaking), add `on_error` parameter (compatible), or batch warnings (#811). | Related to #811 (batch warnings), #941 (`score()` redesign), #823 (reduce `try()`). | Design decision needed: (a) let errors propagate as errors, (b) add `on_error = c(\"warn\", \"error\")` param to `score()`, or (c) combine with #811's batch-warning approach. Implementation touches `run_safely()` in `R/score.R` and test expectations. | N/A | N/A | — | N/A |\n    91→| 625 | Implement a `get_unique_values()` function | implementation-ready | 2024-02-10 | enhancement | low | small | yes | Still relevant. No implementation work done. Function does not exist. Would compute `length(unique(values))` per forecast-unit column (via `get_forecast_unit()`), optionally grouped by model. Similar diagnostic purpose to `get_forecast_counts()` but different output. | Loosely related to #581 (NA in print), #935 (print bug), #621 (closed, prettier print). No hard blockers for standalone function. | Implement a new exported `get_unique_values()` function that takes a forecast object, calls `get_forecast_unit()` to identify relevant columns, and returns a summary table of unique value counts per column. Optionally integrate into `print.forecast()`. | [x] | [ ] | — | [ ] |\n    92→| 608 | Define input format for categorical forecasts | implementation-ready | 2024-01-17 | discussion | low | small | no | Largely resolved. Input format fully defined and implemented in `forecast_nominal` (PR #837) and `forecast_ordinal` (PR #977). Column named `predicted_label`. Strict completeness enforced. Only unimplemented item is a helper function to auto-expand incomplete forecasts with zero-probability rows, which has no dedicated tracking issue. Can be closed. | #604 (parent, closed), #607 (naming, closed). No blockers. | Close this issue as completed. Optionally open a new targeted issue for the helper function that auto-expands incomplete forecasts with zero-probability rows if still desired. | N/A | N/A | — | N/A |\n    93→| 581 | Add the number of `NA` values to the planned print method for `forecast_*` objects, rather than throw a message? | feature request | 2024-01-05 | enhancement | medium | medium | partially | Still relevant. None of the three requested features implemented: `print.forecast()` shows no NA info, NA message in `assert_forecast_generic()` is still vague (no row count), no `get_NA()` helper exists. Issue #537 is a near-duplicate and both should be addressed together. | Near-duplicate of #537 (should be resolved together). Tangentially related to #731, #542, #935 (print bug). | Enhance `print.forecast()` to display NA row count, improve the message in `assert_forecast_generic()` to report exact number of NA rows, and consider adding a `get_na_summary()` helper. Consolidate with #537. | N/A | N/A | — | N/A |\n    94→| 557 | sample_to_quantile.numeric method | low-priority | 2024-01-05 | enhancement | low | small | yes | Effectively resolved. `sample_to_quantile()` was replaced by `as_forecast_quantile.forecast_sample()` in PR #876. The motivating code (`interval_coverage_sample()`) was deleted entirely. No current code needs a `.numeric` method. Can be closed. | Tangentially related to #1026 (reverse conversion). No blockers. | Close this issue as resolved. The refactoring in PR #876 and deletion of `interval_coverage_sample()` eliminated the code pattern that motivated this request. | N/A | N/A | — | N/A |\n    95→| 546 | Simplify \"Getting Started\" \\| Split into smaller chunks | documentation | 2024-01-05 | documentation | medium | medium | no | Still relevant. The old `scoringutils.Rmd` was deleted (PR #836) but the promised replacement \"Get started\" vignette was never written. Package currently has no introductory vignette at all. README has a \"Quick start\" section but no pkgdown \"Get started\" page. @seabbs volunteered in #759 but no PR materialized. | Tightly coupled with #759 (future of scoringutils.Rmd) — should be resolved together. Related to #789 (metric customization docs). | Write a new introductory \"Getting Started\" vignette (as `vignettes/scoringutils.Rmd` for pkgdown) providing a concise end-to-end walkthrough. Can close both #546 and #759 simultaneously. Requires editorial decisions on scope and content. | N/A | N/A | — | N/A |\n    96→| 542 | Improve ways to debug `score()` | question | 2024-01-04 | enhancement | medium | medium | partially | Still relevant. Neither action item is done: (1) no vignette documents `debugonce()` for scoring rule debugging, (2) `run_safely()` still only reports the metric name in warnings, not which forecast unit/row caused the failure. The `{cli}` migration (PR #583) improved formatting but not informativeness. | Closely related to #646 (stop converting errors to warnings — open, breaking change). Related to closed #391, #418, #543, #583. | Two tasks remain: (1) add documentation showing `debugonce()` usage for debugging scoring failures, and (2) enhance `run_safely()`/`apply_metrics()` to include forecast-unit context in warning messages. Coordinate error-vs-warning design with #646. | N/A | N/A | — | N/A |\n    97→| 537 | Inform users about the number of rows that have NA values | good first issue, question, implementation-ready, high-priority | 2024-01-03 | enhancement | medium | medium | partially | Still relevant. The message in `assert_forecast_generic()` (line 150 of `R/class-forecast.R`) still says the generic \"Some rows containing NA values may be removed\" without a row count. No NA info in `print.forecast`, no `get_na()` helper. The `anyNA()` check and `na.omit()` call are already in place, so computing the count is trivial. | Near-duplicate of #581 (NA count in print method) — should be resolved together. Related to #731 (wis NA warnings). | Minimal fix: add NA row count to the existing `cli_inform` message in `assert_forecast_generic()`. For a more complete resolution, combine with #581 to also add NA summary to `print.forecast` and potentially create a `get_na()` diagnostic helper. | N/A | N/A | — | N/A |\n    98→| 522 | Create a way to test user-provided scoring functions | feature request | 2023-12-06 | feature | medium | medium | partially | Still relevant. No progress made. `validate_metrics()` only checks that items are functions (via `checkmate::check_function`), not signature compatibility, output shape, or runtime behavior. Custom metric errors are silently converted to warnings by `run_safely()`, making debugging hard. Each of the 6+ forecast types passes different arguments to metrics. | Loosely related to #362 (non-standard metric I/O), #823 (reduce `try()`). | Implement an exported `test_metrics()` function that runs each metric on a small representative subset of a forecast object and reports informative diagnostics: signature compatibility, runtime errors, and output shape validation. Needs design decision on API. | N/A | N/A | — | N/A |\n    99→| 520 | Create documentation/vignette for the naming scheme of our functions | help wanted, question | 2023-11-30 | documentation | low | medium | partially | Still relevant. The issue body already describes the naming scheme in detail but no vignette or formal documentation exists. CONTRIBUTING.md only references tidyverse style guide. Issue #948 (closed) debated `get_` prefix but decided to keep it. 65 exported functions follow conventions consistently. ~2 years old with no implementation. | Related to closed #948, #610, #476, #357. No blockers. | Create a developer-facing vignette or CONTRIBUTING.md section formalizing the naming conventions (scoring rules: `<name>_<type>`, `get_*`, `as_forecast_*`, `is_forecast_*`, `plot_*`, `add_*`, `assert_`/`check_`/`test_`). Update to reflect current classes including `forecast_nominal`, `forecast_ordinal`, `forecast_sample_multivariate`. | N/A | N/A | — | N/A |\n   100→| 513 | Investigate R scripts that get generated based on our vignettes | | 2023-11-28 | cleanup | low | small | yes | Already resolved. The `.R` and `.html` vignette artifacts (generated by `knitr` tangle) are gitignored via `vignettes/.gitignore`, excluded from linting via `.lintr`, and not tracked in git. All three mitigations from the discussion are in place. Issue can be closed. | None. | Close this issue — all requested investigation and mitigations (gitignore, buildignore, lint-ignore for vignette-generated `.R` files) are already implemented. No code changes needed. | N/A | N/A | — | N/A |\n   101→| 492 | Expose `get_range_from_quantile()` to the user | question | 2023-11-22 | enhancement | low | small | yes | Still relevant. Function exists at `R/helper-quantile-interval-range.R:185` as `@keywords internal`, not exported. Maintainer consensus reached: rename to `get_interval_range()` and create `add_interval_range()` wrapper. Neither implemented. Original motivating function `plot_ranges()` was deleted, reducing urgency. 3 internal call sites. | Parent meta-issue #484. Tangentially related to #885 (export decisions). | Rename `get_range_from_quantile()` to `get_interval_range()`, export it, and create a new exported `add_interval_range()` that adds an `interval_range` column to a quantile-format data.table. Update 3 internal call sites, NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |\n   102→| 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |\n   103→| 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |\n   104→| 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |\n   105→| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |\n   106→| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n   107→| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n   108→| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n   109→| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n   110→| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n   111→| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n   112→| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n   113→| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n   114→| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n   115→| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n   116→| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n   117→| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n   118→| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n   119→| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n   120→| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n   121→| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n   122→| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n   123→| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n   124→| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n   125→| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n   126→| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n   127→| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n   128→| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n   129→| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n   130→\n   131→TRIAGE_COMPLETE"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"24c35b68-86bf-40bf-a251-d0bb029573ac","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","content":"# scoringutils Issue Triage\n\n> Auto-generated by Ralph triage loop. Do not edit manually during the loop.\n\n## Summary\n\n| Metric | Count |\n|--------|-------|\n| Total open issues | 106 |\n| Triaged | 106 |\n| Remaining | 0 |\n\n## Pipeline\n\n- [ ] **Test design** — Test specs written for automatable issues\n- [ ] **Implement** — Fixes implemented, PRs created\n- [ ] **Review** — All PRs independently reviewed\n- [ ] **Summary** — Human summary generated\n\n## Issues\n\n| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n|---|-------|--------|--------|----------|----------|------------|-------------|-------------|--------------|----------------|-----------|-------------|-----|----------|\n| 1065 | Release reminder - February 2026 | | 2026-02-02 | meta | medium | medium | partially | Automated reminder. Dev version (2.1.2.9000) has multivariate feature + bug fix accumulated over 161 days. No blocking bugs. Two open PRs (#1060, #1062) are infrastructure changes. | Maintainers should decide whether to merge #1060/#1062 before releasing. | Decide whether to cut a new release (likely v2.2.0 given multivariate feature). If yes, run `usethis::use_release_issue()` and proceed with CRAN submission. If not, close the issue. | N/A | N/A | — | N/A |\n| 1064 | Pre-ranking for multivariate calibration assessment | | 2026-01-30 | feature | medium | large | no | No pre-ranking or multivariate calibration diagnostics exist in the codebase. The `forecast_sample_multivariate` class and energy score are implemented and stable. Design decision needed: integrate into `score()` (Option A) or standalone `get_multivariate_ranks()` function (Option B). MST method may need `igraph` dependency. | None blocking. Builds on existing `forecast_sample_multivariate` infrastructure. | Implement pre-ranking methods (multivariate rank, optionally MST rank) from Gneiting et al. (2008) as calibration diagnostics for multivariate forecasts, after resolving the API design choice with maintainers. | N/A | N/A | — | N/A |\n| 1057 | Revise README to remove news about the package rewrite | documentation | 2026-01-29 | documentation | low | medium | partially | Open, no assignee. The rewrite messaging and Google Drive paper links appear in 4 locations in README.Rmd. Mechanical edits are straightforward but replacement text requires editorial decisions about where to point users for documentation. | Loosely related to #810 (pkgdown for dev docs) and #759 (future of scoringutils.Rmd vignette). | Edit `README.Rmd` to remove/soften the \"major re-write\" paragraph and replace Google Drive paper links with pointers to package vignettes/docs, then re-render README.md. Confirm preferred documentation hierarchy with maintainers first. | N/A | N/A | — | N/A |\n| 1055 | Model confidence sets | feature request | 2026-01-19 | feature | medium | large | no | Design phase — methodology not yet agreed upon. 6 comments with active debate. Naive pairwise p-value approach acknowledged as incomplete. Three alternatives discussed: Hansen et al. MCS, novel extension for incomplete sets, Bayesian approach. No consensus reached. | Relates to #750 (p-value computation refactor), #769 (pairwise comparison arguments), #37 (Diebold-Mariano test), #1022 (two-model pairwise). | Resolve design consensus on which MCS methodology to implement (Hansen et al. on complete sets, novel incomplete-set extension, or Bayesian), then implement `get_model_confidence_set()` building on existing pairwise comparison infrastructure. | N/A | N/A | — | N/A |\n| 1046 | multivariate scoring in scoring rules vignette | documentation | 2025-11-04 | documentation | low | small | partially | Open, agreed upon by maintainers. The scoring-rules vignette has zero mentions of multivariate scoring. A dedicated multivariate vignette already exists. Only a cross-reference and brief section are missing. | None. All underlying code and the dedicated multivariate vignette already exist. | Add a short section to `vignettes/scoring-rules.Rmd` describing the energy score for multivariate forecasts and cross-linking to the existing `scoring-multivariate-forecasts` vignette. No code changes needed. | N/A | N/A | — | N/A |\n| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |\n| 1043 | Update print method for multivariate forecasts to show `joint_across` | | 2025-09-21 | enhancement | low | small | yes | Still relevant. No `print.forecast_sample_multivariate` method exists; the generic `print.forecast` handles all types. `joint_across` is not stored as an attribute — `set_grouping()` computes `.mv_group_id` but discards the original `joint_across` value. | Builds on multivariate infrastructure from merged PR #998. | Add a `print.forecast_sample_multivariate()` method that displays `joint_across` columns. Requires either storing `joint_across` as an attribute during object construction or computing it as the set difference between forecast unit and `get_grouping()` output. Update test snapshots. | [x] | [ ] | — | [ ] |\n| 1026 | as_forecast_sample.forecast_quantile | enhancement | 2025-07-13 | enhancement | medium | medium | partially | Still relevant. The reverse conversion (`as_forecast_quantile.forecast_sample`) exists, but `as_forecast_sample.forecast_quantile` does not. No distribution-fitting dependencies in DESCRIPTION. Author plans to borrow from hubverse approach. | Related to #557 (sample_to_quantile.numeric). May introduce new dependency (e.g., `distfromq`). | Implement `as_forecast_sample.forecast_quantile()` S3 method that fits a distribution or uses interpolation to approximate the predictive CDF from quantiles and draws samples. Requires design decision on approximation method and potential new package dependency. | N/A | N/A | — | N/A |\n| 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |\n| 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |\n| 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |\n| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |\n| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [x] | [ ] | — | [ ] |\n| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [x] | [ ] | — | [ ] |\n| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [x] | [ ] | — | [ ] |\n| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [x] | [ ] | — | [ ] |\n| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n| 880 | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()? | | 2024-07-30 | enhancement | low | small | yes | Still relevant. Function hard-codes `0.5` on two lines (107, 115) in `R/class-forecast-quantile.R`. Design decisions already resolved in PR #876 discussion: parameter name `quantile_level`, default `0.5`, passed via `...`. No work done since issue opened. | None. Self-contained enhancement. | Add a `quantile_level` parameter (default `0.5`) to `as_forecast_point.forecast_quantile()`, replacing the two hard-coded `0.5` references, then add tests and documentation. | [x] | [ ] | — | [ ] |\n| 872 | Add a reference for CRPS decomposition approach. | | 2024-07-25 | documentation | low | small | partially | Still relevant. `crps_sample()` in `R/metrics-sample.R` has only a `scoringRules` package reference, not one for the decomposition approach. `wis()` already properly cites Bracher et al. 2021. Minor decision needed on which paper(s) to cite: Bracher et al. 2021 and/or Arnold et al. 2023. | Related to #870 (CRPS/WIS decomposition refactor). Could be addressed in same PR. | Add `@references` entries to `crps_sample()` roxygen block citing paper(s) formalizing the overprediction/underprediction/dispersion decomposition (likely Bracher et al. 2021 and/or Arnold et al. 2023). | N/A | N/A | — | N/A |\n| 870 | Refactor CRPS and WIS decomposition | refactor | 2024-07-22 | refactor | low | medium | partially | Partially addressed. PR #985 (Feb 2025) simplified `crps_sample()` decomposition internally, but the structural duplication remains: `crps_sample()`, `wis()`, and `interval_score()` each independently implement `separate_results` branching, and 6 wrapper functions (3 sample + 3 quantile) repeat identical extraction patterns. | Related to #872 (CRPS decomposition reference). Filed during PR #854 review. On `scoringutils-2.x` milestone. | Extract the decomposition logic (`separate_results` branching and wrapper function boilerplate) into a shared internal helper or factory pattern so `crps_sample()`, `wis()`, and `interval_score()` delegate to the same mechanism instead of each implementing it independently. | N/A | N/A | — | N/A |\n| 852 | Design workflow for creating new forecast classes | enhancement | 2024-07-10 | enhancement | high | large | no | Still relevant. The building blocks exist (`new_forecast()` exported, `as_forecast_generic()` internal, pattern visible in 7 class files) but no documentation, vignette, or guide for third-party class creation exists. Three prerequisite S3 refactors (#889, #888, #740) remain open and block clean extensibility because `get_protected_columns()` and `get_duplicate_forecasts()` hard-code column names, preventing new classes from working seamlessly. `as_forecast_generic()`, `apply_metrics()`, and `as_scores()` are still unexported (#885). | #889 (S3 `get_protected_columns`), #888 (S3 `get_duplicate_forecasts`), #740 (`get_forecast_unit` customization), #885 (export decision for helper functions). | Complete prerequisite S3 refactors (#889, #888), decide export list for class-creation helpers (#885), then write a vignette or article documenting the step-by-step workflow for creating a custom `forecast_*` class (define `as_forecast_<type>`, `assert_forecast.<type>`, `score.<type>`, `get_metrics.<type>`, `get_protected_columns.<type>`, `get_duplicate_forecasts.<type>`). | N/A | N/A | — | N/A |\n| 840 | Clean up input checks | | 2024-06-07 | refactor | low | medium | partially | Still relevant. Input checks are applied inconsistently: `assert_forecast.forecast_binary` and `assert_forecast.forecast_point` call `check_input_*` wrappers (double-validating with scoring functions), while sample/quantile/ordinal/nominal types skip this. `check-input-helpers.R` has a comment marking `check_try()` and wrappers for future deletion. Author leans toward pushing all checks to the scoring-function level. No work done. | Related to #939 (remove `check_input_*` functions), #839, #922. On `scoringutils-2.x` milestone. | Decide where input validation should live (scoring-function level vs. `assert_forecast` level vs. both-consistently). Then either remove `check_input_binary`/`check_input_point` calls from `assert_forecast` methods and delete wrappers + `check_try()`, or add missing checks to sample/quantile `assert_forecast` methods for consistency. | N/A | N/A | — | N/A |\n| 839 | Remove `test_columns_not_present` | | 2024-06-07 | cleanup | low | small | yes | Still relevant. `test_columns_not_present()` is a trivial 1-line wrapper around `checkmate::test_names(..., disjunct.from = ...)` with exactly 1 production call site (`R/class-forecast-binary.R:68`) and 1 test block. `checkmate::test_names` is already imported. Straightforward removal. | Related to #922 (remove `check_columns_present`) and #840 (clean up input checks). Can be batched but has no blocking dependency. | Replace the single call in `R/class-forecast-binary.R:68` with `test_names(colnames(forecast), disjunct.from = ...)`, delete the function definition in `R/check-input-helpers.R`, delete the test block, and remove the generated man page. Optionally bundle with #922. | [x] | [ ] | — | [ ] |\n| 826 | Update `plot_heatmap()` to be a table instead of a plot | | 2024-05-19 | enhancement | low | medium | partially | Still relevant. Accessibility concern from @Bisaloo is valid: current ggplot2 heatmap renders values as image text, inaccessible to screen readers. Conversation stalled — Bisaloo never replied to follow-up about output format (HTML/LaTeX). `kableExtra` is already in `Suggests`. This is a breaking change: function returns `ggplot` object, used with `+ facet_wrap()` in `inst/manuscript/`. Not used in any current vignette. Design decision needed on migration strategy. | None blocking. Thematically adjacent to #713 (visualization vignette). | Resolve stalled design conversation on output format and migration strategy (deprecate + new function vs. breaking change). Implement colored-table version using `kableExtra` (already in Suggests) or `gt` (new dep), update manuscript, replace test/snapshot, and deprecate or replace `plot_heatmap()`. | N/A | N/A | — | N/A |\n| 825 | Package scaffolding | | 2024-05-19 | refactor | low | small | yes | Still relevant. Neither recommendation acted upon: (1) `Metrics` package still imported for 3 trivial functions (`ae`, `se`, `ape`) used only in `R/class-forecast-point.R` — these are one-line base-R replacements. `Metrics` did receive a 2025 CRAN update, reducing urgency. (2) `matrixStats` not adopted; 3 `apply()` calls in `R/metrics-sample.R` could benefit. Suggest splitting into two sub-tasks, prioritizing `Metrics` removal as a quick win. | None blocking. Removing `Metrics` requires updating vignettes (`scoring-rules.Rmd`) and tests (`test-metrics-binary.R`) that reference `Metrics::` functions. | (1) Replace `Metrics::ae/se/ape` with internal one-liners in `R/class-forecast-point.R`, remove from DESCRIPTION/NAMESPACE, update vignettes and tests. (2) Optionally evaluate adding `matrixStats` for 3 `apply()` calls — lower value, separate decision. | [x] | [ ] | — | [ ] |\n| 823 | Reduce number of `try()`, `suppressMessages()`, `suppressWarnings()` used in the package | | 2024-05-19 | refactor | medium | medium | no | Still relevant. 8 direct `try()` calls + 6 indirect via `check_try()` + 1 `suppressWarnings(suppressMessages(...))` in R/ code. Usage has slightly increased since filing due to new `[.forecast` subsetting operators (PR #884). Each `try()` exists for a reason; removal requires architectural rethinking of validation flow, not simple find-and-replace. | Deeply linked to #935 (print on column change), #840 (input check cleanup), #939 (remove `check_input_*`), #885 (export decisions), #811 (batch warnings). | Address incrementally: (1) Replace blanket `try()` in subsetting operators with targeted `tryCatch()`, (2) add `verbose` params to internal validation paths to eliminate `suppress*()`, (3) replace `check_try()` with proper `check_*()` functions as part of #840. | N/A | N/A | — | N/A |\n| 811 | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? | enhancement | 2024-05-07 | enhancement | low | small | yes | Still relevant. `run_safely()` in `R/score.R:210` uses `try()` + immediate `cli_warn()` per failed metric. No batching/deferring mechanism exists. Failed metrics silently dropped from `scores` object via `as_scores()` filtering. Blog post on deferred errors referenced in issue. Only 1 comment from maintainer (\"I like this idea\"). | None blocking. Tangentially related to #823 (reduce `try()`) and #382 (metric/column name clash warnings). | Modify `apply_metrics()`/`run_safely()` to collect error messages in a list during the `lapply` loop, then emit a single batched `cli_warn()` summarizing all failed metrics at the end. Update test expectations in `test-score.R`. | [x] | [ ] | — | [ ] |\n| 810 | Clean up readme by using `pkgdown` to render dev version docs | | 2024-05-05 | documentation | low | small | yes | CLOSED (completed 2026-02-09). PR #1060 merged today, replacing R-based `packageVersion()` detection with pkgdown selective HTML (`.pkgdown-devel`/`.pkgdown-release` fenced divs). `_pkgdown.yml` already had `development: mode: auto`. Dev docs deployed at `/dev/` on gh-pages. | None. Fully resolved. | No action needed — issue closed via merged PR #1060. | N/A | N/A | — | N/A |\n| 808 | Run lintr everywhere | | 2024-05-05 | cleanup | low | medium | partially | Active work in progress. PR #1062 (\"enable lintr for tests, inst, and vignettes\") is open but not yet merged. `.lintr` excludes all `tests/`, `inst/`, and `vignettes/*.R` files. CI only lints changed files on PRs. Main scope is `tests/` (~5,400 lines across 35 files). `inst/` has only 2 R files. `vignettes/*.R` not tracked in git (auto-generated tangles). ~90 `nolint` annotations already exist in `R/` source. | #807 (cli nolint cleanup) should be done first to establish pattern. #513 (vignette .R scripts) related to vignette exclusion. PR #1062 is in flight. | Merge or rebase PR #1062 once reviews pass. May need to fix remaining lint violations or add targeted `# nolint` annotations. Address `tests/` directory-wide lint cleanup separately from `inst/` and `vignettes/`. | N/A | N/A | — | N/A |\n| 807 | Clean up linter statements around `cli` conditions | good first issue, implementation-ready | 2024-05-05 | cleanup | low | medium | yes | Still relevant. ~43 `#nolint start: keyword_quote_linter` blocks across R/ wrap `cli_abort()`/`cli_warn()` calls that use `\"!\" =`, `\"i\" =` named vector syntax. Only 12 instances use Hugo's backtick fix (`\\`!\\` =`). The fix is mechanical: replace `\"!\" =` with `` `!` = `` inside cli condition calls, then remove the `nolint` annotations. Additional `object_usage_linter` suppressions (14 blocks) exist for `.data[...]` patterns in data.table code — these need separate handling. No work done on this issue since filing. | Prerequisite for #808 (run lintr everywhere). Related to PR #583 (which introduced the cli pattern). | Replace all `\"!\" =`, `\"i\" =`, `\"x\" =`, `\"v\" =` with backtick-quoted equivalents (`` `!` = ``, etc.) in `cli_abort()`/`cli_warn()` calls across ~20 R/ files, then delete the corresponding `#nolint start`/`#nolint end` blocks. Handle `object_usage_linter` suppressions separately. | [x] | [ ] | — | [ ] |\n| 793 | How should we document methods that have different arguments than the default method? | | 2024-04-19 | documentation | low | small | yes | Still relevant. Consensus reached (separate pages per @seabbs), but not implemented. Three generics currently lump methods with differing arguments onto one `@rdname` page: `as_forecast_quantile` (`.forecast_sample` has different args), `as_forecast_point` (`.forecast_quantile`), and `get_pit_histogram` (`.forecast_sample` has extra args). Renders a confusing flat argument list. | None. | Remove shared `@rdname` tags for S3 methods whose arguments differ from the generic (`as_forecast_quantile.forecast_sample`, `as_forecast_point.forecast_quantile`, `get_pit_histogram.forecast_sample`) and give each its own documentation page with proper title and description. | [x] | [ ] | — | [ ] |\n| 789 | Add documentation about how to customise scores | documentation | 2024-04-13 | documentation | medium | medium | partially | Partially addressed. The `score()` roxygen docs now have a \"Customising metrics\" section with `purrr::partial()` examples (PR #874). But the core ask — a vignette walkthrough — remains undone. No vignette covers `get_metrics()` → `select_metrics()` → `purrr::partial()` → `score()` workflow. On `scoringutils-2.x` milestone. | Related to #546 (vignette restructuring — decides where customization docs go), #666 (manuscript update). | Create a vignette section (or standalone vignette) walking users through the full metric customization workflow: `get_metrics()`, `select_metrics()`, `purrr::partial()`, custom metric functions, and passing to `score()`. Coordinate with vignette restructuring in #546. | N/A | N/A | — | N/A |\n| 785 | Small updates to the scoring rules Vignette | documentation | 2024-04-10 | documentation | low | small | yes | Still relevant. None of the 7 suggestions from @seabbs's review of PR #762 have been implemented: author field still present, Google Drive link still used, no citations for proper scoring rules or transformed scores paper, `get_metrics` wording not improved for website readers, no introductory paragraph on different probabilistic forecast types. Nearly 2 years open with no progress on these items. | Loosely related to #666 (manuscript update re: Google Drive link), #758 (scoring rules vignette parent issue), #1046 (multivariate scoring in same vignette). | Implement the 7 small documentation edits in `vignettes/scoring-rules.Rmd`: remove author field, replace Google Drive link with proper reference, add citations for proper scoring rules and transformed scores, improve `get_metrics` wording, clean whitespace, and add intro paragraph on probabilistic forecast types. | [x] | [ ] | — | [ ] |\n| 780 | Add a `verbose` argument to `as_forecast()` | | 2024-04-05 | enhancement | low | small | yes | Still relevant. `verbose` already exists on `assert_forecast()` and `assert_forecast_generic()` (PR #778) but has not been threaded through to any `as_forecast_<type>()` constructor. ~11 test lines and 1 internal R source line use `suppressMessages`/`suppressWarnings` wrappers around `as_forecast_*` calls, confirming ongoing need. Author acknowledged low urgency but wants it for test cleanup. | None. PR #778 (verbose on `assert_forecast`) already merged; plumbing is in place. | Add `verbose = TRUE` parameter to each `as_forecast_<type>.default()` method, pass it to their `assert_forecast()` calls, guard direct `cli_warn()`/`cli_inform()` calls behind the flag, and replace `suppressMessages`/`suppressWarnings` wrappers in tests and internal code with `verbose = FALSE`. | [x] | [ ] | — | [ ] |\n| 769 | Rethink the way arguments are passed in `get_pairwise_comparisons()` | package improvement | 2024-04-03 | refactor | low | small | yes | Still relevant. `test_type`, `one_sided`, and `n_permutations` are still passed via `...` through 3 levels (`get_pairwise_comparisons` → `pairwise_comparison_one_group` → `compare_forecasts`). Users must read internal `compare_forecasts` docs to discover options. PR #994 added `test_type = NULL` but didn't address the `...` pattern. #750 (split out p-values) was deferred. | Loosely related to #750 (if p-values split out, this becomes moot — but #750 deferred indefinitely). No hard blockers. | Promote `test_type`, `one_sided`, and `n_permutations` from `...` to explicit named arguments in `get_pairwise_comparisons()` and `add_relative_skill()`, improving API discoverability. Update roxygen `@param` tags accordingly. | [x] | [ ] | — | [ ] |\n| 763 | We should check for consistency when scoring binary forecasts | feature request, high-priority | 2024-03-30 | enhancement | medium | small | yes | Still relevant. No code warns when 0/1 factor levels are in counterintuitive order (e.g., `levels = c(1, 0)` making `0` the reference level). `assert_input_binary()` only validates factor with 2 levels and predicted in [0,1]. No helper for safe factor conversion exists. @seabbs questioned whether this is must-have for 2.0 or nice-to-have. | Loosely related to #840 (input check cleanup) and #939 (remove `check_input_*` functions). | Add a check in `assert_input_binary()` that warns when observed values are \"0\"/\"1\" and level order is `c(1, 0)`. Optionally create `as_binary_observed()` helper with a `reference_level` argument for safe factor conversion. | [x] | [ ] | — | [ ] |\n| 759 | Future of the current `scoringutils.Rmd` vignette | documentation, question | 2024-03-27 | documentation | medium | medium | partially | Partially addressed. The outdated `scoringutils.Rmd` was removed in PR #836 (May 2024). However, the promised replacement \"Get started\" vignette was never written — @seabbs volunteered June 2024 but no PR materialized. Package has no introductory vignette. README also has dead links to deleted vignettes (`metric-details`, `scoring-forecasts-directly`). | Related to #546 (simplify getting started). Completing this would likely close #546. Dead README links should be fixed regardless. | Write a new \"Get started\" / quick-start vignette to replace the deleted `scoringutils.Rmd`. Fix dead vignette links in README.Rmd. Consider merging #546 into this issue. | N/A | N/A | — | N/A |\n| 758 | Create a vignette that explains scoring rules | documentation | 2024-03-27 | documentation | medium | large | partially | Partially addressed. Part 1 (explain each scoring rule) is covered by the existing `scoring-rules.Rmd` vignette. Part 2 (practical guidance on choosing a scoring rule — convergence, overconfidence, locality, scale sensitivity) remains unaddressed. The issue body contains a detailed vignette stub. Three illustration PNGs exist in `vignettes/` but are orphaned (not referenced by any vignette). | Parent issue for #929, #930, #933, #934 (illustration issues). Related to #785 (scoring rules vignette updates), #1046 (multivariate content). | Narrow scope to creating a \"Choosing a scoring rule\" vignette covering comparative guidance (convergence, overconfidence, locality, scale sensitivity). Coordinate with #929/#930/#933/#934 to incorporate the orphaned illustrations. | N/A | N/A | — | N/A |\n| 752 | Split up `plot_pit()` into a method for vectors and one for data.frames | | 2024-03-27 | refactor | low | small | yes | Fully resolved. `plot_pit()` was removed entirely in PRs #940 and #949. Replaced by `get_pit_histogram()` (S3 generic with methods for `forecast_sample` and `forecast_quantile`) and `pit_histogram_sample()` for low-level computation. Plotting is now via standard ggplot2 calls. The dual-purpose if/else concern no longer exists. | None. | Close as completed. The issue's concern was superseded by a complete removal of `plot_pit()` in favor of `get_pit_histogram()` with proper S3 dispatch. | N/A | N/A | — | N/A |\n| 750 | Move computation of p-values out of `get_pairwise_comparisons()`? | question | 2024-03-26 | discussion | low | medium | no | Partially addressed. PR #994 added `test_type = NULL` to skip p-value computation, resolving the main pain point (spurious warnings from `add_relative_skill()`). The broader architectural question of separating p-value from score ratio computation remains open but contributor consensus was option 1 (do nothing). | #769 (explicit test_type argument) should be considered together. #1022 (two-model case) tangentially related. | Consider closing as won't-fix or converting to long-term design discussion. The practical problem was solved by PR #994. If kept open, the most impactful remaining change would be defaulting `add_relative_skill()` to `test_type = NULL` since it discards p-values anyway. | N/A | N/A | — | N/A |\n| 740 | Make `get_forecast_unit()` and friends customisable such that they can work with new forecast classes | enhancement | 2024-03-24 | refactor | medium | medium | partially | Still relevant. No implementation work done. `get_protected_columns()` remains a plain function with hard-coded column lists (not S3). `get_duplicate_forecasts()` still hard-codes type-specific columns. Related S3 conversions (e.g., `get_forecast_type`, `get_pit`, `as_forecast`) establish the pattern. On `scoringutils-2.x` milestone. | Near-duplicate of #889 (S3 `get_protected_columns`) and #888 (S3 `get_duplicate_forecasts`). Blocks #852 (custom forecast class workflow). | Convert `get_protected_columns()` into an S3 generic with per-class methods (addressing #889), similarly make `get_duplicate_forecasts()` S3 (#888), so each forecast class declares its own columns. Follow established pattern from completed S3 conversions. | N/A | N/A | — | N/A |\n| 731 | Should `wis()` warn about missing values? | question | 2024-03-23 | discussion | low | small | no | Still relevant. Current behavior is correct — `as_forecast()` emits a vague NA message, `clean_forecast()` strips NA rows before `wis()` sees them. Discussion converged on improving the `as_forecast()` message (#537) as the primary fix, with `wis()`-specific warning being secondary. No consensus reached on implementation. | Related to #537 (high-priority: improve NA-removal message). Related to #542 (debug `score()`). Resolving #537 first would clarify what remains here. | Resolve #537 first (improve NA-removal message in `as_forecast()` to report affected row count). Then decide whether `wis()` needs its own warning for direct (non-`score()` pipeline) usage. May be closeable once #537 is done. | N/A | N/A | — | N/A |\n| 718 | Testing fails locally when done repeatedly | enhancement | 2024-03-20 | bug | low | small | yes | Still relevant and unfixed. Root cause: `cli_warn()` with `.frequency = \"once\"` and `.frequency_id = \"small_interval_range\"` suppresses warnings after first test run in same R session. 4 instances of `.frequency = \"once\"` in codebase. Tests pass on CI (fresh process) and on first local run. | No blocking dependencies. Tangentially related to #935 (surprising output behavior). | Add `rlang::reset_message_verbosity(\"small_interval_range\")` before the `expect_warning()` call in `test-metrics-interval-range.R`. Audit the other 3 `.frequency = \"once\"` sites to ensure their tests are similarly resilient. | [x] | [ ] | — | [ ] |\n| 713 | Create vignette that showcases different ways to visualise forecasts and scores | documentation | 2024-03-11 | documentation | medium | medium | no | Still relevant. Package has 7 exported plot functions (`plot_wis`, `plot_heatmap`, `plot_interval_coverage`, `plot_quantile_coverage`, `plot_pairwise_comparisons`, `plot_forecast_counts`, `plot_correlations`) plus `theme_scoringutils()` but no dedicated visualization vignette. Only a deprecated-visualisations vignette exists. Issue requests updated `plot_score_table()` and mention of `hubVis`. | Related to #826 (plot_heatmap accessibility), #49 (calibration plotting), #942 (discrimination plots), #356 (interactions vignette). Does not block other issues. | Create a new `vignettes/visualising-scores.Rmd` with worked examples for all exported plot functions using example data. Include mention of `hubVis` as alternative for forecast visualization (per closed #812), and an updated `plot_score_table()` equivalent. | N/A | N/A | — | N/A |\n| 702 | Update docs to conform to tidyverse styleguide | good first issue | 2024-03-08 | documentation | low | small | partially | Still relevant. ~85% of `@param` tags already conform to sentence case. ~21 clearly non-conforming (lowercase start) and ~21 special-character-starting descriptions across ~15 R files. Blocker PR #705 (file reorg) was merged March 2024. Assigned to @jamesmbaazam but no PR submitted. | No blocking dependencies. Standalone cleanup task. PR #705 dependency resolved. | Fix ~21 `@param` descriptions across ~15 R files to start with capital letter and end with full stop per tidyverse style guide. Also audit ~21 special-character-starting descriptions and all `@return`/`@returns` tags for same conformance. | N/A | N/A | — | N/A |\n| 686 | Improve code coverage | | 2024-03-06 | meta | low | small | partially | Largely resolved. Coverage is at 98%. The specific sub-issue (#703, misnamed test file) was fixed in PR #704. Only `theme_scoringutils()` (18 lines) lacks any test coverage. The remaining 2% is edge-case branches. Could be closed as-is. | Sub-issue #703 resolved. No blockers. | Optionally add a test for `theme_scoringutils()` and a `.codecov.yml` with a coverage target to prevent regressions, then close this issue given the excellent 98% coverage level. | N/A | N/A | — | N/A |\n| 685 | Suppress printing in tests | enhancement, implementation-ready | 2024-03-06 | cleanup | medium | medium | partially | Still relevant. 29 `suppressMessages()` and 16 `suppressWarnings()` remain across 13 test files, silently swallowing conditions instead of asserting them. Partially addressed: 80+ `expect_no_condition()` calls already exist. Root causes include `as_forecast_*()` verbose messaging and the #935 print bug. | Partially depends on #935 (unwanted printing on `:=`). Overlaps significantly with #823 (reduce `suppress*()` in package). | Audit all 29 `suppressMessages()` and 16 `suppressWarnings()` in tests: replace with `expect_message()`/`expect_warning()` for intentional conditions, pass `verbose = FALSE` where available, or use `expect_no_condition()` after fixing the source. Also review `setup.R` score calls. | N/A | N/A | — | N/A |\n| 684 | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` | | 2024-03-06 | cleanup | low | small | yes | Still relevant but fully superseded by #939, which covers all 5 `check_input_*` functions. These 3 functions are dead code: never called in production, not exported, only tested. A comment in `check-input-helpers.R` explicitly marks them for future deletion. Should be closed as duplicate of #939. | Superseded by #939. Related to #840 (input check cleanup) and #823 (reduce `try()`). | Close as duplicate of #939. If handled independently, delete the 3 function definitions, their man pages, roxygen cross-references, and corresponding test blocks. | [x] | [ ] | — | [ ] |\n| 668 | Add argument to control what kind of coverage is computed by `get_coverage()` | feature request | 2024-02-29 | enhancement | low | small | yes | Still relevant and unimplemented. `get_coverage()` always computes all 4 coverage metrics with no user control. The issue body contains a near-complete code proposal (with a minor bug to fix). The function structure already cleanly separates interval and quantile computation paths. On `scoringutils-2.x` milestone. | No blocking dependencies. Standalone enhancement. | Add a `type = c(\"quantile\", \"interval\")` argument to `get_coverage()` that conditionally computes interval coverage, quantile coverage, or both. Update downstream column handling, summarisation, and tests. | [x] | [ ] | — | [ ] |\n| 667 | Create print method for `scores()` objects | feature request | 2024-02-29 | feature | low | small | partially | Still relevant but effectively blocked by #935. No `print.scores` method exists; scores objects fall through to `print.data.table`. Adding a custom print method would exacerbate the unwanted-printing-on-`:=` bug documented in #935. Both maintainers expressed lukewarm interest, deferring to post-release. | Blocked by #935 (print on column change bug). Related to #975 (link to score definitions in print). | Implement `print.scores()` S3 method displaying computed metric names (from `metrics` attribute) before delegating to `NextMethod()`, following the `print.forecast` pattern. Wait until #935 is resolved first to avoid spurious printing on `:=` operations. | N/A | N/A | — | N/A |\n| 666 | Update the scoringutils paper that accompanies the package | question | 2024-02-29 | documentation | medium | large | no | Still relevant but partially stale. Only 2 of ~15 checklist items done. Manuscript in `inst/manuscript/` still references `magrittr`, CITATION still points to 2022 arXiv preprint. README links to Google Drive PDF. Some checklist items reference functions no longer exported. Should be broken into smaller actionable issues. | Related to #759, #789, #546, #758, #1057. | Audit the to-do list against current API to remove stale items, break remaining unchecked items into individual issues, and decide whether to prioritize the \"make a vignette out of the paper\" item before the next CRAN release. Requires human authorship decisions. | N/A | N/A | — | N/A |\n| 646 | Stop converting errors to warnings in `score()`? | help wanted, question, Breaking change | 2024-02-23 | refactor | medium | medium | partially | Still relevant. `run_safely()` in `R/score.R` still converts all metric errors to warnings via `try()` + `cli_warn()` and returns `NULL`, silently dropping failed metrics. Deferred to `scoringutils-2.x` milestone. No consensus on approach: let errors propagate (breaking), add `on_error` parameter (compatible), or batch warnings (#811). | Related to #811 (batch warnings), #941 (`score()` redesign), #823 (reduce `try()`). | Design decision needed: (a) let errors propagate as errors, (b) add `on_error = c(\"warn\", \"error\")` param to `score()`, or (c) combine with #811's batch-warning approach. Implementation touches `run_safely()` in `R/score.R` and test expectations. | N/A | N/A | — | N/A |\n| 625 | Implement a `get_unique_values()` function | implementation-ready | 2024-02-10 | enhancement | low | small | yes | Still relevant. No implementation work done. Function does not exist. Would compute `length(unique(values))` per forecast-unit column (via `get_forecast_unit()`), optionally grouped by model. Similar diagnostic purpose to `get_forecast_counts()` but different output. | Loosely related to #581 (NA in print), #935 (print bug), #621 (closed, prettier print). No hard blockers for standalone function. | Implement a new exported `get_unique_values()` function that takes a forecast object, calls `get_forecast_unit()` to identify relevant columns, and returns a summary table of unique value counts per column. Optionally integrate into `print.forecast()`. | [x] | [ ] | — | [ ] |\n| 608 | Define input format for categorical forecasts | implementation-ready | 2024-01-17 | discussion | low | small | no | Largely resolved. Input format fully defined and implemented in `forecast_nominal` (PR #837) and `forecast_ordinal` (PR #977). Column named `predicted_label`. Strict completeness enforced. Only unimplemented item is a helper function to auto-expand incomplete forecasts with zero-probability rows, which has no dedicated tracking issue. Can be closed. | #604 (parent, closed), #607 (naming, closed). No blockers. | Close this issue as completed. Optionally open a new targeted issue for the helper function that auto-expands incomplete forecasts with zero-probability rows if still desired. | N/A | N/A | — | N/A |\n| 581 | Add the number of `NA` values to the planned print method for `forecast_*` objects, rather than throw a message? | feature request | 2024-01-05 | enhancement | medium | medium | partially | Still relevant. None of the three requested features implemented: `print.forecast()` shows no NA info, NA message in `assert_forecast_generic()` is still vague (no row count), no `get_NA()` helper exists. Issue #537 is a near-duplicate and both should be addressed together. | Near-duplicate of #537 (should be resolved together). Tangentially related to #731, #542, #935 (print bug). | Enhance `print.forecast()` to display NA row count, improve the message in `assert_forecast_generic()` to report exact number of NA rows, and consider adding a `get_na_summary()` helper. Consolidate with #537. | N/A | N/A | — | N/A |\n| 557 | sample_to_quantile.numeric method | low-priority | 2024-01-05 | enhancement | low | small | yes | Effectively resolved. `sample_to_quantile()` was replaced by `as_forecast_quantile.forecast_sample()` in PR #876. The motivating code (`interval_coverage_sample()`) was deleted entirely. No current code needs a `.numeric` method. Can be closed. | Tangentially related to #1026 (reverse conversion). No blockers. | Close this issue as resolved. The refactoring in PR #876 and deletion of `interval_coverage_sample()` eliminated the code pattern that motivated this request. | N/A | N/A | — | N/A |\n| 546 | Simplify \"Getting Started\" \\| Split into smaller chunks | documentation | 2024-01-05 | documentation | medium | medium | no | Still relevant. The old `scoringutils.Rmd` was deleted (PR #836) but the promised replacement \"Get started\" vignette was never written. Package currently has no introductory vignette at all. README has a \"Quick start\" section but no pkgdown \"Get started\" page. @seabbs volunteered in #759 but no PR materialized. | Tightly coupled with #759 (future of scoringutils.Rmd) — should be resolved together. Related to #789 (metric customization docs). | Write a new introductory \"Getting Started\" vignette (as `vignettes/scoringutils.Rmd` for pkgdown) providing a concise end-to-end walkthrough. Can close both #546 and #759 simultaneously. Requires editorial decisions on scope and content. | N/A | N/A | — | N/A |\n| 542 | Improve ways to debug `score()` | question | 2024-01-04 | enhancement | medium | medium | partially | Still relevant. Neither action item is done: (1) no vignette documents `debugonce()` for scoring rule debugging, (2) `run_safely()` still only reports the metric name in warnings, not which forecast unit/row caused the failure. The `{cli}` migration (PR #583) improved formatting but not informativeness. | Closely related to #646 (stop converting errors to warnings — open, breaking change). Related to closed #391, #418, #543, #583. | Two tasks remain: (1) add documentation showing `debugonce()` usage for debugging scoring failures, and (2) enhance `run_safely()`/`apply_metrics()` to include forecast-unit context in warning messages. Coordinate error-vs-warning design with #646. | N/A | N/A | — | N/A |\n| 537 | Inform users about the number of rows that have NA values | good first issue, question, implementation-ready, high-priority | 2024-01-03 | enhancement | medium | medium | partially | Still relevant. The message in `assert_forecast_generic()` (line 150 of `R/class-forecast.R`) still says the generic \"Some rows containing NA values may be removed\" without a row count. No NA info in `print.forecast`, no `get_na()` helper. The `anyNA()` check and `na.omit()` call are already in place, so computing the count is trivial. | Near-duplicate of #581 (NA count in print method) — should be resolved together. Related to #731 (wis NA warnings). | Minimal fix: add NA row count to the existing `cli_inform` message in `assert_forecast_generic()`. For a more complete resolution, combine with #581 to also add NA summary to `print.forecast` and potentially create a `get_na()` diagnostic helper. | N/A | N/A | — | N/A |\n| 522 | Create a way to test user-provided scoring functions | feature request | 2023-12-06 | feature | medium | medium | partially | Still relevant. No progress made. `validate_metrics()` only checks that items are functions (via `checkmate::check_function`), not signature compatibility, output shape, or runtime behavior. Custom metric errors are silently converted to warnings by `run_safely()`, making debugging hard. Each of the 6+ forecast types passes different arguments to metrics. | Loosely related to #362 (non-standard metric I/O), #823 (reduce `try()`). | Implement an exported `test_metrics()` function that runs each metric on a small representative subset of a forecast object and reports informative diagnostics: signature compatibility, runtime errors, and output shape validation. Needs design decision on API. | N/A | N/A | — | N/A |\n| 520 | Create documentation/vignette for the naming scheme of our functions | help wanted, question | 2023-11-30 | documentation | low | medium | partially | Still relevant. The issue body already describes the naming scheme in detail but no vignette or formal documentation exists. CONTRIBUTING.md only references tidyverse style guide. Issue #948 (closed) debated `get_` prefix but decided to keep it. 65 exported functions follow conventions consistently. ~2 years old with no implementation. | Related to closed #948, #610, #476, #357. No blockers. | Create a developer-facing vignette or CONTRIBUTING.md section formalizing the naming conventions (scoring rules: `<name>_<type>`, `get_*`, `as_forecast_*`, `is_forecast_*`, `plot_*`, `add_*`, `assert_`/`check_`/`test_`). Update to reflect current classes including `forecast_nominal`, `forecast_ordinal`, `forecast_sample_multivariate`. | N/A | N/A | — | N/A |\n| 513 | Investigate R scripts that get generated based on our vignettes | | 2023-11-28 | cleanup | low | small | yes | Already resolved. The `.R` and `.html` vignette artifacts (generated by `knitr` tangle) are gitignored via `vignettes/.gitignore`, excluded from linting via `.lintr`, and not tracked in git. All three mitigations from the discussion are in place. Issue can be closed. | None. | Close this issue — all requested investigation and mitigations (gitignore, buildignore, lint-ignore for vignette-generated `.R` files) are already implemented. No code changes needed. | N/A | N/A | — | N/A |\n| 492 | Expose `get_range_from_quantile()` to the user | question | 2023-11-22 | enhancement | low | small | yes | Still relevant. Function exists at `R/helper-quantile-interval-range.R:185` as `@keywords internal`, not exported. Maintainer consensus reached: rename to `get_interval_range()` and create `add_interval_range()` wrapper. Neither implemented. Original motivating function `plot_ranges()` was deleted, reducing urgency. 3 internal call sites. | Parent meta-issue #484. Tangentially related to #885 (export decisions). | Rename `get_range_from_quantile()` to `get_interval_range()`, export it, and create a new exported `add_interval_range()` that adds an `interval_range` column to a quantile-format data.table. Update 3 internal call sites, NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |\n| 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |\n| 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |\n| 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |\n| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |\n| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n\nTRIAGE_COMPLETE","numLines":131,"startLine":1,"totalLines":131}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_014NexMSRbmmFXfDR4k4xJ6Y","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me find the first row where **Automatable** is `yes` and **Has Tests** is `[ ]` (unchecked). Let me scan through the issues:\n\n1. **#1044** - Automatable: yes, Has Tests: N/A → skip\n2. **#1043** - Automatable: yes, Has Tests: [x] → already done\n3. **#1022** - Automatable: yes, Has Tests: [x] → already done\n4. **#1002** - Automatable: yes, Has Tests: [x] → already done\n5. **#983** - Automatable: yes, Has Tests: [x] → already done\n6. **#975** - Automatable: yes, Has Tests: [x] → already done\n7. **#942** - Automatable: yes, Has Tests: [x] → already done\n8. **#922** - Automatable: yes, Has Tests: [x] → already done\n9. **#909** - Automatable: yes, Has Tests: N/A → skip\n10. **#895** - Automatable: yes, Has Tests: [x] → already done\n11. **#880** - Automatable: yes, Has Tests: [x] → already done\n12. **#839** - Automatable: yes, Has Tests: [x] → already done\n13. **#825** - Automatable: yes, Has Tests: [x] → already done\n14. **#811** - Automatable: yes, Has Tests: [x] → already done\n15. **#807** - Automatable: yes, Has Tests: [x] → already done\n16. **#793** - Automatable: yes, Has Tests: [x] → already done\n17. **#785** - Automatable: yes, Has Tests: [x] → already done\n18. **#780** - Automatable: yes, Has Tests: [x] → already done\n19. **#769** - Automatable: yes, Has Tests: [x] → already done\n20. **#763** - Automatable: yes, Has Tests: [x] → already done\n21. **#718** - Automatable: yes, Has Tests: [x] → already done\n22. **#684** - Automatable: yes, Has Tests: [x] → already done\n23. **#668** - Automatable: yes, Has Tests: [x] → already done\n24. **#625** - Automatable: yes, Has Tests: [x] → already done\n25. **#492** - Automatable: yes, Has Tests: [x] → already done\n\nNow let me check the remaining ones more carefully - issues #1044, #454, #445, #412, #406, #382, #363, #360, #193 which have Automatable `yes` but I need to check their Has Tests status."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":25393,"cache_read_input_tokens":21205,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":25393},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"ccad8450-bc83-46de-a9af-20f86120a97d"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_014NexMSRbmmFXfDR4k4xJ6Y","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WxgD9J99P3ezPc2CCQkUrP","name":"TodoWrite","input":{"todos":[{"content":"Scan triage table for next automatable issue without tests","status":"in_progress","activeForm":"Scanning triage table for next automatable issue without tests"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":25393,"cache_read_input_tokens":21205,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":25393},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"e945fd3b-a220-4800-9c9b-188712c542ee"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WxgD9J99P3ezPc2CCQkUrP","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"95bb6646-1883-4c15-8103-b6314b3154cf","tool_use_result":{"oldTodos":[],"newTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"in_progress","activeForm":"Scanning triage table for next automatable issue without tests"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01CZfgEZqfjXFaCmAp1XUBe4","type":"message","role":"assistant","content":[{"type":"text","text":"Let me re-read specific rows more carefully. From the table, I need to check the `Has Tests` column for each automatable=yes issue. Let me look at the ones I haven't fully verified:\n\n- **#1044**: `N/A` — skip\n- **#454**: `[ ]` — **unchecked!** But Automatable is `yes`? Let me re-check. Looking at row 105: `| 454 | ... | yes | ... | [ ] | [ ] | — | [ ] |`\n\nWait, let me re-read more carefully. Issue #454's row says Automatable is `yes` and Has Tests is `[ ]`. But the Action Summary says to refactor deduplication. Let me check #445, #412, #406, #382, #363, #360, #193 too.\n\nLooking back at the table more carefully:\n- #454 (line 105): Automatable `yes`, Has Tests `[ ]` \n- #445 (line 108): Automatable `yes`, Has Tests `[ ]`\n- #412 (line 112): Automatable `yes`, Has Tests `[ ]`\n- #406 (line 114): Automatable `yes`, Has Tests `[ ]`\n- #382 (line 116): Automatable `yes`, Has Tests `[ ]`\n- #363 (line 117): Automatable `yes`, Has Tests `[ ]`\n- #360 (line 119): Automatable `yes`, Has Tests `[ ]`\n- #193 (line 127): Automatable `yes`, Has Tests `[ ]`\n\nThe first one in table order (by issue number descending, starting from the top) would be... Let me scan top-to-bottom through the table:\n\nIssue #1044 (line 29) - Automatable: yes, Has Tests: N/A → skip\nIssue #1043 (line 30) - [x] → done\n...continuing down...\n\nThe first `yes` + `[ ]` in table order is **#454** at line 105. But wait, I need to check more carefully - there might be earlier ones I missed. Let me check #513, #557, etc.\n\n- #513 (line 100): Automatable `yes`, Has Tests `N/A` → skip (already resolved, close-only)\n- #557 (line 94): Automatable `yes`, Has Tests `N/A` → skip\n- #752 (line 77): Automatable `yes`, Has Tests `N/A` → skip\n- #810 (line 66): Automatable `yes`, Has Tests `N/A` → skip (closed)\n- #909 (line 49): Automatable `yes`, Has Tests `N/A` → skip\n- #411 (line 113): Automatable `yes`, Has Tests `N/A` → skip (resolved)\n\nSo the first automatable=yes issue with Has Tests = `[ ]` (unchecked) in the table (going top to bottom by row order) is:\n\nLooking at the table order (top to bottom): #1044 has N/A. Let me find the first `[ ]`:\n\nRow 105: **#454** - `yes`, `[ ]`\nRow 108: **#445** - `yes`, `[ ]`  \nRow 112: **#412** - `yes`, `[ ]`\nRow 114: **#406** - `yes`, `[ ]`\nRow 116: **#382** - `yes`, `[ ]`\nRow 117: **#363** - `yes`, `[ ]`\nRow 119: **#360** - `yes`, `[ ]`\nRow 127: **#193** - `yes`, `[ ]`\n\nThe first one in table order (top-to-bottom) is **#454** at row/line 105.\n\nWait, actually the table is ordered by issue number descending (highest first). Let me re-examine - the first row with Automatable=yes and Has Tests=[ ] going from top to bottom would be the highest-numbered issue. But #1044 is N/A. Let me trace through again more carefully, checking every row...\n\nActually, I've already verified all the issues from #1044 down to #625. All of those with Automatable=yes either have `[x]` or `N/A`. So the first unchecked one is indeed at line 105: **#454**.\n\nLet me now proceed with issue #454."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":813,"cache_read_input_tokens":46598,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":813},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"c2ba1f49-a215-4dd2-b56c-cb75581d4de9"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01CZfgEZqfjXFaCmAp1XUBe4","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NJDETMaUhZKqcmYFtfe5ZW","name":"TodoWrite","input":{"todos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"in_progress","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"pending","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"pending","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":813,"cache_read_input_tokens":46598,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":813},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"93d8634c-498e-4b67-8d1b-89d2021c1932"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NJDETMaUhZKqcmYFtfe5ZW","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"02899d64-82b8-4445-a179-c700fd6d59fd","tool_use_result":{"oldTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"in_progress","activeForm":"Scanning triage table for next automatable issue without tests"}],"newTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"in_progress","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"pending","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"pending","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01L34msYCcWMveBAVVmcqirC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BGX7xK7fjNG5UoWyXiffQD","name":"Bash","input":{"command":"gh issue view 454 --comments","description":"View full issue #454 with comments"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1338,"cache_read_input_tokens":47411,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1338},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"5a260705-417b-4773-8603-e6819fa50466"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BGX7xK7fjNG5UoWyXiffQD","type":"tool_result","content":"author:\tnikosbosse\nassociation:\tcollaborator\nedited:\tfalse\nstatus:\tnone\n--\nI'll move this to 2.1. as it's not critical\n--\nauthor:\tseabbs\nassociation:\tcontributor\nedited:\tfalse\nstatus:\tnone\n--\nThis seems like a very low effort issue to address and I think we want to minimise technical debt going forward. I am happy to address.\n--","is_error":false}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"c5dce8f4-fdf7-421b-b473-1d69ba4b25b1","tool_use_result":{"stdout":"author:\tnikosbosse\nassociation:\tcollaborator\nedited:\tfalse\nstatus:\tnone\n--\nI'll move this to 2.1. as it's not critical\n--\nauthor:\tseabbs\nassociation:\tcontributor\nedited:\tfalse\nstatus:\tnone\n--\nThis seems like a very low effort issue to address and I think we want to minimise technical debt going forward. I am happy to address.\n--","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01K2TBsJodYaCbB2oJ8S4BZr","type":"message","role":"assistant","content":[{"type":"text","text":"The issue view didn't show the full title and body. Let me try with the repo specification."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":194,"cache_read_input_tokens":48749,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":194},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"5923f80d-ef7b-48c0-b7f9-2be47afe732f"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01K2TBsJodYaCbB2oJ8S4BZr","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01B1LgtQuyEiE5hR32mGdRDo","name":"Bash","input":{"command":"gh issue view 454 --repo epiforecasts/scoringutils --json title,body,comments,labels,state","description":"View full issue #454 details in JSON"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":194,"cache_read_input_tokens":48749,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":194},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"da45adcb-5f55-4b1c-a9d0-e5b40d42eabf"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01B1LgtQuyEiE5hR32mGdRDo","type":"tool_result","content":"{\"body\":\"Meta issue: Code refactoring: #459 \\n\\nFunctions share a lot of code that could be turned into helper functions\",\"comments\":[{\"id\":\"IC_kwDODlXCNM55r7tG\",\"author\":{\"login\":\"nikosbosse\"},\"authorAssociation\":\"COLLABORATOR\",\"body\":\"I'll move this to 2.1. as it's not critical\",\"createdAt\":\"2024-04-07T18:48:10Z\",\"includesCreatedEdit\":false,\"isMinimized\":false,\"minimizedReason\":\"\",\"reactionGroups\":[],\"url\":\"https://github.com/epiforecasts/scoringutils/issues/454#issuecomment-2041559878\",\"viewerDidAuthor\":true},{\"id\":\"IC_kwDODlXCNM55u3JR\",\"author\":{\"login\":\"seabbs\"},\"authorAssociation\":\"CONTRIBUTOR\",\"body\":\"This seems like a very low effort issue to address and I think we want to minimise technical debt going forward. I am happy to address.\",\"createdAt\":\"2024-04-08T09:51:56Z\",\"includesCreatedEdit\":false,\"isMinimized\":false,\"minimizedReason\":\"\",\"reactionGroups\":[{\"content\":\"HEART\",\"users\":{\"totalCount\":1}}],\"url\":\"https://github.com/epiforecasts/scoringutils/issues/454#issuecomment-2042327633\",\"viewerDidAuthor\":false}],\"labels\":[{\"id\":\"LA_kwDODlXCNM8AAAABcwBcqg\",\"name\":\"refactor\",\"description\":\"Code refactoring\",\"color\":\"0052CC\"},{\"id\":\"LA_kwDODlXCNM8AAAABdbty1A\",\"name\":\"implementation-ready\",\"description\":\"This is ready for implementation\",\"color\":\"88F77C\"}],\"state\":\"OPEN\",\"title\":\"Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()`\"}","is_error":false}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"c7a91835-9c99-4d76-9778-b2dfac840f71","tool_use_result":{"stdout":"{\"body\":\"Meta issue: Code refactoring: #459 \\n\\nFunctions share a lot of code that could be turned into helper functions\",\"comments\":[{\"id\":\"IC_kwDODlXCNM55r7tG\",\"author\":{\"login\":\"nikosbosse\"},\"authorAssociation\":\"COLLABORATOR\",\"body\":\"I'll move this to 2.1. as it's not critical\",\"createdAt\":\"2024-04-07T18:48:10Z\",\"includesCreatedEdit\":false,\"isMinimized\":false,\"minimizedReason\":\"\",\"reactionGroups\":[],\"url\":\"https://github.com/epiforecasts/scoringutils/issues/454#issuecomment-2041559878\",\"viewerDidAuthor\":true},{\"id\":\"IC_kwDODlXCNM55u3JR\",\"author\":{\"login\":\"seabbs\"},\"authorAssociation\":\"CONTRIBUTOR\",\"body\":\"This seems like a very low effort issue to address and I think we want to minimise technical debt going forward. I am happy to address.\",\"createdAt\":\"2024-04-08T09:51:56Z\",\"includesCreatedEdit\":false,\"isMinimized\":false,\"minimizedReason\":\"\",\"reactionGroups\":[{\"content\":\"HEART\",\"users\":{\"totalCount\":1}}],\"url\":\"https://github.com/epiforecasts/scoringutils/issues/454#issuecomment-2042327633\",\"viewerDidAuthor\":false}],\"labels\":[{\"id\":\"LA_kwDODlXCNM8AAAABcwBcqg\",\"name\":\"refactor\",\"description\":\"Code refactoring\",\"color\":\"0052CC\"},{\"id\":\"LA_kwDODlXCNM8AAAABdbty1A\",\"name\":\"implementation-ready\",\"description\":\"This is ready for implementation\",\"color\":\"88F77C\"}],\"state\":\"OPEN\",\"title\":\"Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()`\"}","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01FjdqttyoKf2itb6mj7Zyud","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me understand the context better. The triage table says:\n- **Title**: Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()`\n- **Action Summary**: Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern.\n- **Status Note**: Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic).\n\nNow let me explore the relevant source files and the test landscape."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":619,"cache_read_input_tokens":48943,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":619},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"f87ba78d-7a60-46d5-a64f-967371f65507"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01FjdqttyoKf2itb6mj7Zyud","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","name":"Task","input":{"description":"Explore coverage code and tests","prompt":"I need to understand the code for issue #454 which is about deduplicating code between `get_coverage()` and `interval_coverage()`. \n\nPlease find and read:\n1. The `get_coverage()` function - likely in `R/get-coverage.R`\n2. The `interval_coverage()` function - likely in `R/metrics-quantile.R` or similar\n3. The `quantile_to_interval()` function (shared helper)\n4. Any existing tests related to coverage: look in `tests/testthat/` for test files containing \"coverage\" \n5. The test helpers/setup files: `tests/testthat/helper-*.R` and `tests/testthat/setup.R`\n\nFor each file, report the full content (or at least the relevant functions) with line numbers. I need to understand:\n- What code is duplicated between `get_coverage()` and `interval_coverage()`\n- What the `quantile_to_interval()` + bounds-check pattern looks like in each\n- How existing tests are structured (testthat conventions, fixtures, example data used)\n- What test data/fixtures are available\n\nWorking directory: /Users/nikos/github-synced/scoringutils-pipeline","subagent_type":"Explore"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":619,"cache_read_input_tokens":48943,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":619},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"71187d62-901e-48e0-9146-cb8ff22c73fd"}
{"type":"user","message":{"role":"user","content":[{"type":"text","text":"I need to understand the code for issue #454 which is about deduplicating code between `get_coverage()` and `interval_coverage()`. \n\nPlease find and read:\n1. The `get_coverage()` function - likely in `R/get-coverage.R`\n2. The `interval_coverage()` function - likely in `R/metrics-quantile.R` or similar\n3. The `quantile_to_interval()` function (shared helper)\n4. Any existing tests related to coverage: look in `tests/testthat/` for test files containing \"coverage\" \n5. The test helpers/setup files: `tests/testthat/helper-*.R` and `tests/testthat/setup.R`\n\nFor each file, report the full content (or at least the relevant functions) with line numbers. I need to understand:\n- What code is duplicated between `get_coverage()` and `interval_coverage()`\n- What the `quantile_to_interval()` + bounds-check pattern looks like in each\n- How existing tests are structured (testthat conventions, fixtures, example data used)\n- What test data/fixtures are available\n\nWorking directory: /Users/nikos/github-synced/scoringutils-pipeline"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"4b4ca299-3c51-49c1-9f9f-6f99b2b16c70"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_015jZFzYG1oGNWqLE8mbs3Ya","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AB3G3Wx4QLysZhHhceWV9R","name":"Glob","input":{"pattern":"**/R/get-coverage.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2069,"cache_read_input_tokens":9234,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2069},"output_tokens":5,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"0715c174-2c42-4226-a5db-983af880d79f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AB3G3Wx4QLysZhHhceWV9R","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"1e73237e-3395-47b3-bfe5-d64c9419b141"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_015jZFzYG1oGNWqLE8mbs3Ya","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Bx2EHyxtJJuTiks96knJ8o","name":"Glob","input":{"pattern":"**/R/metrics-quantile.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2069,"cache_read_input_tokens":9234,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2069},"output_tokens":5,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"a9d667fe-3af6-4632-8e7e-fde8c330385d"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_015jZFzYG1oGNWqLE8mbs3Ya","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Eew7CVJFNBeRiYyrp4GJ1u","name":"Glob","input":{"pattern":"**/R/*.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2069,"cache_read_input_tokens":9234,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2069},"output_tokens":5,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"b48a0495-de1a-4987-9a07-b0a2fcada21a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Bx2EHyxtJJuTiks96knJ8o","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"73729098-ba46-4c7c-bddd-6845b0d7836e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Eew7CVJFNBeRiYyrp4GJ1u","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/R/check-input-helpers.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-binary.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-multivariate-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-nominal.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-ordinal.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-quantile.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-scores.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/documentation-templates.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-duplicate-forecasts.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-forecast-type.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-pit-histogram.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-protected-columns.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-binary.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-interval-range.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-multivariate-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-nominal.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-ordinal.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-point.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-sample.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/metrics.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/scoringutils.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/theme-scoringutils.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/z-globalVariables.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/class-forecast-point.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/forecast-unit.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-correlations.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/get-forecast-counts.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/pairwise-comparisons.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/plot-heatmap.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/plot-wis.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/score.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/summarise_scores.R\n/Users/nikos/github-synced/scoringutils-pipeline/R/transform-forecasts.R"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"2e6466bd-ceae-4fa3-95f0-7a61cadece86"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_015jZFzYG1oGNWqLE8mbs3Ya","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WiMRb5qs6fWXiwFRefQFM9","name":"Glob","input":{"pattern":"**/tests/testthat/test-*coverage*.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2069,"cache_read_input_tokens":9234,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2069},"output_tokens":5,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"8cfcf013-2d48-4cf8-870b-da91e24e1121"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_015jZFzYG1oGNWqLE8mbs3Ya","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AyvPn5ugk31bUCjAJZgSqS","name":"Glob","input":{"pattern":"**/tests/testthat/helper-*.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2069,"cache_read_input_tokens":9234,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2069},"output_tokens":5,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"36cc88af-546f-472a-b69b-6c97bd5a3501"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WiMRb5qs6fWXiwFRefQFM9","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"7dee9238-413c-48e0-ab8f-09868d45c79f"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_015jZFzYG1oGNWqLE8mbs3Ya","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Hwa4JMcujyFmd464ugaL7M","name":"Glob","input":{"pattern":"**/tests/testthat/setup.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2069,"cache_read_input_tokens":9234,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2069},"output_tokens":5,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"a7464d9d-7a58-4b13-9ce8-805e3d09f728"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AyvPn5ugk31bUCjAJZgSqS","type":"tool_result","content":"No files found"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"ecc2bfff-4601-497c-9768-e1338f5051cd"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Hwa4JMcujyFmd464ugaL7M","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"dc58e7f5-0315-41aa-be45-35a277c805be"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01QUDn76QWXYE5icatuNtmiU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UP8wP6xwdFz83DYFJvh37S","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1811,"cache_read_input_tokens":11303,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1811},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"b3ae8507-fdc8-42e1-984d-96fbefb81884"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UP8wP6xwdFz83DYFJvh37S","type":"tool_result","content":"     1→#' @title Get quantile and interval coverage values for quantile-based forecasts\n     2→#'\n     3→#' @description\n     4→#' For a validated forecast object in a quantile-based format\n     5→#' (see [as_forecast_quantile()] for more information), this function computes:\n     6→#' - interval coverage of central prediction intervals\n     7→#' - quantile coverage for predictive quantiles\n     8→#' - the deviation between desired and actual coverage (both for interval and\n     9→#' quantile coverage)\n    10→#'\n    11→#' Coverage values are computed for a specific level of grouping, as specified\n    12→#' in the `by` argument. By default, coverage values are computed per model.\n    13→#'\n    14→#' **Interval coverage**\n    15→#'\n    16→#' Interval coverage for a given interval range is defined as the proportion of\n    17→#' observations that fall within the corresponding central prediction intervals.\n    18→#' Central prediction intervals are symmetric around the median and formed\n    19→#' by two quantiles that denote the lower and upper bound. For example, the 50%\n    20→#' central prediction interval is the interval between the 0.25 and 0.75\n    21→#' quantiles of the predictive distribution.\n    22→#'\n    23→#' **Quantile coverage**\n    24→#'\n    25→#' Quantile coverage for a given quantile level is defined as the proportion of\n    26→#' observed values that are smaller than the corresponding predictive quantile.\n    27→#' For example, the 0.5 quantile coverage is the proportion of observed values\n    28→#' that are smaller than the 0.5 quantile of the predictive distribution.\n    29→#' Just as above, for a single observation and the quantile of a single\n    30→#' predictive distribution, the value will either be `TRUE` or `FALSE`.\n    31→#'\n    32→#' **Coverage deviation**\n    33→#'\n    34→#' The coverage deviation is the difference between the desired coverage\n    35→#' (can be either interval or quantile coverage) and the\n    36→#' actual coverage. For example, if the desired coverage is 90% and the actual\n    37→#' coverage is 80%, the coverage deviation is -0.1.\n    38→#' @return\n    39→#' A data.table with columns as specified in `by` and additional\n    40→#' columns for the coverage values described above\n    41→#' @inheritParams score\n    42→#' @param by character vector that denotes the level of grouping for which the\n    43→#'   coverage values should be computed. By default (`\"model\"`), one coverage\n    44→#'   value per model will be returned.\n    45→#' @return\n    46→#' a data.table with columns \"interval_coverage\",\n    47→#' \"interval_coverage_deviation\", \"quantile_coverage\",\n    48→#' \"quantile_coverage_deviation\" and the columns specified in `by`.\n    49→#' @importFrom data.table setcolorder\n    50→#' @importFrom checkmate assert_subset\n    51→#' @examples\n    52→#' example_quantile |>\n    53→#'   as_forecast_quantile() |>\n    54→#'   get_coverage(by = \"model\")\n    55→#' @export\n    56→#' @keywords scoring\n    57→#' @export\n    58→get_coverage <- function(forecast, by = \"model\") {\n    59→  # input checks ---------------------------------------------------------------\n    60→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n    61→  assert_subset(get_forecast_type(forecast), \"quantile\")\n    62→\n    63→  # remove \"quantile_level\" and \"interval_range\" from `by` if present, as these\n    64→  # are included anyway\n    65→  by <- setdiff(by, c(\"quantile_level\", \"interval_range\"))\n    66→  assert_subset(by, names(forecast))\n    67→\n    68→  # convert to wide interval format and compute interval coverage --------------\n    69→  interval_forecast <- quantile_to_interval(forecast, format = \"wide\")\n    70→  interval_forecast[,\n    71→    interval_coverage := (observed <= upper) & (observed >= lower)\n    72→  ][, c(\"lower\", \"upper\", \"observed\") := NULL]\n    73→  interval_forecast[, interval_coverage_deviation :=\n    74→                      interval_coverage - interval_range / 100]\n    75→\n    76→  # merge interval range data with original data -------------------------------\n    77→  # preparations\n    78→  forecast[, interval_range := get_range_from_quantile(quantile_level)]\n    79→  forecast_cols <- colnames(forecast) # store so we can reset column order later\n    80→  forecast_unit <- get_forecast_unit(forecast)\n    81→\n    82→  forecast <- merge(forecast, interval_forecast,\n    83→                    by = unique(c(forecast_unit, \"interval_range\")))\n    84→\n    85→  # compute quantile coverage and deviation ------------------------------------\n    86→  forecast[, quantile_coverage := observed <= predicted]\n    87→  forecast[, quantile_coverage_deviation := quantile_coverage - quantile_level]\n    88→\n    89→  # summarise coverage values according to `by` and cleanup --------------------\n    90→  # reset column order\n    91→  new_metrics <- c(\"interval_coverage\", \"interval_coverage_deviation\",\n    92→                   \"quantile_coverage\", \"quantile_coverage_deviation\")\n    93→  setcolorder(forecast, unique(c(forecast_cols, \"interval_range\", new_metrics)))\n    94→  # remove forecast class and convert to regular data.table\n    95→  forecast <- as.data.table(forecast)\n    96→  by <- unique(c(by, \"quantile_level\", \"interval_range\"))\n    97→  # summarise\n    98→  forecast <- forecast[, lapply(.SD, mean), by = by, .SDcols = new_metrics]\n    99→  return(forecast[])\n   100→}\n   101→\n   102→\n   103→#' @title Plot interval coverage\n   104→#'\n   105→#' @description\n   106→#' Plot interval coverage values (see [get_coverage()] for more information).\n   107→#'\n   108→#' @param coverage A data frame of coverage values as produced by\n   109→#' [get_coverage()].\n   110→#' @param colour According to which variable shall the graphs be coloured?\n   111→#' Default is \"model\".\n   112→#' @returns ggplot object with a plot of interval coverage\n   113→#' @importFrom ggplot2 ggplot scale_colour_manual scale_fill_manual .data\n   114→#' facet_wrap facet_grid geom_polygon geom_line xlab ylab\n   115→#' @importFrom checkmate assert_subset\n   116→#' @importFrom data.table dcast\n   117→#' @export\n   118→#' @examples\n   119→#' \\dontshow{\n   120→#'   data.table::setDTthreads(2) # restricts number of cores used on CRAN\n   121→#' }\n   122→#' example <- as_forecast_quantile(example_quantile)\n   123→#' coverage <- get_coverage(example, by = \"model\")\n   124→#' plot_interval_coverage(coverage)\n   125→plot_interval_coverage <- function(coverage,\n   126→                                   colour = \"model\") {\n   127→  coverage <- ensure_data.table(coverage)\n   128→  assert_subset(colour, names(coverage))\n   129→\n   130→  # in case quantile columns are present, remove them and then take unique\n   131→  # values. This doesn't visually affect the plot, but prevents lines from being\n   132→  # drawn twice.\n   133→  del <- c(\"quantile_level\", \"quantile_coverage\", \"quantile_coverage_deviation\")\n   134→  suppressWarnings(coverage[, eval(del) := NULL])\n   135→  coverage <- unique(coverage)\n   136→\n   137→  ## overall model calibration - empirical interval coverage\n   138→  p1 <- ggplot(coverage, aes(\n   139→    x = interval_range,\n   140→    colour = .data[[colour]]\n   141→  )) +\n   142→    geom_polygon(\n   143→      data = data.frame(\n   144→        x = c(0, 0, 100),\n   145→        y = c(0, 100, 100),\n   146→        g = c(\"o\", \"o\", \"o\"),\n   147→        stringsAsFactors = TRUE\n   148→      ),\n   149→      aes(\n   150→        x = x, y = y, group = g,\n   151→        fill = g\n   152→      ),\n   153→      alpha = 0.05,\n   154→      colour = \"white\",\n   155→      fill = \"olivedrab3\"\n   156→    ) +\n   157→    geom_line(\n   158→      aes(y = interval_range),\n   159→      colour = \"grey\",\n   160→      linetype = \"dashed\"\n   161→    ) +\n   162→    geom_line(aes(y = interval_coverage * 100)) +\n   163→    theme_scoringutils() +\n   164→    ylab(\"% Obs inside interval\") +\n   165→    xlab(\"Nominal interval coverage\") +\n   166→    coord_cartesian(expand = FALSE)\n   167→\n   168→  return(p1)\n   169→}\n   170→\n   171→\n   172→#' @title Plot quantile coverage\n   173→#'\n   174→#' @description\n   175→#' Plot quantile coverage values (see [get_coverage()] for more information).\n   176→#'\n   177→#' @inheritParams plot_interval_coverage\n   178→#' @param colour String, according to which variable shall the graphs be\n   179→#' coloured? Default is \"model\".\n   180→#' @returns A ggplot object with a plot of interval coverage\n   181→#' @importFrom ggplot2 ggplot scale_colour_manual scale_fill_manual .data aes\n   182→#'   scale_y_continuous geom_line\n   183→#' @importFrom checkmate assert_subset assert_data_frame\n   184→#' @importFrom data.table dcast\n   185→#' @export\n   186→#' @examples\n   187→#' example <- as_forecast_quantile(example_quantile)\n   188→#' coverage <- get_coverage(example, by = \"model\")\n   189→#' plot_quantile_coverage(coverage)\n   190→\n   191→plot_quantile_coverage <- function(coverage,\n   192→                                   colour = \"model\") {\n   193→  coverage <- assert_data_frame(coverage)\n   194→  assert_subset(colour, names(coverage))\n   195→\n   196→  p2 <- ggplot(\n   197→    data = coverage,\n   198→    aes(x = quantile_level, colour = .data[[colour]])\n   199→  ) +\n   200→    geom_polygon(\n   201→      data = data.frame(\n   202→        x = c(\n   203→          0, 0.5, 0.5,\n   204→          0.5, 0.5, 1\n   205→        ),\n   206→        y = c(\n   207→          0, 0, 0.5,\n   208→          0.5, 1, 1\n   209→        ),\n   210→        g = c(\"o\", \"o\", \"o\"),\n   211→        stringsAsFactors = TRUE\n   212→      ),\n   213→      aes(\n   214→        x = x, y = y, group = g,\n   215→        fill = g\n   216→      ),\n   217→      alpha = 0.05,\n   218→      colour = \"white\",\n   219→      fill = \"olivedrab3\"\n   220→    ) +\n   221→    geom_line(\n   222→      aes(y = quantile_level),\n   223→      colour = \"grey\",\n   224→      linetype = \"dashed\"\n   225→    ) +\n   226→    geom_line(aes(y = quantile_coverage)) +\n   227→    theme_scoringutils() +\n   228→    xlab(\"Quantile level\") +\n   229→    ylab(\"% Obs below quantile level\") +\n   230→    scale_y_continuous(\n   231→      labels = function(x) {\n   232→        paste(100 * x)\n   233→      }\n   234→    ) +\n   235→    coord_cartesian(expand = FALSE)\n   236→\n   237→  return(p2)\n   238→}\n   239→"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"8e90cba0-ae13-4fd1-8fae-900c75988d86"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01QUDn76QWXYE5icatuNtmiU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HpoGhvRYtM78jLdEJb7BAa","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1811,"cache_read_input_tokens":11303,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1811},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"1634921c-2d6c-45c3-adc3-263592f9189b"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01QUDn76QWXYE5icatuNtmiU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018QN5ipdurqYSNDTq1Gr3zH","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1811,"cache_read_input_tokens":11303,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1811},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"92496b38-180e-48fe-ac52-1166a220daf5"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HpoGhvRYtM78jLdEJb7BAa","type":"tool_result","content":"     1→#' @title Assert that inputs are correct for quantile-based forecast\n     2→#' @description\n     3→#' Function assesses whether the inputs correspond to the\n     4→#' requirements for scoring quantile-based forecasts.\n     5→#' @param predicted Input to be checked. Should be nxN matrix of predictive\n     6→#'   quantiles, n (number of rows) being the number of data points and N\n     7→#'   (number of columns) the number of quantiles per forecast.\n     8→#'   If `observed` is just a single number, then predicted can just be a\n     9→#'   vector of size N.\n    10→#' @param quantile_level Input to be checked. Should be a vector of size N that\n    11→#'   denotes the quantile levels corresponding to the columns of the prediction\n    12→#'   matrix.\n    13→#' @param unique_quantile_levels Whether the quantile levels are required to be\n    14→#'   unique (`TRUE`, the default) or not (`FALSE`).\n    15→#' @importFrom checkmate assert assert_numeric check_matrix check_vector\n    16→#' @inherit document_assert_functions params return\n    17→#' @keywords internal_input_check\n    18→assert_input_quantile <- function(observed, predicted, quantile_level,\n    19→                                  unique_quantile_levels = TRUE) {\n    20→  assert_numeric(observed, min.len = 1)\n    21→  n_obs <- length(observed)\n    22→\n    23→  assert_numeric(\n    24→    quantile_level, min.len = 1, lower = 0, upper = 1,\n    25→    unique = unique_quantile_levels\n    26→  )\n    27→  n_quantiles <- length(quantile_level)\n    28→  if (n_obs == 1) {\n    29→    assert(\n    30→      # allow one of two options\n    31→      check_numeric_vector(predicted, min.len = n_quantiles),\n    32→      check_matrix(predicted, mode = \"numeric\",\n    33→                   nrows = n_obs, ncols = n_quantiles)\n    34→    )\n    35→    assert(check_vector(quantile_level, len = length(predicted)))\n    36→  } else {\n    37→    assert(\n    38→      check_matrix(predicted, mode = \"numeric\",\n    39→                   nrows = n_obs, ncols = n_quantiles)\n    40→    )\n    41→  }\n    42→  return(invisible(NULL))\n    43→}\n    44→\n    45→#' @title Check that inputs are correct for quantile-based forecast\n    46→#' @inherit assert_input_quantile params description\n    47→#' @inherit check_input_sample return description\n    48→#' @keywords internal_input_check\n    49→check_input_quantile <- function(observed, predicted, quantile_level) {\n    50→  result <- check_try(\n    51→    assert_input_quantile(observed, predicted, quantile_level)\n    52→  )\n    53→  return(result)\n    54→}\n    55→\n    56→\n    57→#' Weighted interval score (WIS)\n    58→#' @description\n    59→#' The WIS is a proper scoring rule used to evaluate forecasts in an interval- /\n    60→#' quantile-based format. See Bracher et al. (2021). Smaller values are better.\n    61→#'\n    62→#' As the name suggest the score assumes that a forecast comes in the form of\n    63→#' one or multiple central prediction intervals. A prediction interval is\n    64→#' characterised by a lower and an upper bound formed by a pair of predictive\n    65→#' quantiles. For example, a 50% central prediction interval is formed by the\n    66→#' 0.25 and 0.75 quantiles of the predictive distribution.\n    67→#'\n    68→#' **Interval score**\n    69→#'\n    70→#' The interval score (IS) is the sum of three components:\n    71→#' overprediction, underprediction and dispersion. For a single prediction\n    72→#' interval only one of the components is non-zero. If for a single prediction\n    73→#' interval the observed value is below the lower bound, then the interval\n    74→#' score is equal to the absolute difference between the lower bound and the\n    75→#' observed value (\"underprediction\"). \"Overprediction\" is defined analogously.\n    76→#' If the observed value falls within the bounds of the prediction interval,\n    77→#' then the interval score is equal to the width of the prediction interval,\n    78→#' i.e. the difference between the upper and lower bound. For a single interval,\n    79→#' we therefore have:\n    80→#'\n    81→#' \\deqn{\n    82→#' \\textrm{IS} = (\\textrm{upper} - \\textrm{lower}) + \\frac{2}{\\alpha}(\\textrm{lower}\n    83→#'  - \\textrm{observed}) *\n    84→#' \\mathbf{1}(\\textrm{observed} < \\textrm{lower}) +\n    85→#' \\frac{2}{\\alpha}(\\textrm{observed} - \\textrm{upper}) *\n    86→#' \\mathbf{1}(\\textrm{observed} > \\textrm{upper})\n    87→#' }{\n    88→#' score = (upper - lower) + 2/alpha * (lower - observed) *\n    89→#' 1(observed < lower) + 2/alpha * (observed - upper) *\n    90→#' 1(observed > upper)\n    91→#' }\n    92→#' where \\eqn{\\mathbf{1}()}{1()} is the indicator function and\n    93→#' indicates how much is outside the prediction interval.\n    94→#' \\eqn{\\alpha}{alpha} is the decimal value that indicates how much is outside\n    95→#' the prediction interval. For a 90% prediction interval, for example,\n    96→#' \\eqn{\\alpha}{alpha} is equal to 0.1. No specific distribution is assumed,\n    97→#' but the interval formed by the quantiles has to be symmetric around the\n    98→#' median (i.e you can't use the 0.1 quantile as the lower bound and the 0.7\n    99→#' quantile as the upper bound).\n   100→#' Non-symmetric quantiles can be scored using the function [quantile_score()].\n   101→#'\n   102→#' For a set of \\eqn{k = 1, \\dots, K} prediction intervals and the median\n   103→#' \\eqn{m}, we can compute a weighted interval score (WIS) as the sum of the\n   104→#' interval scores for individual intervals:\n   105→#' \\deqn{\n   106→#' \\text{WIS}_{\\alpha_{\\{0:K\\}}}(F, y) = \\frac{1}{K + 1/2}\n   107→#' \\times \\left(w_0 \\times |y - m| + \\sum_{k=1}^{K}\n   108→#' \\left\\{ w_k \\times \\text{IS}_{\\alpha_k}(F, y) \\right\\}\\right)\n   109→#' }{\n   110→#' }\n   111→#'\n   112→#' The individual scores are usually weighted with\n   113→#' \\eqn{w_k = \\frac{\\alpha_k}{2}}{alpha_k / 2}. This weight ensures that\n   114→#' for an increasing number of equally spaced quantiles, the WIS\n   115→#' converges to the continuous ranked probability score (CRPS).\n   116→#'\n   117→#' **Quantile score**\n   118→#'\n   119→#' In addition to the interval score, there also exists a quantile score (QS)\n   120→#' (see [quantile_score()]), which is equal to the so-called pinball loss.\n   121→#' The quantile score can be computed for a single quantile (whereas the\n   122→#' interval score requires two quantiles that form an interval). However,\n   123→#' the intuitive decomposition into overprediction, underprediction and\n   124→#' dispersion does not exist for the quantile score.\n   125→#'\n   126→#' **Two versions of the weighted interval score**\n   127→#'\n   128→#' There are two ways to conceptualise the weighted interval score across\n   129→#' several quantiles / prediction intervals and the median.\n   130→#'\n   131→#' In one view, you would treat the WIS as the average of quantile scores (and\n   132→#' the median as 0.5-quantile) (this is the default for `wis()`). In another\n   133→#' view, you would treat the WIS as the average of several interval scores +\n   134→#' the difference between the observed value and median forecast. The effect of\n   135→#' that is that in contrast to the first view, the median has twice as much\n   136→#' weight (because it is weighted like a prediction interval, rather than like\n   137→#' a single quantile). Both are valid ways to conceptualise the WIS and you\n   138→#' can control the behaviour with the `count_median_twice`-argument.\n   139→#'\n   140→#' **WIS components**:\n   141→#' WIS components can be computed individually using the functions\n   142→#' `overprediction`, `underprediction`, and `dispersion.`\n   143→#'\n   144→#' @inheritParams interval_score\n   145→#' @param observed Numeric vector of size n with the observed values.\n   146→#' @param predicted Numeric nxN matrix of predictive\n   147→#'   quantiles, n (number of rows) being the number of forecasts (corresponding\n   148→#'   to the number of observed values) and N\n   149→#'   (number of columns) the number of quantiles per forecast.\n   150→#'   If `observed` is just a single number, then predicted can just be a\n   151→#'   vector of size N.\n   152→#' @param quantile_level Vector of of size N with the quantile levels\n   153→#'   for which predictions were made.\n   154→#' @param count_median_twice If TRUE, count the median twice in the score.\n   155→#' @param na.rm If TRUE, ignore NA values when computing the score.\n   156→#' @importFrom stats weighted.mean\n   157→#' @importFrom checkmate assert_logical\n   158→#' @inheritSection illustration-input-metric-quantile Input format\n   159→#' @return\n   160→#' `wis()`: a numeric vector with WIS values of size n (one per observation),\n   161→#' or a list with separate entries if `separate_results` is `TRUE`.\n   162→#' @export\n   163→#' @references\n   164→#' Evaluating epidemic forecasts in an interval format,\n   165→#' Johannes Bracher, Evan L. Ray, Tilmann Gneiting and Nicholas G. Reich, 2021,\n   166→#' <https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618>\n   167→#' @keywords metric\n   168→#' @examples\n   169→#' observed <- c(1, -15, 22)\n   170→#' predicted <- rbind(\n   171→#'   c(-1, 0, 1, 2, 3),\n   172→#'   c(-2, 1, 2, 2, 4),\n   173→#'   c(-2, 0, 3, 3, 4)\n   174→#' )\n   175→#' quantile_level <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n   176→#' wis(observed, predicted, quantile_level)\n   177→wis <- function(observed,\n   178→                predicted,\n   179→                quantile_level,\n   180→                separate_results = FALSE,\n   181→                weigh = TRUE,\n   182→                count_median_twice = FALSE,\n   183→                na.rm = FALSE) {\n   184→  assert_input_quantile(observed, predicted, quantile_level)\n   185→  reformatted <- quantile_to_interval(observed, predicted, quantile_level)\n   186→\n   187→  # check that all quantile levels form valid prediction intervals\n   188→  interval_ranges <- get_range_from_quantile(\n   189→    quantile_level[quantile_level != 0.5]\n   190→  )\n   191→  complete_intervals <-\n   192→    duplicated(interval_ranges) | duplicated(interval_ranges, fromLast = TRUE)\n   193→  if (!all(complete_intervals) && !isTRUE(na.rm)) {\n   194→    #nolint start: keyword_quote_linter object_usage_linter\n   195→    incomplete <- quantile_level[quantile_level != 0.5][!complete_intervals]\n   196→    cli_abort(\n   197→      c(\n   198→        \"!\" = \"Not all quantile levels specified form symmetric prediction\n   199→        intervals.\n   200→        The following quantile levels miss a corresponding lower/upper bound:\n   201→        {.val {incomplete}}.\n   202→        You can drop incomplete prediction intervals using `na.rm = TRUE`.\"\n   203→      )\n   204→    )\n   205→    #nolint end\n   206→  }\n   207→\n   208→  assert_logical(separate_results, len = 1)\n   209→  assert_logical(weigh, len = 1)\n   210→  assert_logical(count_median_twice, len = 1)\n   211→  assert_logical(na.rm, len = 1)\n   212→\n   213→  if (separate_results) {\n   214→    cols <- c(\"wis\", \"dispersion\", \"underprediction\", \"overprediction\")\n   215→  } else {\n   216→    cols <- \"wis\"\n   217→  }\n   218→\n   219→  reformatted[, eval(cols) := do.call(\n   220→    interval_score,\n   221→    list(\n   222→      observed = observed,\n   223→      lower = lower,\n   224→      upper = upper,\n   225→      interval_range = interval_range,\n   226→      weigh = weigh,\n   227→      separate_results = separate_results\n   228→    )\n   229→  )]\n   230→\n   231→  if (count_median_twice) {\n   232→    reformatted[, weight := 1]\n   233→  } else {\n   234→    reformatted[, weight := ifelse(interval_range == 0, 0.5, 1)]\n   235→  }\n   236→\n   237→  # summarise results by forecast_id\n   238→  reformatted <- reformatted[\n   239→    , lapply(.SD, weighted.mean, na.rm = na.rm, w = weight),\n   240→    by = \"forecast_id\",\n   241→    .SDcols = colnames(reformatted) %like% paste(cols, collapse = \"|\")\n   242→  ]\n   243→\n   244→  if (separate_results) {\n   245→    return(list(\n   246→      wis = reformatted$wis,\n   247→      dispersion = reformatted$dispersion,\n   248→      underprediction = reformatted$underprediction,\n   249→      overprediction = reformatted$overprediction\n   250→    ))\n   251→  } else {\n   252→    return(reformatted$wis)\n   253→  }\n   254→}\n   255→\n   256→\n   257→#' @return\n   258→#' `dispersion_quantile()`: a numeric vector with dispersion values (one per\n   259→#' observation).\n   260→#' @param ... Additional arguments passed on to `wis()` from functions\n   261→#'   `overprediction_quantile()`, `underprediction_quantile()` and\n   262→#'   `dispersion_quantile()`.\n   263→#' @export\n   264→#' @rdname wis\n   265→#' @keywords metric\n   266→dispersion_quantile <- function(observed, predicted, quantile_level, ...) {\n   267→  args <- list(...)\n   268→  args$separate_results <- TRUE\n   269→  assert_input_quantile(observed, predicted, quantile_level)\n   270→  out <- do.call(\n   271→    wis, c(list(observed), list(predicted), list(quantile_level), args)\n   272→  )\n   273→  return(out$dispersion)\n   274→}\n   275→\n   276→\n   277→#' @return\n   278→#' `overprediction_quantile()`: a numeric vector with overprediction values\n   279→#' (one per observation).\n   280→#' @export\n   281→#' @rdname wis\n   282→#' @keywords metric\n   283→overprediction_quantile <- function(observed, predicted, quantile_level, ...) {\n   284→  args <- list(...)\n   285→  args$separate_results <- TRUE\n   286→  assert_input_quantile(observed, predicted, quantile_level)\n   287→  out <- do.call(\n   288→    wis, c(list(observed), list(predicted), list(quantile_level), args)\n   289→  )\n   290→  return(out$overprediction)\n   291→}\n   292→\n   293→\n   294→#' @return\n   295→#' `underprediction_quantile()`: a numeric vector with underprediction values\n   296→#' (one per observation)\n   297→#' @export\n   298→#' @rdname wis\n   299→#' @keywords metric\n   300→underprediction_quantile <- function(observed, predicted, quantile_level, ...) {\n   301→  args <- list(...)\n   302→  args$separate_results <- TRUE\n   303→  assert_input_quantile(observed, predicted, quantile_level)\n   304→  out <- do.call(\n   305→    wis, c(list(observed), list(predicted), list(quantile_level), args)\n   306→  )\n   307→  return(out$underprediction)\n   308→}\n   309→\n   310→\n   311→#' @title Interval coverage (for quantile-based forecasts)\n   312→#' @description\n   313→#' Check whether the observed value is within a given central\n   314→#' prediction interval. The prediction interval is defined by a lower and an\n   315→#' upper bound formed by a pair of predictive quantiles. For example, a 50%\n   316→#' prediction interval is formed by the 0.25 and 0.75 quantiles of the\n   317→#' predictive distribution.\n   318→#' @inheritParams wis\n   319→#' @inheritSection illustration-input-metric-quantile Input format\n   320→#' @param interval_range A single number with the range of the prediction\n   321→#'   interval in percent (e.g. 50 for a 50% prediction interval) for which you\n   322→#'   want to compute interval coverage.\n   323→#' @importFrom checkmate assert_number\n   324→#' @importFrom cli cli_warn\n   325→#' @return\n   326→#' A vector of length n with elements either TRUE,\n   327→#' if the observed value is within the corresponding prediction interval, and\n   328→#' FALSE otherwise.\n   329→#' @name interval_coverage\n   330→#' @export\n   331→#' @keywords metric\n   332→#' @examples\n   333→#' observed <- c(1, -15, 22)\n   334→#' predicted <- rbind(\n   335→#'   c(-1, 0, 1, 2, 3),\n   336→#'   c(-2, 1, 2, 2, 4),\n   337→#'    c(-2, 0, 3, 3, 4)\n   338→#' )\n   339→#' quantile_level <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n   340→#' interval_coverage(observed, predicted, quantile_level)\n   341→interval_coverage <- function(observed, predicted,\n   342→                              quantile_level, interval_range = 50) {\n   343→  assert_input_quantile(observed, predicted, quantile_level)\n   344→  assert_number(interval_range)\n   345→  necessary_quantiles <- c(\n   346→    (100 - interval_range) / 2,\n   347→    100 - (100 - interval_range) / 2\n   348→  ) / 100\n   349→  if (!all(necessary_quantiles %in% quantile_level)) {\n   350→    #nolint start: keyword_quote_linter object_usage_linter\n   351→    cli_abort(\n   352→      c(\n   353→        \"!\" = \"To compute the interval coverage for an interval range of\n   354→        {.val {interval_range}%}, the {.val {necessary_quantiles}} quantiles\n   355→        are required\"\n   356→      )\n   357→    )\n   358→    #nolint end\n   359→  }\n   360→  r <- interval_range\n   361→  reformatted <- quantile_to_interval(observed, predicted, quantile_level)\n   362→  reformatted <- reformatted[interval_range %in% r]\n   363→  reformatted[, interval_coverage := (observed >= lower) & (observed <= upper)]\n   364→  return(reformatted$interval_coverage)\n   365→}\n   366→\n   367→\n   368→#' @title Determines bias of quantile forecasts\n   369→#'\n   370→#' @description\n   371→#' Determines bias from quantile forecasts. For an increasing number of\n   372→#' quantiles this measure converges against the sample based bias version\n   373→#' for integer and continuous forecasts.\n   374→#'\n   375→#' @details\n   376→#' For quantile forecasts, bias is measured as\n   377→#'\n   378→#' \\deqn{\n   379→#' B_t = (1 - 2 \\cdot \\max \\{i | q_{t,i} \\in Q_t \\land q_{t,i} \\leq x_t\\})\n   380→#'  \\mathbf{1}( x_t \\leq q_{t, 0.5}) \\\\\n   381→#' + (1 - 2 \\cdot \\min \\{i | q_{t,i} \\in Q_t \\land q_{t,i} \\geq x_t\\})\n   382→#'  1( x_t \\geq q_{t, 0.5}),}\n   383→#'\n   384→#' where \\eqn{Q_t} is the set of quantiles that form the predictive\n   385→#' distribution at time \\eqn{t} and \\eqn{x_t} is the observed value. For\n   386→#' consistency, we define \\eqn{Q_t} such that it always includes the element\n   387→#' \\eqn{q_{t, 0} = - \\infty} and \\eqn{q_{t,1} = \\infty}.\n   388→#' \\eqn{1()} is the indicator function that is \\eqn{1} if the\n   389→#' condition is satisfied and \\eqn{0} otherwise.\n   390→#'\n   391→#' In clearer terms, bias \\eqn{B_t} is:\n   392→#' - \\eqn{1 - 2 \\cdot} the maximum percentile rank for which the corresponding\n   393→#' quantile is still smaller than or equal to the observed value,\n   394→#' *if the observed value is smaller than the median of the predictive\n   395→#' distribution.*\n   396→#' - \\eqn{1 - 2 \\cdot} the minimum percentile rank for which the corresponding\n   397→#' quantile is still larger than or equal to the observed value *if the observed\n   398→#' value is larger\n   399→#' than the median of the predictive distribution.*.\n   400→#' - \\eqn{0} *if the observed value is exactly the median* (both terms cancel\n   401→#' out)\n   402→#'\n   403→#' Bias can assume values between -1 and 1 and is 0 ideally (i.e. unbiased).\n   404→#'\n   405→#' Note that if the given quantiles do not contain the median, the median is\n   406→#' imputed as a linear interpolation of the two innermost quantiles. If the\n   407→#' median is not available and cannot be imputed, an error will be thrown.\n   408→#' Note that in order to compute bias, quantiles must be non-decreasing with\n   409→#' increasing quantile levels.\n   410→#'\n   411→#' For a large enough number of quantiles, the\n   412→#' percentile rank will equal the proportion of predictive samples below the\n   413→#' observed value, and the bias metric coincides with the one for\n   414→#' continuous forecasts (see [bias_sample()]).\n   415→#'\n   416→#' @param quantile_level Vector of of size N with the quantile levels\n   417→#'   for which predictions were made. Note that if this does not contain the\n   418→#'   median (0.5) then the median is imputed as being the mean of the two\n   419→#'   innermost quantiles.\n   420→#' @param na.rm Logical. Should missing values be removed?\n   421→#' @importFrom cli cli_inform\n   422→#' @inheritParams wis\n   423→#' @inheritSection illustration-input-metric-quantile Input format\n   424→#' @returns scalar with the quantile bias for a single quantile prediction\n   425→#' @export\n   426→#' @keywords metric\n   427→#' @examples\n   428→#' predicted <- matrix(c(1.5:23.5, 3.3:25.3), nrow = 2, byrow = TRUE)\n   429→#' quantile_level <- c(0.01, 0.025, seq(0.05, 0.95, 0.05), 0.975, 0.99)\n   430→#' observed <- c(15, 12.4)\n   431→#' bias_quantile(observed, predicted, quantile_level)\n   432→bias_quantile <- function(observed, predicted, quantile_level, na.rm = TRUE) {\n   433→  assert_input_quantile(observed, predicted, quantile_level)\n   434→  # for bias quantile to work, at least one quantile level has to be <= 0.5\n   435→  # and at least one >= 0.5\n   436→  assert_vector(quantile_level[quantile_level <= 0.5], min.len = 1)\n   437→  assert_vector(quantile_level[quantile_level >= 0.5], min.len = 1)\n   438→\n   439→  n <- length(observed)\n   440→  N <- length(quantile_level)\n   441→  if (is.null(dim(predicted))) {\n   442→    dim(predicted) <- c(n, N)\n   443→  }\n   444→  if (!(0.5 %in% quantile_level)) {\n   445→    #nolint start: keyword_quote_linter\n   446→    cli_inform(\n   447→      c(\n   448→        \"i\" = \"Median not available, interpolating median from the two\n   449→        innermost quantiles in order to compute bias.\"\n   450→      )\n   451→    )\n   452→    #nolint end\n   453→  }\n   454→  bias <- sapply(1:n, function(i) {\n   455→    bias_quantile_single_vector(\n   456→      observed[i], predicted[i, ], quantile_level, na.rm\n   457→    )\n   458→  })\n   459→  return(bias)\n   460→}\n   461→\n   462→\n   463→#' Compute bias for a single vector of quantile predictions\n   464→#' @description\n   465→#' Internal function to compute bias for a single observed value,\n   466→#' a vector of predicted values and a vector of quantiles.\n   467→#' @param observed Scalar with the observed value.\n   468→#' @param predicted Vector of length N (corresponding to the number of\n   469→#'   quantiles) that holds predictions.\n   470→#' @inheritParams bias_quantile\n   471→#' @importFrom cli cli_abort\n   472→#' @returns scalar with the quantile bias for a single quantile prediction\n   473→#' @keywords internal\n   474→bias_quantile_single_vector <- function(observed, predicted,\n   475→                                        quantile_level, na.rm) {\n   476→\n   477→  assert_number(observed)\n   478→  # other checks should have happend before\n   479→\n   480→  predicted_has_NAs <- anyNA(predicted)\n   481→  quantile_has_NAs <- anyNA(quantile_level)\n   482→\n   483→  if (any(predicted_has_NAs, quantile_has_NAs)) {\n   484→    if (!na.rm) {\n   485→      return(NA_real_)\n   486→    }\n   487→    quantile_level <- quantile_level[!is.na(predicted)]\n   488→    predicted <- predicted[!is.na(predicted)]\n   489→    predicted <- predicted[!is.na(quantile_level)]\n   490→    quantile_level <- quantile_level[!is.na(quantile_level)]\n   491→  }\n   492→\n   493→  order <- order(quantile_level)\n   494→  predicted <- predicted[order]\n   495→  if (!all(diff(predicted) >= 0)) {\n   496→    #nolint start: keyword_quote_linter\n   497→    cli_abort(\n   498→      c(\n   499→        \"!\" = \"Predictions must not be decreasing with increasing\n   500→        quantile level.\"\n   501→      )\n   502→    )\n   503→    #nolint end\n   504→  }\n   505→\n   506→  median_prediction <- interpolate_median(predicted, quantile_level)\n   507→\n   508→  if (observed == median_prediction) {\n   509→    bias <- 0\n   510→    return(bias)\n   511→  } else if (observed < median_prediction) {\n   512→    if (observed < min(predicted)) {\n   513→      bias <- 1\n   514→    } else {\n   515→      q <- max(quantile_level[predicted <= observed])\n   516→      bias <- 1 - 2 * q\n   517→    }\n   518→  } else if (observed > median_prediction) {\n   519→    if (observed > max(predicted)) {\n   520→      bias <- -1\n   521→    } else {\n   522→      q <- min(quantile_level[predicted >= observed])\n   523→      bias <- 1 - 2 * q\n   524→    }\n   525→  }\n   526→  return(bias)\n   527→}\n   528→\n   529→#' Helper function to interpolate the median prediction if it is not available\n   530→#' @description\n   531→#' Internal function to interpolate the median prediction if it is not\n   532→#' available in the given quantile levels.\n   533→#' This is done using linear interpolation between the two innermost quantiles.\n   534→#' @inheritParams bias_quantile_single_vector\n   535→#' @inheritSection illustration-input-metric-quantile Input format\n   536→#' @returns scalar with the imputed median prediction\n   537→#' @keywords internal\n   538→interpolate_median <- function(predicted, quantile_level) {\n   539→  if (0.5 %in% quantile_level) {\n   540→    median_prediction <- predicted[quantile_level == 0.5]\n   541→  } else {\n   542→    # determine the two innermost quantiles\n   543→    upper_q_level <- max(quantile_level[quantile_level < 0.5])\n   544→    lower_q_level <- min(quantile_level[quantile_level > 0.5])\n   545→    upper_value <- predicted[quantile_level == upper_q_level]\n   546→    lower_value <- predicted[quantile_level == lower_q_level]\n   547→\n   548→    # do a linear interpolation\n   549→    # weight is the proportion of the distance between the lower quantile and\n   550→    # the median relative to the distance between the upper and lower quantile\n   551→    w <- (0.5 - lower_q_level) / (upper_q_level - lower_q_level)\n   552→    median_prediction <- lower_value + w * (upper_value - lower_value)\n   553→  }\n   554→  return(median_prediction)\n   555→}\n   556→\n   557→\n   558→#' Absolute error of the median (quantile-based version)\n   559→#'\n   560→#' Compute the absolute error of the median calculated as\n   561→#' \\deqn{\n   562→#'   |\\text{observed} - \\text{median prediction}|\n   563→#' }\n   564→#' The median prediction is the predicted value for which quantile_level == 0.5.\n   565→#' The function requires 0.5 to be among the quantile levels in `quantile_level`.\n   566→#'\n   567→#' @inheritParams wis\n   568→#' @inheritSection illustration-input-metric-quantile Input format\n   569→#' @returns Numeric vector of length N with the absolute error of the median.\n   570→#' @seealso [ae_median_sample()]\n   571→#' @importFrom stats median\n   572→#' @importFrom cli cli_warn\n   573→#' @keywords metric\n   574→#' @examples\n   575→#' observed <- rnorm(30, mean = 1:30)\n   576→#' predicted_values <- replicate(3, rnorm(30, mean = 1:30))\n   577→#' ae_median_quantile(\n   578→#'   observed, predicted_values, quantile_level = c(0.2, 0.5, 0.8)\n   579→#' )\n   580→#' @export\n   581→ae_median_quantile <- function(observed, predicted, quantile_level) {\n   582→  assert_input_quantile(observed, predicted, quantile_level)\n   583→  if (!any(quantile_level == 0.5)) {\n   584→    #nolint start: keyword_quote_linter\n   585→    cli_abort(\n   586→      c(\n   587→        \"!\" = \"In order to compute the absolute error of the median,\n   588→        {.val 0.5} must be among the quantiles given\"\n   589→      )\n   590→    )\n   591→    #nolint end\n   592→  }\n   593→  if (is.null(dim(predicted))) {\n   594→    predicted <- matrix(predicted, nrow = 1)\n   595→  }\n   596→  predicted <- predicted[, quantile_level == 0.5]\n   597→  abs_error_median <- abs(observed - predicted)\n   598→  return(abs_error_median)\n   599→}\n   600→\n   601→\n   602→#' @title Quantile score\n   603→#'\n   604→#' @description\n   605→#' Proper Scoring Rule to score quantile predictions. Smaller values are better.\n   606→#' The quantile score is closely related to the interval score (see [wis()]) and\n   607→#' is the quantile equivalent that works with single quantiles instead of\n   608→#' central prediction intervals.\n   609→#'\n   610→#' The quantile score, also called pinball loss, for a single quantile\n   611→#' level \\eqn{\\tau} is defined as\n   612→#' \\deqn{\n   613→#'   \\text{QS}_\\tau(F, y) = 2 \\cdot \\{ \\mathbf{1}(y \\leq q_\\tau) - \\tau\\} \\cdot (q_\\tau - y) =\n   614→#'   \\begin{cases}\n   615→#' 2 \\cdot (1 - \\tau) * q_\\tau - y,       & \\text{if } y \\leq q_\\tau\\\\\n   616→#' 2 \\cdot \\tau * |q_\\tau - y|,           & \\text{if } y > q_\\tau,\n   617→#' \\end{cases}\n   618→#' }\n   619→#' with \\eqn{q_\\tau} being the \\eqn{\\tau}-quantile of the predictive\n   620→#' distribution \\eqn{F}, and \\eqn{\\mathbf{1}(\\cdot)} the indicator function.\n   621→#'\n   622→#' The weighted interval score for a single prediction interval can be obtained\n   623→#' as the average of the quantile scores for the lower and upper quantile of\n   624→#' that prediction interval:\n   625→#' \\deqn{\n   626→#'   \\text{WIS}_\\alpha(F, y) = \\frac{\\text{QS}_{\\alpha/2}(F, y)\n   627→#'   + \\text{QS}_{1 - \\alpha/2}(F, y)}{2}.\n   628→#' }\n   629→#' See the SI of Bracher et al. (2021) for more details.\n   630→#'\n   631→#' `quantile_score()` returns the average quantile score across the quantile\n   632→#' levels provided. For a set of quantile levels that form pairwise central\n   633→#' prediction intervals, the quantile score is equivalent to the interval score.\n   634→#' @returns Numeric vector of length n with the quantile score. The scores are\n   635→#' averaged across quantile levels if multiple quantile levels are provided\n   636→#' (the result of calling `rowMeans()` on the matrix of quantile scores that\n   637→#' is computed based on the observed and predicted values).\n   638→#' @inheritParams wis\n   639→#' @inheritSection illustration-input-metric-quantile Input format\n   640→#' @examples\n   641→#' observed <- rnorm(10, mean = 1:10)\n   642→#' alpha <- 0.5\n   643→#'\n   644→#' lower <- qnorm(alpha / 2, observed)\n   645→#' upper <- qnorm((1 - alpha / 2), observed)\n   646→#'\n   647→#' qs_lower <- quantile_score(observed,\n   648→#'   predicted = matrix(lower),\n   649→#'   quantile_level = alpha / 2\n   650→#' )\n   651→#' qs_upper <- quantile_score(observed,\n   652→#'   predicted = matrix(upper),\n   653→#'   quantile_level = 1 - alpha / 2\n   654→#' )\n   655→#' interval_score <- (qs_lower + qs_upper) / 2\n   656→#' interval_score2 <- quantile_score(\n   657→#'   observed,\n   658→#'   predicted = cbind(lower, upper),\n   659→#'   quantile_level = c(alpha / 2, 1 - alpha / 2)\n   660→#' )\n   661→#'\n   662→#' # this is the same as the following\n   663→#' wis(\n   664→#'   observed,\n   665→#'   predicted = cbind(lower, upper),\n   666→#'   quantile_level = c(alpha / 2, 1 - alpha / 2)\n   667→#' )\n   668→#' @importFrom checkmate test_atomic_vector\n   669→#' @export\n   670→#' @keywords metric\n   671→#' @references Strictly Proper Scoring Rules, Prediction,and Estimation,\n   672→#' Tilmann Gneiting and Adrian E. Raftery, 2007, Journal of the American\n   673→#' Statistical Association, Volume 102, 2007 - Issue 477\n   674→#'\n   675→#' Evaluating epidemic forecasts in an interval format,\n   676→#' Johannes Bracher, Evan L. Ray, Tilmann Gneiting and Nicholas G. Reich, 2021,\n   677→#' <https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618>\n   678→quantile_score <- function(observed,\n   679→                           predicted,\n   680→                           quantile_level,\n   681→                           weigh = TRUE) {\n   682→  assert_input_quantile(observed, predicted, quantile_level)\n   683→\n   684→  # compute score - this is the version explained in the SI of Bracher et. al.\n   685→  obs_smaller_pred <- observed <= predicted\n   686→\n   687→  if (test_atomic_vector(predicted)) {\n   688→    obs_smaller_pred <- matrix(obs_smaller_pred, nrow = 1)\n   689→  }\n   690→  # subtract quantile level tau from result of indicator function (1(y <= q_tau))\n   691→  # (subtraction is element-wise and by row)\n   692→  tau_diff <- sweep(obs_smaller_pred, 2, quantile_level)\n   693→\n   694→  error <- predicted - observed\n   695→\n   696→  score <- 2 * tau_diff * error\n   697→\n   698→  # By default, the quantile score already corresponds to the WIS such that\n   699→  # simply averaging the quantile scores gives the WIS\n   700→  if (weigh) {\n   701→    return(rowMeans(score))\n   702→  } else {\n   703→    # adapt score such that mean of adapted quantile scores corresponds to\n   704→    # unweighted interval score of the corresponding prediction interval\n   705→    central_interval <- abs(0.5 - quantile_level) * 2\n   706→    alpha <- 1 - central_interval\n   707→\n   708→    # unweighted score' = 2 * score / alpha\n   709→    score <- 2 * sweep(score, 2, alpha, FUN = \"/\")\n   710→    return(rowMeans(score))\n   711→  }\n   712→}\n   713→"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"31fbf47f-5b1d-48f5-8c40-9ecfe7cd921a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018QN5ipdurqYSNDTq1Gr3zH","type":"tool_result","content":"     1→# ==================== Functions internally used for scoring ===================\n     2→# These functions would ideally be replaced in the future\n     3→\n     4→#' Transform from a quantile format to an interval format\n     5→#' @description\n     6→#' Internal helper function to transform from a quantile format to an interval\n     7→#' format (which is no longer a supported forecast format, but still used\n     8→#' internally. The function mimics an S3 generic, but is not actually an S3\n     9→#' generic, as we want the functions to be internal and not exported.)\n    10→#'\n    11→#' **Quantile format**\n    12→#' In a quantile format, a prediction is characterised by one or multiple\n    13→#' predicted values and the corresponding quantile levels. For example, a\n    14→#' prediction in a quantile format could be represented by the 0.05, 0.25, 0.5,\n    15→#' 0.75 and 0.95 quantiles of the predictive distribution.\n    16→#'\n    17→#' **Interval format**\n    18→#' In the interval format, two quantiles are assumed to form a prediction\n    19→#' interval. Prediction intervals need to be symmetric around the median and\n    20→#' are characterised by a lower and an upper bound. The lower bound is defined\n    21→#' by the lower quantile and the upper bound is defined by the upper quantile.\n    22→#' A 90% prediction interval, for example, covers 90% of the probability mass\n    23→#' and is defined by the 5% and 95% quantiles. A forecast could therefore\n    24→#' be characterised by one or multiple prediction intervals, e.g. the lower\n    25→#' and upper bounds of the 50% and 90% prediction intervals (corresponding to\n    26→#' the 0.25 and 0.75 as well as the 0.05 and 0.095 quantiles).\n    27→#' @param ... Arguments\n    28→#' @returns A data.table with forecasts in an interval format.\n    29→#' @keywords internal\n    30→#' @importFrom cli cli_abort\n    31→quantile_to_interval <- function(...) {\n    32→  dots <- list(...)\n    33→  if (is.data.frame(dots[[1]])) {\n    34→    do.call(quantile_to_interval_dataframe, dots)\n    35→  } else if (is.numeric(dots[[1]])) {\n    36→    do.call(quantile_to_interval_numeric, dots)\n    37→  } else {\n    38→    cli_abort(\"Input must be either a data.frame or a numeric vector.\")\n    39→  }\n    40→}\n    41→\n    42→\n    43→#' @param forecast A data.table with forecasts in a quantile-based format (see\n    44→#'   [as_forecast_quantile()]).\n    45→#' @param format The format of the output. Either \"long\" or \"wide\". If \"long\"\n    46→#'   (the default), there will be a column `boundary` (with values either\n    47→#'   \"upper\" or \"lower\" and a column `interval_range` that contains the range of\n    48→#'   the interval. If \"wide\", there will be a column `interval_range` and two\n    49→#'   columns `lower` and `upper` that contain the lower and upper bounds of the\n    50→#'   prediction interval, respectively.\n    51→#' @param keep_quantile_col keep the `quantile_level` column in the final\n    52→#'   output after transformation (default is FALSE). This only works if\n    53→#'   `format = \"long\"`. If `format = \"wide\"`, the `quantile_level` column will\n    54→#'   always be dropped.\n    55→#' @return\n    56→#' *quantile_to_interval_dataframe*:\n    57→#'   a data.table in an interval format (either \"long\" or \"wide\"), with or\n    58→#'   without a `quantile_level` column. Rows will not be reordered.\n    59→#' @importFrom data.table copy\n    60→#' @rdname quantile_to_interval\n    61→#' @keywords internal\n    62→quantile_to_interval_dataframe <- function(forecast,\n    63→                                           format = \"long\",\n    64→                                           keep_quantile_col = FALSE,\n    65→                                           ...) {\n    66→  # After this transformation, the object will no longer be a valid forecast\n    67→  # object so we unclass it\n    68→  forecast <- as.data.table(forecast)\n    69→\n    70→  forecast[, boundary := ifelse(quantile_level <= 0.5, \"lower\", \"upper\")]\n    71→  forecast[, interval_range := get_range_from_quantile(quantile_level)]\n    72→\n    73→  # add median quantile\n    74→  median <- forecast[quantile_level == 0.5, ]\n    75→  median[, boundary := \"upper\"]\n    76→\n    77→  forecast <- data.table::rbindlist(list(forecast, median))\n    78→  if (!keep_quantile_col) {\n    79→    forecast[, quantile_level := NULL]\n    80→  }\n    81→\n    82→  if (length(unique(forecast$boundary)) < 2) {\n    83→    cli_abort(\n    84→      c(\n    85→        #nolint start: keyword_quote_linter\n    86→        `!` = \"No valid forecast intervals found.\",\n    87→        `i` = \"A forecast interval comprises two\n    88→      quantiles with quantile levels symmetric around the median\n    89→      (e.g. 0.25 and 0.75)\"\n    90→        #nolint end\n    91→      )\n    92→    )\n    93→  }\n    94→\n    95→  if (format == \"wide\") {\n    96→    suppressWarnings(forecast[, \"quantile_level\" := NULL])\n    97→    forecast <- dcast(forecast, ... ~ boundary, value.var = \"predicted\")\n    98→    # if there are NA values in `predicted`, this introduces a column \"NA\"\n    99→    if (\"NA\" %in% colnames(forecast) && all(is.na(forecast[[\"NA\"]]))) {\n   100→      forecast[, \"NA\" := NULL]\n   101→    }\n   102→  }\n   103→  return(forecast[])\n   104→}\n   105→\n   106→\n   107→#' @inheritParams wis\n   108→#' @return\n   109→#' *quantile_to_interval.numeric*:\n   110→#' a data.table in a wide interval format with columns `forecast_id`,\n   111→#' `observed`, `lower`, `upper`, and `interval_range`. The `forecast_id` column\n   112→#' is a unique identifier for each forecast. Rows will be reordered according to\n   113→#' `forecast_id` and `interval_range`.\n   114→#' @rdname quantile_to_interval\n   115→#' @keywords internal\n   116→quantile_to_interval_numeric <- function(observed,\n   117→                                         predicted,\n   118→                                         quantile_level,\n   119→                                         ...) {\n   120→  assert_input_quantile(observed, predicted, quantile_level)\n   121→\n   122→  n <- length(observed)\n   123→  N <- length(quantile_level)\n   124→\n   125→  dt <- data.table(\n   126→    forecast_id = rep(1:n, each = N),\n   127→    observed = rep(observed, each = N),\n   128→    predicted = as.vector(t(predicted)),\n   129→    quantile_level = quantile_level\n   130→  )\n   131→  out <- quantile_to_interval(dt, format = \"wide\")\n   132→  out <- out[order(forecast_id, interval_range)]\n   133→  return(out)\n   134→}\n   135→\n   136→\n   137→#' @title Change data from a sample-based format to a long interval range format\n   138→#'\n   139→#' @description\n   140→#'\n   141→#' Transform data from a format that is based on predictive samples to a format\n   142→#' based on interval ranges.\n   143→#'\n   144→#' @inheritParams as_forecast_quantile\n   145→#' @param keep_quantile_col keep quantile_level column, default is TRUE\n   146→#' @returns A data.table in a long interval interval range format\n   147→#' @importFrom data.table as.data.table\n   148→#' @importFrom stats quantile\n   149→#' @keywords internal\n   150→\n   151→sample_to_interval_long <- function(data,\n   152→                                    interval_range = c(0, 50, 90),\n   153→                                    type = 7,\n   154→                                    keep_quantile_col = TRUE) {\n   155→  data <- ensure_data.table(data)\n   156→\n   157→  lower_quantiles <- (100 - interval_range) / 200\n   158→  upper_quantiles <- 1 - lower_quantiles\n   159→  quantile_levels <- sort(unique(c(lower_quantiles, upper_quantiles)))\n   160→\n   161→  data <- as_forecast_quantile(\n   162→    data,\n   163→    probs = quantile_levels,\n   164→    type = type\n   165→  )\n   166→\n   167→  data <- quantile_to_interval(data, keep_quantile_col = keep_quantile_col)\n   168→\n   169→  return(data[])\n   170→}\n   171→\n   172→#' Get interval range belonging to a quantile\n   173→#' @description\n   174→#' Every quantile can be thought of either as the lower or the\n   175→#' upper bound of a symmetric central prediction interval. This helper function\n   176→#' returns the range of the central prediction interval to which the quantile\n   177→#' belongs.\n   178→#'\n   179→#' Due to numeric instability that sometimes occurred in the past, ranges are\n   180→#' rounded to 10 decimal places. This is not a problem for the vast majority of\n   181→#' use cases, but it is something to be aware of.\n   182→#' @param quantile_level A numeric vector of quantile levels of size N.\n   183→#' @returns a numeric vector of interval ranges of size N\n   184→#' @keywords internal\n   185→get_range_from_quantile <- function(quantile_level) {\n   186→  boundary <- ifelse(quantile_level <= 0.5, \"lower\", \"upper\")\n   187→  interval_range <- ifelse(\n   188→    boundary == \"lower\",\n   189→    round((1 - 2 * quantile_level) * 100, digits = 10),\n   190→    round((2 * quantile_level - 1) * 100, digits = 10)\n   191→  )\n   192→  return(interval_range)\n   193→}\n   194→"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"7f58076a-a954-49be-9c3b-7714cc2d42f7"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01QUDn76QWXYE5icatuNtmiU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GpWHEGQpL9WwSu3XXsNhv6","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1811,"cache_read_input_tokens":11303,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1811},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"c1bbe614-2ae9-48a3-bb35-c0a22cc9ca66"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GpWHEGQpL9WwSu3XXsNhv6","type":"tool_result","content":"     1→# ==============================================================================\n     2→# `get_coverage()` # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→ex_coverage <- example_quantile[model == \"EuroCOVIDhub-ensemble\"]\n     5→\n     6→test_that(\"get_coverage() works as expected\", {\n     7→  cov <- example_quantile |>\n     8→    get_coverage(by = get_forecast_unit(example_quantile))\n     9→\n    10→  expect_identical(\n    11→    sort(colnames(cov)),\n    12→    sort(c(get_forecast_unit(example_quantile), c(\n    13→      \"interval_range\", \"quantile_level\", \"interval_coverage\", \"interval_coverage_deviation\",\n    14→      \"quantile_coverage\", \"quantile_coverage_deviation\"\n    15→    )))\n    16→  )\n    17→\n    18→  expect_identical(nrow(cov), nrow(na.omit(example_quantile)))\n    19→\n    20→  expect_s3_class(\n    21→    cov,\n    22→    c(\"data.table\", \"data.frame\"),\n    23→    exact = TRUE\n    24→  )\n    25→})\n    26→\n    27→test_that(\"get_coverage() outputs an object of class c('data.table', 'data.frame'\", {\n    28→  cov <- get_coverage(example_quantile)\n    29→  expect_s3_class(cov, c(\"data.table\", \"data.frame\"), exact = TRUE)\n    30→})\n    31→\n    32→test_that(\"get_coverage() can deal with non-symmetric prediction intervals\", {\n    33→  # the expected result is that `get_coverage()` just works. However,\n    34→  # all interval coverages with missing values should just be `NA`\n    35→  test <- data.table::copy(example_quantile)\n    36→  test <- test[!quantile_level %in% c(0.2, 0.3, 0.5)]\n    37→\n    38→  cov <- expect_no_condition(get_coverage(test))\n    39→\n    40→  prediction_intervals <- get_range_from_quantile(c(0.2, 0.3, 0.5))\n    41→\n    42→  missing <- cov[interval_range %in% prediction_intervals]\n    43→  not_missing <- cov[!interval_range %in% prediction_intervals]\n    44→\n    45→  expect_true(all(is.na(missing$interval_coverage)))\n    46→  expect_false(anyNA(not_missing))\n    47→\n    48→  # test for a version where values are not missing, but just `NA`\n    49→  # since `get_coverage()` calls `na.omit`, the result should be the same.\n    50→  test <- data.table::copy(example_quantile)\n    51→  test <- test[quantile_level %in% c(0.2, 0.3, 0.5), predicted := NA]\n    52→  cov2 <- get_coverage(test)\n    53→  expect_identical(cov, cov2)\n    54→})\n    55→\n    56→\n    57→# ==============================================================================\n    58→# plot_interval_coverage() # nolint: commented_code_linter\n    59→# ==============================================================================\n    60→test_that(\"plot_interval_coverage() works as expected\", {\n    61→  coverage <- example_quantile |>\n    62→    na.omit() |>\n    63→    as_forecast_quantile() |>\n    64→    get_coverage(by = \"model\")\n    65→  p <- plot_interval_coverage(coverage)\n    66→  expect_s3_class(p, \"ggplot\")\n    67→  skip_on_cran()\n    68→  suppressWarnings(vdiffr::expect_doppelganger(\"plot_interval_coverage\", p))\n    69→\n    70→  # make sure that plot_interval_coverage() doesn't drop column names\n    71→  expect_true(all(c(\n    72→    \"interval_coverage\", \"interval_coverage_deviation\",\n    73→    \"quantile_coverage\", \"quantile_coverage_deviation\"\n    74→  ) %in%\n    75→    names(coverage)))\n    76→})\n    77→\n    78→\n    79→# ==============================================================================\n    80→# plot_quantile_coverage() # nolint: commented_code_linter\n    81→# ==============================================================================\n    82→test_that(\"plot_quantile_coverage() works as expected\", {\n    83→  coverage <- example_quantile |>\n    84→    na.omit() |>\n    85→    as_forecast_quantile() |>\n    86→    get_coverage(by = c(\"model\", \"quantile_level\"))\n    87→\n    88→  p <- plot_quantile_coverage(coverage)\n    89→  expect_s3_class(p, \"ggplot\")\n    90→  skip_on_cran()\n    91→  suppressWarnings(vdiffr::expect_doppelganger(\"plot_quantile_coverage\", p))\n    92→})\n    93→"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"12c60890-fc2d-4d1f-ae91-e0f85884ed51"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01QUDn76QWXYE5icatuNtmiU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UCEH8LvzMfELDweVRYGQgW","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1811,"cache_read_input_tokens":11303,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1811},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"74065fcb-72cb-4c7c-adab-3025eb2d4bfe"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UCEH8LvzMfELDweVRYGQgW","type":"tool_result","content":"     1→data.table::setDTthreads(2) # restricts number of cores used on CRAN\n     2→\n     3→metrics_no_cov <- get_metrics(\n     4→  example_quantile,\n     5→  exclude = c(\"interval_coverage_50\", \"interval_coverage_90\")\n     6→)\n     7→metrics_no_cov_no_ae <- get_metrics(\n     8→  example_quantile,\n     9→  exclude = c(\n    10→    \"interval_coverage_50\", \"interval_coverage_90\",\n    11→    \"ae_median\"\n    12→  )\n    13→)\n    14→\n    15→example_quantile_df <- as.data.frame(na.omit(example_quantile))\n    16→checkmate::assert_number(length(class(example_quantile_df)))\n    17→\n    18→\n    19→# pre-computed scores\n    20→scores_quantile <- score(example_quantile)\n    21→scores_sample_continuous <- score(example_sample_continuous)\n    22→scores_sample_discrete <- score(example_sample_discrete)\n    23→scores_point <- score(example_point)\n    24→scores_binary <- score(example_binary)\n    25→scores_nominal <- score(example_nominal)\n    26→"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"f22e0b8d-0d25-4a1d-9016-7ad8d6ff0d1d"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01Favoejh4NHCjYFVRdM1fGC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AxuHomFDNtryVjdm2xpa6i","name":"Glob","input":{"pattern":"**/tests/testthat/test-*interval*.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":19403,"cache_read_input_tokens":13114,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":19403},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"bedee1ec-8cd8-429d-80f1-226f2c2cebbc"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AxuHomFDNtryVjdm2xpa6i","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-helper-quantile-interval-range.R\n/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-interval-range.R"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"57473723-77db-4f33-befe-584d7e25ce4d"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01Favoejh4NHCjYFVRdM1fGC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NYJCWmpxQjSCDjXzJk5FJk","name":"Glob","input":{"pattern":"**/tests/testthat/test-metrics-quantile.R","path":"/Users/nikos/github-synced/scoringutils-pipeline"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":19403,"cache_read_input_tokens":13114,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":19403},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"36bd3927-9c18-469e-a2a4-25240e1cfed3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NYJCWmpxQjSCDjXzJk5FJk","type":"tool_result","content":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"cd4b14c2-4304-4788-8eab-0fc668cd68d3"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01XF7wixoLv2rpzVrNFTgJr6","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016aauiHcqgKywED3ayWUrDD","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":398,"cache_read_input_tokens":32517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":398},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"f0ce9e3b-0424-44d0-ae7c-b09a6a0889ca"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016aauiHcqgKywED3ayWUrDD","type":"tool_result","content":"     1→observed <- c(1, -15, 22)\n     2→predicted <- rbind(\n     3→  c(-1, 0, 1, 2, 3),\n     4→  c(-2, 1, 2, 2, 4),\n     5→  c(-2, 0, 3, 3, 4)\n     6→)\n     7→quantile_level <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n     8→\n     9→# covidHubUtils test:\n    10→y <- c(1, -15, 22)\n    11→forecast_quantiles_matrix <- rbind(\n    12→  c(-1, 0, 1, 2, 3),\n    13→  c(-2, 1, 2, 2, 4),\n    14→  c(-2, 0, 3, 3, 4)\n    15→)\n    16→forecast_quantile_probs <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n    17→\n    18→\n    19→# ==============================================================================\n    20→# check_input_quantile() # nolint: commented_code_linter\n    21→# ==============================================================================\n    22→test_that(\"check_input_quantile() works as expected\", {\n    23→  # expect no error if dimensions are ok\n    24→  expect_true(\n    25→    check_input_quantile(\n    26→      1:10, matrix(1:20, nrow = 10),\n    27→      quantile_level = c(0.1, 0.9)\n    28→    )\n    29→  )\n    30→\n    31→  # expect error if dimensions are not ok\n    32→  expect_match(\n    33→    check_input_quantile(\n    34→      1:10, matrix(1:20, nrow = 10),\n    35→      quantile_level = seq(0.1, 0.9, length.out = 8)\n    36→    ),\n    37→    \"Assertion on 'predicted' failed: Must have exactly 8 cols, but has 2 cols.\"\n    38→  )\n    39→})\n    40→\n    41→# ============================================================================ #\n    42→# Input handling ===============================================================\n    43→# ============================================================================ #\n    44→test_that(\"Input checking for quantile forecasts works\", {\n    45→  # everything correct\n    46→  expect_no_condition(\n    47→    scoringutils:::assert_input_quantile(observed, predicted, quantile_level) # nolint: undesirable_operator_linter\n    48→  )\n    49→\n    50→  # quantile_level > 1 # nolint: commented_code_linter\n    51→  expect_error(\n    52→    scoringutils:::assert_input_quantile(observed, predicted, quantile_level + 1), # nolint: undesirable_operator_linter\n    53→    \"Assertion on 'quantile_level' failed: Element 1 is not <= 1.\"\n    54→  )\n    55→\n    56→  # quantile_level < 0 # nolint: commented_code_linter\n    57→  expect_error(\n    58→    scoringutils:::assert_input_quantile(observed, predicted, quantile_level - 1), # nolint: undesirable_operator_linter\n    59→    \"Assertion on 'quantile_level' failed: Element 1 is not >= 0.\"\n    60→  )\n    61→\n    62→  # 10 observations, but only 3 forecasts\n    63→  expect_error(\n    64→    scoringutils:::assert_input_quantile(1:10, predicted, quantile_level), # nolint: undesirable_operator_linter\n    65→    \"Assertion on 'predicted' failed: Must have exactly 10 rows, but has 3 rows.\"\n    66→  )\n    67→\n    68→  # observed value is a factor\n    69→  expect_error(\n    70→    scoringutils:::assert_input_quantile(factor(1:10), predicted, quantile_level), # nolint: undesirable_operator_linter\n    71→    \"Assertion on 'observed' failed: Must be of type 'numeric', not 'factor'.\"\n    72→  )\n    73→\n    74→  # observed is a single number and does not have the same length as predicted\n    75→  # There seems to be an issue with the error message: there is one \\n to many\n    76→  # such that the test fails when executed alone, but works when executed\n    77→  # together with others.\n    78→  expect_error(\n    79→    scoringutils:::assert_input_quantile(1, predicted, quantile_level), # nolint: undesirable_operator_linter\n    80→    \"Assertion failed.*check_numeric_vector.*check_matrix\"\n    81→  )\n    82→\n    83→  # predicted is a vector\n    84→  expect_error(\n    85→    scoringutils:::assert_input_quantile(observed, as.vector(predicted), quantile_level), # nolint: undesirable_operator_linter\n    86→    \"Assertion on 'predicted' failed: Must be of type 'matrix', not 'double'.\"\n    87→  )\n    88→})\n    89→\n    90→\n    91→# ============================================================================ #\n    92→# wis ==========================================================================\n    93→# ============================================================================ #a\n    94→test_that(\"wis works standalone, median only\", {\n    95→  y <- c(1, -15, 22)\n    96→  lower <- upper <- predicted_quantile <- c(1, 2, 3)\n    97→  quantile_probs <- 0.5\n    98→\n    99→  actual <- interval_score(y,\n   100→    lower = lower, upper = upper,\n   101→    weigh = TRUE,\n   102→    interval_range = 0\n   103→  )\n   104→\n   105→  actual_wis <- wis(\n   106→    observed = y,\n   107→    predicted = matrix(predicted_quantile),\n   108→    quantile_level = quantile_probs\n   109→  )\n   110→\n   111→  expected <- abs(y - lower)\n   112→\n   113→  expect_identical(actual, expected)\n   114→})\n   115→\n   116→test_that(\"`wis()` works within score for median forecast\", {\n   117→  test_data <- data.frame(\n   118→    observed = c(1, -15, 22),\n   119→    predicted = 1:3,\n   120→    quantile_level = rep(0.5, each = 3),\n   121→    model = \"model1\",\n   122→    date = 1:3,\n   123→    stringsAsFactors = FALSE\n   124→  ) |>\n   125→    as_forecast_quantile()\n   126→\n   127→  metrics <- get_metrics(example_quantile) |>\n   128→    select_metrics(select = c(\"ae_median\", \"wis\"))\n   129→  metrics$wis <- purrr::partial(metrics$wis, count_median_twice = TRUE)\n   130→\n   131→  eval <- score(test_data, metrics = metrics)\n   132→  expect_equal(eval$ae_median, eval$wis) # nolint: expect_identical_linter\n   133→})\n   134→\n   135→test_that(\"`wis()` equals `interval_score()`, 1 interval only\", {\n   136→  y <- c(1, -15, 22)\n   137→  lower <- c(0, 1, 0)\n   138→  upper <- c(2, 2, 3)\n   139→  quantile_probs <- c(0.25, 0.75)\n   140→  predicted <- matrix(c(lower, upper), ncol = 2)\n   141→\n   142→  alpha <- 0.5\n   143→  expected <- (upper - lower) * (alpha / 2) + c(0, 1 - (-15), 22 - 3)\n   144→\n   145→  actual <- interval_score(\n   146→    y,\n   147→    lower = lower, upper = upper,\n   148→    weigh = TRUE,\n   149→    interval_range = 50\n   150→  )\n   151→\n   152→  actual_wis <- wis(\n   153→    observed = y,\n   154→    predicted = predicted,\n   155→    quantile_level = quantile_probs\n   156→  )\n   157→\n   158→  expect_identical(actual, expected)\n   159→  expect_identical(actual_wis, expected)\n   160→})\n   161→\n   162→test_that(\"wis() works within score for one interval\", {\n   163→  test_data <- data.frame(\n   164→    observed = rep(c(1, -15, 22), times = 2),\n   165→    quantile_level = rep(c(0.25, 0.75), each = 3),\n   166→    predicted = c(0, 1, 0, 2, 2, 3),\n   167→    model = \"model1\",\n   168→    date = rep(1:3, times = 2),\n   169→    stringsAsFactors = FALSE\n   170→  ) |>\n   171→    as_forecast_quantile()\n   172→\n   173→  metrics <- get_metrics(example_quantile) |>\n   174→    select_metrics(select = \"wis\")\n   175→  metrics$wis <- purrr::partial(metrics$wis, count_median_twice = TRUE)\n   176→\n   177→  eval <- score(test_data, metrics = metrics)\n   178→\n   179→  eval <- summarise_scores(eval, by = c(\"model\", \"date\"))\n   180→\n   181→  lower <- c(0, 1, 0)\n   182→  upper <- c(2, 2, 3)\n   183→  alpha <- 0.5\n   184→\n   185→  expected <- (upper - lower) * (alpha / 2) + c(0, 1 - (-15), 22 - 3)\n   186→\n   187→  expect_equal(expected, eval$wis) # nolint: expect_identical_linter\n   188→})\n   189→\n   190→test_that(\"`wis()` works 1 interval and median\", {\n   191→  test_data <- data.frame(\n   192→    observed = rep(c(1, -15, 22), times = 3),\n   193→    quantile_level = rep(c(0.25, 0.5, 0.75), each = 3),\n   194→    predicted = c(0, 1, 0, 1, 2, 3, 2, 2, 3),\n   195→    model = \"model1\",\n   196→    date = rep(1:3, times = 3),\n   197→    stringsAsFactors = FALSE\n   198→  ) |>\n   199→    as_forecast_quantile()\n   200→\n   201→  metrics <- get_metrics(example_quantile, select = \"wis\")\n   202→  metrics$wis <- purrr::partial(metrics$wis, count_median_twice = TRUE)\n   203→\n   204→  eval <- score(test_data, metrics = metrics)\n   205→\n   206→  eval <- summarise_scores(eval, by = c(\"model\", \"date\"))\n   207→\n   208→  y <- c(1, -15, 22)\n   209→  quantile <- rbind(c(0, 1, 2), c(1, 2, 2), c(0, 3, 3))\n   210→  quantile_probs <- c(0.25, 0.5, 0.75)\n   211→\n   212→  alpha <- 0.5\n   213→  expected <- 0.5 * (\n   214→    abs(y - quantile[, 2]) +\n   215→      (quantile[, 3] - quantile[, 1]) * (alpha / 2) + c(0, 1 - (-15), 22 - 3)\n   216→  )\n   217→\n   218→  actual_wis <- wis(\n   219→    observed = y,\n   220→    predicted = quantile,\n   221→    quantile_level = quantile_probs,\n   222→    count_median_twice = TRUE\n   223→  )\n   224→\n   225→  expect_equal(eval$wis, expected) # nolint: expect_identical_linter\n   226→  expect_equal(actual_wis, expected) # nolint: expect_identical_linter\n   227→})\n   228→\n   229→test_that(\"wis works, 2 intervals and median\", {\n   230→  test_data <- data.frame(\n   231→    observed = rep(c(1, -15, 22), times = 5),\n   232→    quantile_level = rep(c(0.1, 0.25, 0.5, 0.75, 0.9), each = 3),\n   233→    predicted = c(\n   234→      -1, -2, -2, 0, 1, 0, 1, 2, 3,\n   235→      2, 2, 3, 3, 4, 4\n   236→    ),\n   237→    model = \"model1\",\n   238→    date = rep(1:3, times = 5),\n   239→    stringsAsFactors = FALSE\n   240→  ) |>\n   241→    as_forecast_quantile()\n   242→\n   243→  metrics <- get_metrics(example_quantile, select = \"wis\")\n   244→  metrics$wis <- purrr::partial(metrics$wis, count_median_twice = TRUE)\n   245→\n   246→  eval <- score(test_data, metrics = metrics)\n   247→\n   248→  eval <- summarise_scores(eval, by = c(\"model\", \"date\"))\n   249→\n   250→  quantile <- forecast_quantiles_matrix\n   251→  quantile_probs <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n   252→\n   253→  alpha1 <- 0.2\n   254→  alpha2 <- 0.5\n   255→\n   256→  expected <- (1 / 3) * (\n   257→    abs(y - quantile[, 3]) +\n   258→      (quantile[, 5] - quantile[, 1]) * (alpha1 / 2) + c(0, (-2) - (-15), 22 - 4) +\n   259→      (quantile[, 4] - quantile[, 2]) * (alpha2 / 2) + c(0, 1 - (-15), 22 - 3)\n   260→  )\n   261→\n   262→  actual_wis <- wis(\n   263→    observed = y,\n   264→    predicted = quantile,\n   265→    quantile_level = quantile_probs,\n   266→    count_median_twice = TRUE\n   267→  )\n   268→\n   269→  expect_equal( # nolint: expect_identical_linter\n   270→    as.numeric(eval$wis),\n   271→    as.numeric(expected)\n   272→  )\n   273→  expect_equal(actual_wis, expected) # nolint: expect_identical_linter\n   274→})\n   275→\n   276→# additional tests from the covidhubutils repo\n   277→test_that(\"wis is correct, median only - test corresponds to covidHubUtils\", {\n   278→  forecast_quantiles_matrix <- forecast_quantiles_matrix[, 3, drop = FALSE]\n   279→  forecast_quantile_probs <- forecast_quantile_probs[3]\n   280→\n   281→  target_end_dates <- as.Date(\"2020-01-01\") + c(7, 14, 7)\n   282→  horizons <- c(\"1\", \"2\", \"1\")\n   283→  locations <- c(\"01\", \"01\", \"02\")\n   284→  target_variables <- rep(\"inc death\", length(y))\n   285→\n   286→  forecast_target_end_dates <-\n   287→    rep(target_end_dates, times = ncol(forecast_quantiles_matrix))\n   288→  forecast_horizons <- rep(horizons, times = ncol(forecast_quantiles_matrix))\n   289→  forecast_locations <- rep(locations, times = ncol(forecast_quantiles_matrix))\n   290→  forecast_target_variables <-\n   291→    rep(target_variables, times = ncol(forecast_quantiles_matrix))\n   292→  forecast_quantile_probs <- rep(forecast_quantile_probs, each = length(y))\n   293→  forecast_quantiles <- forecast_quantiles_matrix\n   294→  dim(forecast_quantiles) <- prod(dim(forecast_quantiles))\n   295→\n   296→  test_truth <- data.frame(\n   297→    model = rep(\"truth_source\", length(y)),\n   298→    target_variable = target_variables,\n   299→    target_end_date = target_end_dates,\n   300→    location = locations,\n   301→    value = y,\n   302→    stringsAsFactors = FALSE\n   303→  )\n   304→\n   305→  n_forecasts <- length(forecast_quantiles)\n   306→  test_forecasts <- data.frame(\n   307→    model = rep(\"m1\", n_forecasts),\n   308→    forecast_date = rep(as.Date(\"2020-01-01\"), n_forecasts),\n   309→    location = forecast_locations,\n   310→    horizon = forecast_horizons,\n   311→    temporal_resolution = rep(\"wk\", n_forecasts),\n   312→    target_variable = forecast_target_variables,\n   313→    target_end_date = forecast_target_end_dates,\n   314→    type = rep(\"quantile\", n_forecasts),\n   315→    quantile_level = forecast_quantile_probs,\n   316→    value = forecast_quantiles,\n   317→    stringsAsFactors = FALSE\n   318→  )\n   319→  # make a version that conforms to scoringutils format\n   320→  truth_formatted <- data.table::as.data.table(test_truth)\n   321→  truth_formatted[, `:=`(model = NULL)]\n   322→  data.table::setnames(truth_formatted, old = \"value\", new = \"observed\")\n   323→\n   324→  forecasts_formated <- data.table::as.data.table(test_forecasts)\n   325→  data.table::setnames(forecasts_formated, old = \"value\", new = \"predicted\")\n   326→\n   327→  data_formatted <- merge(forecasts_formated, truth_formatted)\n   328→\n   329→  metrics <- get_metrics(example_quantile, select = \"wis\")\n   330→  metrics$wis <- purrr::partial(metrics$wis, count_median_twice = TRUE)\n   331→\n   332→  eval <- score(\n   333→    as_forecast_quantile(data_formatted),\n   334→    metrics = metrics\n   335→  )\n   336→\n   337→  expected <- abs(y - forecast_quantiles_matrix[, 1])\n   338→\n   339→  actual_wis <- wis(\n   340→    observed = y,\n   341→    predicted = matrix(forecast_quantiles_matrix),\n   342→    quantile_level = 0.5,\n   343→    count_median_twice = FALSE\n   344→  )\n   345→\n   346→  expect_equal(eval$wis, expected) # nolint: expect_identical_linter\n   347→  expect_equal(actual_wis, expected) # nolint: expect_identical_linter\n   348→})\n   349→\n   350→test_that(\"wis is correct, 1 interval only - test corresponds to covidHubUtils\", {\n   351→  forecast_quantiles_matrix <- forecast_quantiles_matrix[, c(1, 5), drop = FALSE]\n   352→  forecast_quantile_probs <- forecast_quantile_probs[c(1, 5)]\n   353→\n   354→  target_end_dates <- as.Date(\"2020-01-01\") + c(7, 14, 7)\n   355→  horizons <- c(\"1\", \"2\", \"1\")\n   356→  locations <- c(\"01\", \"01\", \"02\")\n   357→  target_variables <- rep(\"inc death\", length(y))\n   358→\n   359→  forecast_target_end_dates <-\n   360→    rep(target_end_dates, times = ncol(forecast_quantiles_matrix))\n   361→  forecast_horizons <- rep(horizons, times = ncol(forecast_quantiles_matrix))\n   362→  forecast_locations <- rep(locations, times = ncol(forecast_quantiles_matrix))\n   363→  forecast_target_variables <-\n   364→    rep(target_variables, times = ncol(forecast_quantiles_matrix))\n   365→  forecast_quantile_probs <- rep(forecast_quantile_probs, each = length(y))\n   366→  forecast_quantiles <- forecast_quantiles_matrix\n   367→  dim(forecast_quantiles) <- prod(dim(forecast_quantiles))\n   368→\n   369→  test_truth <- data.frame(\n   370→    model = rep(\"truth_source\", length(y)),\n   371→    target_variable = target_variables,\n   372→    target_end_date = target_end_dates,\n   373→    location = locations,\n   374→    value = y,\n   375→    stringsAsFactors = FALSE\n   376→  )\n   377→\n   378→  n_forecasts <- length(forecast_quantiles)\n   379→  test_forecasts <- data.frame(\n   380→    model = rep(\"m1\", n_forecasts),\n   381→    forecast_date = rep(as.Date(\"2020-01-01\"), n_forecasts),\n   382→    location = forecast_locations,\n   383→    horizon = forecast_horizons,\n   384→    temporal_resolution = rep(\"wk\", n_forecasts),\n   385→    target_variable = forecast_target_variables,\n   386→    target_end_date = forecast_target_end_dates,\n   387→    type = rep(\"quantile\", n_forecasts),\n   388→    quantile_level = forecast_quantile_probs,\n   389→    value = forecast_quantiles,\n   390→    stringsAsFactors = FALSE\n   391→  )\n   392→\n   393→  # make a version that conforms to scoringutils format\n   394→  truth_formatted <- data.table::as.data.table(test_truth)\n   395→  truth_formatted[, `:=`(model = NULL)]\n   396→  data.table::setnames(truth_formatted, old = \"value\", new = \"observed\")\n   397→\n   398→  forecasts_formated <- data.table::as.data.table(test_forecasts)\n   399→  data.table::setnames(forecasts_formated, old = \"value\", new = \"predicted\")\n   400→\n   401→  data_formatted <- merge(forecasts_formated, truth_formatted) |>\n   402→    as_forecast_quantile()\n   403→\n   404→  metrics <- get_metrics(example_quantile, select = \"wis\")\n   405→\n   406→  eval <- suppressMessages(score(data_formatted, metrics = metrics))\n   407→\n   408→  eval <- summarise_scores(eval,\n   409→    by = c(\n   410→      \"model\", \"location\", \"target_variable\",\n   411→      \"target_end_date\", \"forecast_date\", \"horizon\"\n   412→    )\n   413→  )\n   414→\n   415→  alpha1 <- 0.2\n   416→  expected <- (forecast_quantiles_matrix[, 2] - forecast_quantiles_matrix[, 1]) * (alpha1 / 2) +\n   417→    c(0, (-2) - (-15), 22 - 4)\n   418→\n   419→  actual_wis <- wis(\n   420→    observed = y,\n   421→    predicted = forecast_quantiles_matrix,\n   422→    quantile_level = c(0.1, 0.9)\n   423→  )\n   424→\n   425→  expect_equal(eval$wis, expected) # nolint: expect_identical_linter\n   426→  expect_equal(actual_wis, expected) # nolint: expect_identical_linter\n   427→})\n   428→\n   429→test_that(\"wis is correct, 2 intervals and median - test corresponds to covidHubUtils\", {\n   430→  target_end_dates <- as.Date(\"2020-01-01\") + c(7, 14, 7)\n   431→  horizons <- c(\"1\", \"2\", \"1\")\n   432→  locations <- c(\"01\", \"01\", \"02\")\n   433→  target_variables <- rep(\"inc death\", length(y))\n   434→\n   435→  forecast_target_end_dates <-\n   436→    rep(target_end_dates, times = ncol(forecast_quantiles_matrix))\n   437→  forecast_horizons <- rep(horizons, times = ncol(forecast_quantiles_matrix))\n   438→  forecast_locations <- rep(locations, times = ncol(forecast_quantiles_matrix))\n   439→  forecast_target_variables <-\n   440→    rep(target_variables, times = ncol(forecast_quantiles_matrix))\n   441→  forecast_quantile_probs <- rep(forecast_quantile_probs, each = length(y))\n   442→  forecast_quantiles <- forecast_quantiles_matrix\n   443→  dim(forecast_quantiles) <- prod(dim(forecast_quantiles))\n   444→\n   445→  test_truth <- data.frame(\n   446→    model = rep(\"truth_source\", length(y)),\n   447→    target_variable = target_variables,\n   448→    target_end_date = target_end_dates,\n   449→    location = locations,\n   450→    value = y,\n   451→    stringsAsFactors = FALSE\n   452→  )\n   453→\n   454→  n_forecasts <- length(forecast_quantiles)\n   455→  test_forecasts <- data.frame(\n   456→    model = rep(\"m1\", n_forecasts),\n   457→    forecast_date = rep(as.Date(\"2020-01-01\"), n_forecasts),\n   458→    location = forecast_locations,\n   459→    horizon = forecast_horizons,\n   460→    temporal_resolution = rep(\"wk\", n_forecasts),\n   461→    target_variable = forecast_target_variables,\n   462→    target_end_date = forecast_target_end_dates,\n   463→    type = rep(\"quantile\", n_forecasts),\n   464→    quantile_level = forecast_quantile_probs,\n   465→    value = forecast_quantiles,\n   466→    stringsAsFactors = FALSE\n   467→  )\n   468→\n   469→  # make a version that conforms to scoringutils format\n   470→  truth_formatted <- data.table::as.data.table(test_truth)\n   471→  truth_formatted[, `:=`(model = NULL)]\n   472→  data.table::setnames(truth_formatted, old = \"value\", new = \"observed\")\n   473→\n   474→  forecasts_formated <- data.table::as.data.table(test_forecasts)\n   475→  data.table::setnames(forecasts_formated, old = \"value\", new = \"predicted\")\n   476→\n   477→  data_formatted <- merge(forecasts_formated, truth_formatted) |>\n   478→    as_forecast_quantile()\n   479→\n   480→  eval <- score(data_formatted, metrics = metrics_no_cov)\n   481→\n   482→  eval <- summarise_scores(eval,\n   483→    by = c(\n   484→      \"model\", \"location\", \"target_variable\",\n   485→      \"target_end_date\", \"forecast_date\", \"horizon\"\n   486→    )\n   487→  )\n   488→\n   489→  alpha1 <- 0.2\n   490→  alpha2 <- 0.5\n   491→  expected <- (1 / 2.5) * (\n   492→    0.5 * abs(y - forecast_quantiles_matrix[, 3]) +\n   493→      (forecast_quantiles_matrix[, 5] - forecast_quantiles_matrix[, 1]) * (alpha1 / 2) + c(0, (-2) - (-15), 22 - 4) +\n   494→      (forecast_quantiles_matrix[, 4] - forecast_quantiles_matrix[, 2]) * (alpha2 / 2) + c(0, 1 - (-15), 22 - 3)\n   495→  )\n   496→\n   497→  actual_wis <- wis(\n   498→    observed = y,\n   499→    predicted = forecast_quantiles_matrix,\n   500→    quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9),\n   501→    count_median_twice = FALSE\n   502→  )\n   503→  expect_equal(eval$wis, expected) # nolint: expect_identical_linter\n   504→  expect_equal(actual_wis, expected) # nolint: expect_identical_linter\n   505→\n   506→  # check whether `count_median_twice = TRUE` also works\n   507→  expected2 <- (1 / 3) * (\n   508→    abs(y - forecast_quantiles_matrix[, 3]) +\n   509→      (forecast_quantiles_matrix[, 5] - forecast_quantiles_matrix[, 1]) * (alpha1 / 2) + c(0, (-2) - (-15), 22 - 4) +\n   510→      (forecast_quantiles_matrix[, 4] - forecast_quantiles_matrix[, 2]) * (alpha2 / 2) + c(0, 1 - (-15), 22 - 3)\n   511→  )\n   512→\n   513→  actual_wis2 <- wis(\n   514→    observed = y,\n   515→    predicted = forecast_quantiles_matrix,\n   516→    quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9),\n   517→    count_median_twice = TRUE\n   518→  )\n   519→\n   520→  metrics <- get_metrics(example_quantile, \"wis\")\n   521→  metrics$wis <- purrr::partial(wis, count_median_twice = TRUE)\n   522→  eval2 <- eval <- score(data_formatted, metrics = metrics)\n   523→  eval2 <- summarise_scores(eval2,\n   524→    by = c(\n   525→      \"model\", \"location\", \"target_variable\",\n   526→      \"target_end_date\", \"forecast_date\", \"horizon\"\n   527→    )\n   528→  )\n   529→  expect_equal(eval2$wis, expected2) # nolint: expect_identical_linter\n   530→  expect_equal(actual_wis2, expected2) # nolint: expect_identical_linter\n   531→})\n   532→\n   533→test_that(\"Quantlie score and interval score yield the same result, weigh = FALSE\", {\n   534→  # calling quantile_score and wis should return the same result if all\n   535→  # quantiles form central prediction intervals\n   536→  expect_equal( # nolint: expect_identical_linter\n   537→    quantile_score(observed, predicted, quantile_level),\n   538→    wis(observed, predicted, quantile_level)\n   539→  )\n   540→\n   541→  observed <- rnorm(10, mean = 1:10)\n   542→  alphas <- c(0.1, 0.5, 0.9)\n   543→\n   544→  for (alpha in alphas) {\n   545→    lower <- qnorm(alpha / 2, rnorm(10, mean = 1:10))\n   546→    upper <- qnorm((1 - alpha / 2), rnorm(10, mean = 11:20))\n   547→\n   548→    w <- FALSE\n   549→    is <- interval_score(\n   550→      observed = observed,\n   551→      lower = lower,\n   552→      upper = upper,\n   553→      interval_range = (1 - alpha) * 100,\n   554→      weigh = w\n   555→    )\n   556→\n   557→    wis <- wis(\n   558→      observed = observed,\n   559→      predicted = cbind(lower, upper),\n   560→      quantile_level = c(alpha / 2, 1 - alpha / 2),\n   561→      count_median_twice = FALSE,\n   562→      weigh = w\n   563→    )\n   564→\n   565→    qs_lower <- quantile_score(observed,\n   566→      predicted = matrix(lower),\n   567→      quantile_level = alpha / 2,\n   568→      weigh = w\n   569→    )\n   570→    qs_upper <- quantile_score(observed,\n   571→      predicted = matrix(upper),\n   572→      quantile_level = 1 - alpha / 2,\n   573→      weigh = w\n   574→    )\n   575→    expect_equal((qs_lower + qs_upper) / 2, is) # nolint: expect_identical_linter\n   576→    expect_equal(wis, is) # nolint: expect_identical_linter\n   577→  }\n   578→})\n   579→\n   580→\n   581→test_that(\"wis errors when there are no valid forecast intervals\", {\n   582→  # test that wis throws an error when there are no valid forecast intervals\n   583→  # (i.e. the quantiles do not form central prediction intervals)\n   584→  observed <- 0.1\n   585→  predicted <- c(\n   586→    0.1032978, 0.1078166, 0.1906849, 0.1969254, 0.2017249,\n   587→    0.2078487, 0.2810377, 0.3062168, 0.4006721, 0.8219655\n   588→  )\n   589→  quantile_level <- c(\n   590→    0.600, 0.650, 0.700, 0.750, 0.800,\n   591→    0.850, 0.900, 0.950, 0.975, 0.990\n   592→  )\n   593→\n   594→  expect_error(\n   595→    wis(observed, predicted, quantile_level),\n   596→    \"No valid forecast intervals found.\"\n   597→  )\n   598→})\n   599→\n   600→\n   601→# ============================================================================ #\n   602→# Quantile score ============================================================= #\n   603→# ============================================================================ #\n   604→test_that(\"Quantlie score and interval score yield the same result, weigh = TRUE\", {\n   605→  observed <- rnorm(10, mean = 1:10)\n   606→  alphas <- c(0.1, 0.5, 0.9)\n   607→\n   608→  for (alpha in alphas) {\n   609→    lower <- qnorm(alpha / 2, rnorm(10, mean = 1:10))\n   610→    upper <- qnorm((1 - alpha / 2), rnorm(10, mean = 11:20))\n   611→\n   612→    w <- TRUE\n   613→    is <- interval_score(\n   614→      observed = observed,\n   615→      lower = lower,\n   616→      upper = upper,\n   617→      interval_range = (1 - alpha) * 100,\n   618→      weigh = w\n   619→    )\n   620→\n   621→    wis <- wis(\n   622→      observed = observed,\n   623→      predicted = cbind(lower, upper),\n   624→      quantile_level = c(alpha / 2, 1 - alpha / 2),\n   625→      count_median_twice = FALSE,\n   626→      weigh = w\n   627→    )\n   628→\n   629→    qs_lower <- quantile_score(observed,\n   630→      predicted = matrix(lower),\n   631→      quantile_level = alpha / 2,\n   632→      weigh = w\n   633→    )\n   634→    qs_upper <- quantile_score(observed,\n   635→      predicted = matrix(upper),\n   636→      quantile_level = 1 - alpha / 2,\n   637→      weigh = w\n   638→    )\n   639→    expect_equal((qs_lower + qs_upper) / 2, is) # nolint: expect_identical_linter\n   640→    expect_equal(wis, is) # nolint: expect_identical_linter\n   641→  }\n   642→})\n   643→\n   644→test_that(\"wis works with separate results\", {\n   645→  wis <- wis(\n   646→    observed = y,\n   647→    predicted = forecast_quantiles_matrix,\n   648→    quantile_level = forecast_quantile_probs,\n   649→    separate_results = TRUE\n   650→  )\n   651→  expect_equal(wis$wis, wis$dispersion + wis$overprediction + wis$underprediction) # nolint: expect_identical_linter\n   652→})\n   653→\n   654→\n   655→# ============================================================================ #\n   656→# overprediction, underprediction, dispersion ================================ #\n   657→# ============================================================================ #\n   658→test_that(\"wis is the sum of overprediction, underprediction, dispersion\", {\n   659→  wis <- wis(\n   660→    observed = y,\n   661→    predicted = forecast_quantiles_matrix,\n   662→    quantile_level = forecast_quantile_probs\n   663→  )\n   664→\n   665→  d <- dispersion_quantile(y, forecast_quantiles_matrix, forecast_quantile_probs)\n   666→  o <- overprediction_quantile(y, forecast_quantiles_matrix, forecast_quantile_probs)\n   667→  u <- underprediction_quantile(y, forecast_quantiles_matrix, forecast_quantile_probs)\n   668→\n   669→  expect_equal(wis, d + o + u) # nolint: expect_identical_linter\n   670→})\n   671→\n   672→\n   673→# ============================================================================ #\n   674→# `interval_coverage` =============================================== #\n   675→# ============================================================================ #\n   676→test_that(\"interval_coverage works\", {\n   677→  expect_equal( # nolint: expect_identical_linter\n   678→    interval_coverage(observed, predicted, quantile_level, interval_range = 50),\n   679→    c(TRUE, FALSE, FALSE)\n   680→  )\n   681→})\n   682→\n   683→test_that(\"interval_coverage rejects wrong inputs\", {\n   684→  expect_error(\n   685→    interval_coverage(observed, predicted, quantile_level, interval_range = c(50, 0)),\n   686→    \"Assertion on 'interval_range' failed: Must have length 1.\"\n   687→  )\n   688→})\n   689→\n   690→test_that(\"interval_coverage_quantile throws a warning when a required quantile is not available\", {\n   691→  dropped_quantile_pred <- predicted[, -4]\n   692→  dropped_quantiles <- quantile_level[-4]\n   693→  expect_error(\n   694→    interval_coverage(\n   695→      observed, dropped_quantile_pred, dropped_quantiles,\n   696→      interval_range = 50\n   697→    ),\n   698→    paste(\n   699→      \"To compute the interval coverage for an interval range of \\\"50%\\\",\",\n   700→      \"the 0.25 and 0.75 quantiles are required\"\n   701→    )\n   702→  )\n   703→})\n   704→\n   705→\n   706→# ============================================================================ #\n   707→# `bias_quantile` ============================================================ #\n   708→# ============================================================================ #\n   709→test_that(\"bias_quantile() works as expected\", {\n   710→  predicted <- c(1, 2, 3)\n   711→  quantiles <- c(0.1, 0.5, 0.9)\n   712→  expect_equal( # nolint: expect_identical_linter\n   713→    bias_quantile(observed = 2, predicted, quantiles),\n   714→    0\n   715→  )\n   716→  predicted <- c(0, 1, 2)\n   717→  quantiles <- c(0.1, 0.5, 0.9)\n   718→  expect_equal( # nolint: expect_identical_linter\n   719→    bias_quantile(observed = 2, predicted, quantiles),\n   720→    -0.8\n   721→  )\n   722→\n   723→  predicted <- c(\n   724→    705.500, 1127.000, 4006.250, 4341.500, 4709.000, 4821.996,\n   725→    5340.500, 5451.000, 5703.500, 6087.014, 6329.500, 6341.000,\n   726→    6352.500, 6594.986, 6978.500, 7231.000, 7341.500, 7860.004,\n   727→    7973.000, 8340.500, 8675.750, 11555.000, 11976.500\n   728→  )\n   729→\n   730→  quantile_level <- c(0.01, 0.025, seq(0.05, 0.95, 0.05), 0.975, 0.99)\n   731→\n   732→  observed <- 8062\n   733→  expect_equal(bias_quantile(observed, predicted, quantile_level), -0.8)\n   734→})\n   735→\n   736→test_that(\"bias_quantile handles matrix input\", {\n   737→  observed <- seq(10, 0, length.out = 4)\n   738→  predicted <- matrix(1:12, ncol = 3)\n   739→  quantiles <- c(0.1, 0.5, 0.9)\n   740→  expect_equal(\n   741→    bias_quantile(observed, predicted, quantiles),\n   742→    c(-1.0, -0.8, 0.8, 1.0)\n   743→  )\n   744→})\n   745→\n   746→\n   747→test_that(\"bias_quantile() handles vector that is too long\", {\n   748→  predicted <- c(NA, 1, 2, 3)\n   749→  quantiles <- c(0.1, 0.5, 0.9)\n   750→\n   751→  expect_error(\n   752→    bias_quantile(observed = 2, predicted, quantiles),\n   753→    \"Assertion on 'quantile_level' failed: Must have length 4, but has length 3.\"\n   754→  )\n   755→})\n   756→\n   757→test_that(\"bias_quantile() handles NA values\", {\n   758→  predicted <- c(NA, 1, 2)\n   759→  quantiles <- c(0.1, 0.5, 0.9)\n   760→  expect_equal( # nolint: expect_identical_linter\n   761→    bias_quantile(observed = 2, predicted, quantiles),\n   762→    -0.8\n   763→  )\n   764→  predicted <- c(0, 1, 2)\n   765→  quantiles <- c(0.1, 0.5, NA)\n   766→  expect_equal( # nolint: expect_identical_linter\n   767→    bias_quantile(observed = 2, predicted, quantiles),\n   768→    -1\n   769→  )\n   770→  expect_equal( # nolint: expect_identical_linter\n   771→    bias_quantile(observed = 2, predicted, quantiles, na.rm = FALSE),\n   772→    NA_real_\n   773→  )\n   774→})\n   775→\n   776→test_that(\"bias_quantile() errors if no predictions\", {\n   777→  expect_error(\n   778→    bias_quantile(observed = 2, numeric(0), numeric(0)),\n   779→    \"Assertion on 'quantile_level' failed: Must have length >= 1, but has length 0\"\n   780→  )\n   781→})\n   782→\n   783→test_that(\"bias_quantile() returns correct bias if value below the median\", {\n   784→  predicted <- c(1, 2, 4, 5)\n   785→  quantiles <- c(0.1, 0.3, 0.7, 0.9)\n   786→  suppressMessages(\n   787→    expect_equal(bias_quantile(observed = 1, predicted, quantiles), 0.8)\n   788→  )\n   789→})\n   790→\n   791→test_that(\"bias_quantile() returns correct bias if value above median\", {\n   792→  predicted <- c(1, 2, 4, 5)\n   793→  quantiles <- c(0.1, 0.3, 0.7, 0.9)\n   794→  suppressMessages(\n   795→    expect_equal(bias_quantile(observed = 5, predicted, quantiles), -0.8)\n   796→  )\n   797→})\n   798→\n   799→test_that(\"bias_quantile() returns correct bias if value at the median\", {\n   800→  predicted <- c(1, 2, 3, 4)\n   801→  quantiles <- c(0.1, 0.3, 0.5, 0.7)\n   802→\n   803→  expect_equal(bias_quantile(observed = 3, predicted, quantiles), 0) # nolint: expect_identical_linter\n   804→})\n   805→\n   806→test_that(\"bias_quantile() returns 1 if true value below min prediction\", {\n   807→  predicted <- c(2, 3, 4, 5)\n   808→  quantiles <- c(0.1, 0.3, 0.7, 0.9)\n   809→\n   810→  suppressMessages(\n   811→    expect_equal(bias_quantile(observed = 1, predicted, quantiles), 1) # nolint: expect_identical_linter\n   812→  )\n   813→})\n   814→\n   815→test_that(\"bias_quantile() returns -1 if true value above max prediction\", {\n   816→  predicted <- c(1, 2, 3, 4)\n   817→  quantiles <- c(0.1, 0.3, 0.5, 0.7)\n   818→\n   819→  expect_equal(bias_quantile(observed = 6, predicted, quantiles), -1) # nolint: expect_identical_linter\n   820→})\n   821→\n   822→test_that(\"bias_quantile(): quantiles must be between 0 and 1\", {\n   823→  predicted <- 1:4\n   824→\n   825→  # Failing example\n   826→  quantiles <- c(-0.1, 0.3, 0.5, 0.8)\n   827→  expect_error(\n   828→    bias_quantile(observed = 3, predicted, quantiles),\n   829→    \"Assertion on 'quantile_level' failed: Element 1 is not >= 0.\"\n   830→  )\n   831→\n   832→  # Passing counter example\n   833→  quantiles <- c(0.1, 0.3, 0.5, 0.8)\n   834→  expect_silent(bias_quantile(observed = 3, predicted, quantiles))\n   835→})\n   836→\n   837→test_that(\"bias_quantile(): quantiles must be increasing\", {\n   838→  predicted <- 1:4\n   839→\n   840→  # Failing example\n   841→  quantiles <- c(0.8, 0.3, 0.5, 0.9)\n   842→  expect_error(\n   843→    bias_quantile(observed = 3, predicted, quantiles),\n   844→    \"Predictions must not be decreasing with increasing quantile level\"\n   845→  )\n   846→\n   847→  # Passing counter example\n   848→  quantiles <- c(0.3, 0.5, 0.8, 0.9)\n   849→  expect_silent(bias_quantile(observed = 3, predicted, quantiles))\n   850→})\n   851→\n   852→test_that(\"bias_quantile(): predictions must be increasing\", {\n   853→  predicted <- c(1, 2, 4, 3)\n   854→  quantiles <- c(0.1, 0.3, 0.5, 0.9)\n   855→\n   856→  expect_error(\n   857→    bias_quantile(observed = 3, predicted, quantiles),\n   858→    \"Predictions must not be decreasing with increasing quantile level\"\n   859→  )\n   860→  expect_silent(bias_quantile(observed = 3, 1:4, quantiles))\n   861→})\n   862→\n   863→test_that(\"bias_quantile(): quantile levels must be unique\", {\n   864→  predicted <- 1:4\n   865→\n   866→  # Failing example\n   867→  quantiles <- c(0.3, 0.3, 0.5, 0.8)\n   868→  expect_error(\n   869→    bias_quantile(observed = 3, predicted, quantiles),\n   870→    \"Assertion on 'quantile_level' failed: Contains duplicated values, position 2.\"\n   871→  )\n   872→\n   873→  # Passing example\n   874→  quantiles <- c(0.3, 0.5, 0.8, 0.9)\n   875→  expect_silent(bias_quantile(observed = 3, predicted, quantiles))\n   876→})\n   877→\n   878→test_that(\"bias_quantile only produces one message\", {\n   879→  expect_message(\n   880→    bias_quantile(observed, predicted[, -3], quantile_level[-3]),\n   881→    \"Median not available, interpolating median from the two innermost quantiles in order to compute bias.\"\n   882→  )\n   883→})\n   884→\n   885→test_that(\"bias_quantile() works with point forecasts\", {\n   886→  predicted <- 1\n   887→  observed <- 1\n   888→  quantile_level <- 0.5\n   889→  expect_equal(bias_quantile(observed, predicted, quantile_level), 0) # nolint: expect_identical_linter\n   890→})\n   891→\n   892→\n   893→test_that(\"bias_quantile() handles cases where median is not available\", {\n   894→  predicted <- c(1, 10)\n   895→  observed <- 15\n   896→  quantile_level <- c(0.2, 0.4)\n   897→\n   898→  expect_error(\n   899→    bias_quantile(observed, predicted, quantile_level),\n   900→    \"Assertion on 'quantile_level\\\\[quantile_leve\\\\l >= 0.5]' failed: Must have length >= 1, but has length 0.\"\n   901→  )\n   902→})\n   903→\n   904→# `interpolate_median` ======================================================= #\n   905→test_that(\"interpolation in `interpolate_median` works\", {\n   906→  predicted <- c(1, 10)\n   907→  observed <- 15\n   908→  quantile_level <- c(0.4, 0.6)\n   909→\n   910→  # median is missing, symmetric quantile levels given\n   911→  expect_equal(interpolate_median(predicted, quantile_level), mean(predicted)) # nolint: expect_identical_linter\n   912→  quantile_level <- c(0.1, 0.9)\n   913→  expect_equal(interpolate_median(predicted, quantile_level), mean(predicted)) # nolint: expect_identical_linter\n   914→\n   915→  1 + 0.4 / 0.8 * (10 - 1)\n   916→\n   917→  # asymmetric quantile levels given\n   918→  quantile_level <- c(0.3, 0.6)\n   919→  expect_equal(interpolate_median(predicted, quantile_level), 1 + 0.2 / 0.3 * (10 - 1)) # nolint: expect_identical_linter\n   920→\n   921→  quantile_level <- c(0.2, 0.6)\n   922→  expect_equal(interpolate_median(predicted, quantile_level), 1 + 0.3 / 0.4 * (10 - 1)) # nolint: expect_identical_linter\n   923→})\n   924→\n   925→\n   926→\n   927→# ============================================================================ #\n   928→# `ae_median_quantile` ======================================================= #\n   929→# ============================================================================ #\n   930→\n   931→test_that(\"ae_median_quantile() works as_expected\", {\n   932→  observed <- 1:30\n   933→  predicted_values <- matrix(2:31)\n   934→  expect_equal( # nolint: expect_identical_linter\n   935→    ae_median_quantile(observed, predicted_values, quantile_level = 0.5),\n   936→    as.numeric(predicted_values - observed)\n   937→  )\n   938→\n   939→  # using a vector as input for predicted does not work if there are several\n   940→  # observed values\n   941→  expect_error(\n   942→    ae_median_quantile(observed, as.numeric(predicted_values), quantile_level = 0.5),\n   943→    \"Assertion on 'predicted' failed: Must be of type 'matrix', not 'double'.\"\n   944→  )\n   945→\n   946→  # it does work if there is only a single observed value - in this case\n   947→  # the vector gets treated as one forecast - which means that we need to\n   948→  # supply multiple quantile levels now, because it assumes that single forecast\n   949→  # is represented by several quantiles\n   950→  expect_equal( # nolint: expect_identical_linter\n   951→    ae_median_quantile(observed[1], as.numeric(predicted_values)[1:3], quantile_level = c(0.1, 0.5, 0.7)),\n   952→    2\n   953→  )\n   954→\n   955→  # test that we get a warning if there are inputs without a 0.5 quantile\n   956→  expect_error(\n   957→    ae_median_quantile(observed, predicted_values, quantile_level = 0.6),\n   958→    \"In order to compute the absolute error of the median, \"\n   959→  )\n   960→})\n   961→\n   962→\n   963→\n   964→# ============================================================================ #\n   965→# quantile_score() =============================================================\n   966→# ============================================================================ #\n   967→test_that(\"quantile_score() works regardless of whether input is vector or matrix\", {\n   968→  observed <- rnorm(1, mean = 2)\n   969→  alpha <- seq(0.2, 0.8, 0.2)\n   970→  predicted <- rnorm(4, mean = 1:4)\n   971→\n   972→\n   973→  expect_equal( # nolint: expect_identical_linter\n   974→    quantile_score(observed, predicted, quantile_level = alpha),\n   975→    quantile_score(observed, t(matrix(predicted)), quantile_level = alpha)\n   976→  )\n   977→})\n   978→"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"b33e1ee8-d895-4e7e-a448-e953521a8288"}
{"type":"assistant","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01XF7wixoLv2rpzVrNFTgJr6","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016wacG3zBHXNtWjdyrL2VPZ","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-helper-quantile-interval-range.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":398,"cache_read_input_tokens":32517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":398},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"de0a680c-99f2-4542-90f1-e4f86748885a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016wacG3zBHXNtWjdyrL2VPZ","type":"tool_result","content":"     1→# ==============================================================================\n     2→# quantile_to_interval() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→\n     5→test_that(\"quantile_to_interval_dataframe() works\", {\n     6→  quantile <- data.frame(\n     7→    date = as.Date(\"2020-01-01\") + 1:10,\n     8→    model = \"model1\",\n     9→    observed = 1:10,\n    10→    predicted = c(2:11, 4:13),\n    11→    quantile_level = rep(c(0.25, 0.75), each = 10),\n    12→    stringsAsFactors = FALSE\n    13→  )\n    14→  long <- data.frame(\n    15→    date = as.Date(\"2020-01-01\") + 1:10,\n    16→    model = \"model1\",\n    17→    observed = 1:10,\n    18→    predicted = c(2:11, 4:13),\n    19→    interval_range = 50,\n    20→    boundary = rep(c(\"lower\", \"upper\"), each = 10),\n    21→    stringsAsFactors = FALSE\n    22→  )\n    23→  long2 <- as.data.frame(quantile_to_interval(\n    24→    quantile,\n    25→    keep_quantile_col = FALSE\n    26→  ))\n    27→  data.table::setcolorder(long2, names(long))\n    28→  # for some reason this is needed to pass the unit tests on gh actions\n    29→  long2$boundary <- as.character(long2$boundary)\n    30→  long$boundary <- as.character(long$boundary)\n    31→  expect_identical(long, as.data.frame(long2))\n    32→\n    33→  # check that it handles NA values\n    34→  setDT(quantile)\n    35→  quantile[c(1, 3, 11, 13), c(\"observed\", \"predicted\", \"quantile_level\") := NA]\n    36→  # in this instance, a problem appears because there is an NA value both\n    37→  # for the upper and lower bound.\n    38→\n    39→  # the data.table behavior differs before/after v1.16.0\n    40→  #  - before, it's a 'message'\n    41→  #  - after, it's a 'warning'\n    42→  #  - the conditionMessage() also differs\n    43→  expected_condition <- tryCatch(\n    44→    dcast(data.table(a = c(1, 1), b = 2, c = 3), a ~ b, value.var = \"c\"),\n    45→    condition = identity\n    46→  )\n    47→  expect_condition(\n    48→    quantile_to_interval(\n    49→      quantile,\n    50→      keep_quantile_col = FALSE,\n    51→      format = \"wide\"\n    52→    ),\n    53→    class = class(expected_condition)[1L], # testthat 3e requires exactly one class\n    54→    regexp = \"[Aa]ggregate.*default.*length\"\n    55→  )\n    56→  quantile <- quantile[-c(1, 3), ]\n    57→  wide2 <- scoringutils:::quantile_to_interval( # nolint: undesirable_operator_linter\n    58→    quantile,\n    59→    keep_quantile_col = FALSE,\n    60→    format = \"wide\"\n    61→  )\n    62→  expect_identical(nrow(wide2), 10L)\n    63→  expect_false((\"NA\") %in% colnames(wide2))\n    64→  expect_equal(sum(wide2$lower, na.rm = TRUE), 59) # nolint: expect_identical_linter\n    65→})\n    66→\n    67→\n    68→test_that(\"sample_to_range_long works\", {\n    69→  samples <- data.frame(\n    70→    date = as.Date(\"2020-01-01\") + 1:10,\n    71→    model = \"model1\",\n    72→    observed = 1:10,\n    73→    predicted = c(rep(0, 10), 2:11, 3:12, 4:13, rep(100, 10)),\n    74→    sample_id = rep(1:5, each = 10),\n    75→    stringsAsFactors = FALSE\n    76→  )\n    77→\n    78→  long <- data.frame(\n    79→    date = as.Date(\"2020-01-01\") + 1:10,\n    80→    model = \"model1\",\n    81→    observed = 1:10,\n    82→    predicted = c(2:11, 4:13),\n    83→    interval_range = 50,\n    84→    boundary = rep(c(\"lower\", \"upper\"), each = 10),\n    85→    stringsAsFactors = FALSE\n    86→  )\n    87→\n    88→  long2 <- scoringutils:::sample_to_interval_long(as_forecast_sample(samples), # nolint: undesirable_operator_linter\n    89→    interval_range = 50,\n    90→    keep_quantile_col = FALSE\n    91→  )\n    92→  long2 <- long2[order(model, boundary, date)]\n    93→  data.table::setcolorder(long2, names(long))\n    94→\n    95→  # for some reason this is needed to pass the unit tests on gh actions\n    96→  long2$boundary <- as.character(long2$boundary)\n    97→  long$boundary <- as.character(long$boundary)\n    98→\n    99→  expect_equal(long, as.data.frame(long2)) # nolint: expect_identical_linter\n   100→})\n   101→\n   102→test_that(\"quantile_to_range works - scalar and vector case\", {\n   103→  predicted <- 9:1\n   104→  quantile <- rev(seq(0.1, 0.9, 0.1))\n   105→  observed <- 5\n   106→\n   107→  # check output is produced\n   108→  out1 <- quantile_to_interval(observed, predicted, quantile)\n   109→  expect_snapshot(out1)\n   110→\n   111→  # check order of predictions doesn't matter\n   112→  predicted <- 1:9\n   113→  quantile <- seq(0.1, 0.9, 0.1)\n   114→  out2 <- quantile_to_interval(observed, predicted, quantile)\n   115→  expect_identical(out1, out2)\n   116→\n   117→  # check error if observed is a vector and predicted is a vector as well\n   118→  expect_error(\n   119→    quantile_to_interval(\n   120→      observed = c(1, 2), predicted = c(1, 2), quantile_level = c(0.1, 0.9)\n   121→    ),\n   122→    \"Assertion on 'predicted' failed: Must be of type 'matrix', not 'double'.\"\n   123→  )\n   124→\n   125→  # check NA values are handled gracefully - there should just be NA in the output\n   126→  predicted <- c(1:8, NA)\n   127→  quantile <- seq(0.1, 0.9, 0.1)\n   128→  observed <- 5\n   129→  out3 <- quantile_to_interval(observed, predicted, quantile)\n   130→  expect_snapshot(out3)\n   131→\n   132→  # check non-symmetrical intervals are handled gracefully\n   133→  # result should be newly introduced ranges where one value is NA\n   134→  predicted <- 1:9\n   135→  quantile <- c(seq(0.1, 0.8, 0.1), 0.95)\n   136→  observed <- 5\n   137→  out4 <- quantile_to_interval(observed, predicted, quantile)\n   138→  expect_snapshot(out4)\n   139→\n   140→  # check function works without a median\n   141→  predicted <- 1:8\n   142→  quantile <- c(seq(0.1, 0.4, 0.1), seq(0.6, 0.9, 0.1))\n   143→  observed <- 5\n   144→  expect_no_condition(\n   145→    quantile_to_interval(observed, predicted, quantile)\n   146→  )\n   147→\n   148→  # check a one-dimensional matrix works fine\n   149→  predicted <- matrix(1:9, nrow = 1)\n   150→  quantile <- seq(0.1, 0.9, 0.1)\n   151→  expect_no_condition(\n   152→    quantile_to_interval(observed, predicted, quantile)\n   153→  )\n   154→})\n   155→\n   156→test_that(\"quantile_to_range works - matrix case\", {\n   157→  n <- 5\n   158→  N <- 9\n   159→  predicted <- matrix(1:45, nrow = n, ncol = N)\n   160→  quantile <- seq(0.1, 0.9, 0.1)\n   161→  observed <- seq(21, 25, 1)\n   162→\n   163→  # check output is produced\n   164→  out1 <- quantile_to_interval(observed, predicted, quantile)\n   165→  expect_snapshot(out1)\n   166→\n   167→  # check order of predictions doesn't matter\n   168→  predicted <- matrix(\n   169→    c(41:45, 36:40, 31:35, 26:30, 21:25, 16:20, 11:15, 6:10, 1:5),\n   170→    nrow = n,\n   171→    ncol = N\n   172→  )\n   173→  quantile <- rev(seq(0.1, 0.9, 0.1))\n   174→  out2 <- quantile_to_interval(observed, predicted, quantile)\n   175→  expect_identical(out1, out2)\n   176→\n   177→  # check NA values are fine\n   178→  predicted[1, 1] <- NA\n   179→  expect_no_condition(\n   180→    quantile_to_interval(observed, predicted, quantile)\n   181→  )\n   182→})\n   183→\n   184→\n   185→test_that(\"quantile_to_interval works - data.frame case\", {\n   186→  dt <- data.table(\n   187→    observed = 5,\n   188→    predicted = 1:9,\n   189→    quantile_level = seq(0.1, 0.9, 0.1)\n   190→  )\n   191→\n   192→  expect_no_condition(\n   193→    quantile_to_interval(dt)\n   194→  )\n   195→\n   196→  expect_no_condition(\n   197→    quantile_to_interval(dt, format = \"wide\")\n   198→  )\n   199→\n   200→  # check that the number of rows after transformation is equal to the number\n   201→  # of rows plus the number of medians added (as upper boundary of a 0%\n   202→  # prediction interval)\n   203→  ex <- example_quantile[!is.na(predicted)]\n   204→  n_preds <- nrow(ex)\n   205→  n_medians <- nrow(ex[quantile_level == 0.5])\n   206→  ex_interval <- quantile_to_interval(ex, keep_quantile_col = TRUE)\n   207→  expect_identical(\n   208→    nrow(ex_interval),\n   209→    n_preds + n_medians\n   210→  )\n   211→\n   212→  expect_identical(\n   213→    colnames(ex_interval),\n   214→    c(colnames(ex), \"boundary\", \"interval_range\")\n   215→  )\n   216→\n   217→  expect_error(\n   218→    quantile_to_interval(x = \"not working\"),\n   219→    \"Input must be either a data.frame or a numeric vector.\"\n   220→  )\n   221→})\n   222→"}]},"parent_tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"3606a006-f4dc-402a-b222-618d403800ed"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012bpKwRDNUYYAAbfRRuuC5g","type":"tool_result","content":[{"type":"text","text":"Perfect! Now I have a comprehensive understanding of the code. Let me create a detailed report for you.\n\n## Summary Report: Issue #454 - Deduplicating Code Between `get_coverage()` and `interval_coverage()`\n\n### Overview\n\nIssue #454 is about identifying and deduplicating code shared between two functions:\n1. **`get_coverage()`** - `/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R` (lines 58-100)\n2. **`interval_coverage()`** - `/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R` (lines 341-365)\n\nBoth functions use similar patterns with helper functions to transform quantile-based forecasts to interval format and validate bounds.\n\n---\n\n## 1. `get_coverage()` Function\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R` (lines 58-100)\n\n```r\nget_coverage <- function(forecast, by = \"model\") {\n  # input checks\n  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n  assert_subset(get_forecast_type(forecast), \"quantile\")\n  \n  by <- setdiff(by, c(\"quantile_level\", \"interval_range\"))\n  assert_subset(by, names(forecast))\n  \n  # convert to wide interval format and compute interval coverage\n  interval_forecast <- quantile_to_interval(forecast, format = \"wide\")  # <-- SHARED CALL\n  interval_forecast[,\n    interval_coverage := (observed <= upper) & (observed >= lower)  # <-- BOUNDS CHECK\n  ][, c(\"lower\", \"upper\", \"observed\") := NULL]\n  interval_forecast[, interval_coverage_deviation :=\n                      interval_coverage - interval_range / 100]\n  \n  # merge interval range data with original data\n  forecast[, interval_range := get_range_from_quantile(quantile_level)]  # <-- SHARED CALL\n  forecast_cols <- colnames(forecast)\n  forecast_unit <- get_forecast_unit(forecast)\n  \n  forecast <- merge(forecast, interval_forecast,\n                    by = unique(c(forecast_unit, \"interval_range\")))\n  \n  # compute quantile coverage and deviation\n  forecast[, quantile_coverage := observed <= predicted]  # <-- BOUNDS CHECK\n  forecast[, quantile_coverage_deviation := quantile_coverage - quantile_level]\n  \n  # summarise and cleanup\n  new_metrics <- c(\"interval_coverage\", \"interval_coverage_deviation\",\n                   \"quantile_coverage\", \"quantile_coverage_deviation\")\n  setcolorder(forecast, unique(c(forecast_cols, \"interval_range\", new_metrics)))\n  forecast <- as.data.table(forecast)\n  by <- unique(c(by, \"quantile_level\", \"interval_range\"))\n  forecast <- forecast[, lapply(.SD, mean), by = by, .SDcols = new_metrics]\n  return(forecast[])\n}\n```\n\n**Key Operations:**\n- Takes a data.table forecast object in quantile format\n- Calls `quantile_to_interval()` to convert to wide interval format\n- Performs bounds check: `(observed <= upper) & (observed >= lower)`\n- Computes coverage deviation as difference from expected range\n- Summarizes by grouping variables with mean aggregation\n\n---\n\n## 2. `interval_coverage()` Function\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R` (lines 341-365)\n\n```r\ninterval_coverage <- function(observed, predicted,\n                              quantile_level, interval_range = 50) {\n  assert_input_quantile(observed, predicted, quantile_level)\n  assert_number(interval_range)\n  necessary_quantiles <- c(\n    (100 - interval_range) / 2,\n    100 - (100 - interval_range) / 2\n  ) / 100\n  if (!all(necessary_quantiles %in% quantile_level)) {\n    cli_abort(\n      c(\n        \"!\" = \"To compute the interval coverage for an interval range of\n        {.val {interval_range}%}, the {.val {necessary_quantiles}} quantiles\n        are required\"\n      )\n    )\n  }\n  r <- interval_range\n  reformatted <- quantile_to_interval(observed, predicted, quantile_level)  # <-- SHARED CALL\n  reformatted <- reformatted[interval_range %in% r]\n  reformatted[, interval_coverage := (observed >= lower) & (observed <= upper)]  # <-- BOUNDS CHECK\n  return(reformatted$interval_coverage)\n}\n```\n\n**Key Operations:**\n- Takes numeric vectors: `observed`, `predicted`, `quantile_level`\n- Validates that required quantiles exist for the requested interval range\n- Calls `quantile_to_interval()` to convert vectors to interval format\n- Performs bounds check: `(observed >= lower) & (observed <= upper)` (same logic, different order)\n- Returns boolean vector of coverage values\n\n---\n\n## 3. The `quantile_to_interval()` Helper Function\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R` (lines 31-104)\n\nThis is a polymorphic function that delegates based on input type:\n\n```r\nquantile_to_interval <- function(...) {\n  dots <- list(...)\n  if (is.data.frame(dots[[1]])) {\n    do.call(quantile_to_interval_dataframe, dots)\n  } else if (is.numeric(dots[[1]])) {\n    do.call(quantile_to_interval_numeric, dots)\n  } else {\n    cli_abort(\"Input must be either a data.frame or a numeric vector.\")\n  }\n}\n```\n\n**Dataframe version** (lines 62-104):\n- Takes a forecast data.table with `quantile_level` and `predicted` columns\n- Adds `boundary` column (lower/upper based on quantile_level <= 0.5)\n- Calls `get_range_from_quantile()` to compute interval_range\n- Handles median quantile (0.5) specially by duplicating it\n- Can return long or wide format\n\n**Numeric version** (lines 116-134):\n- Takes vectors: `observed`, `predicted`, `quantile_level`\n- Creates a data.table with `forecast_id`, `observed`, `predicted`, `quantile_level`\n- Delegates to dataframe version with `format = \"wide\"`\n- Returns ordered results by `forecast_id` and `interval_range`\n\n---\n\n## 4. The `get_range_from_quantile()` Helper\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R` (lines 185-193)\n\n```r\nget_range_from_quantile <- function(quantile_level) {\n  boundary <- ifelse(quantile_level <= 0.5, \"lower\", \"upper\")\n  interval_range <- ifelse(\n    boundary == \"lower\",\n    round((1 - 2 * quantile_level) * 100, digits = 10),\n    round((2 * quantile_level - 1) * 100, digits = 10)\n  )\n  return(interval_range)\n}\n```\n\nConverts a quantile level to the interval range it represents (e.g., 0.25 → 50, 0.75 → 50).\n\n---\n\n## 5. Test Files and Test Data\n\n### Test File: `test-get-coverage.R`\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R`\n\n**Tests for `get_coverage()`:**\n\n1. **Basic functionality test** (lines 6-25):\n   - Verifies correct column names and structure\n   - Uses `example_quantile` dataset\n   - Checks for expected columns: `interval_coverage`, `interval_coverage_deviation`, `quantile_coverage`, `quantile_coverage_deviation`\n\n2. **Class verification test** (lines 27-30):\n   - Ensures output is a data.table\n\n3. **Non-symmetric intervals test** (lines 32-54):\n   - Tests graceful handling of missing quantiles\n   - Missing interval_range values should be `NA`\n   - Tests both explicit removal and NA values in predicted column\n\n**Tests for `plot_interval_coverage()`** (lines 60-76):\n- Generates coverage data and plots it\n- Uses visual regression testing with `vdiffr`\n\n**Tests for `plot_quantile_coverage()`** (lines 82-92):\n- Similar visual regression tests\n\n### Test File: `test-metrics-quantile.R`\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R`\n\n**Tests for `interval_coverage()`** (lines 676-703):\n\n1. **Basic functionality test** (lines 676-681):\n   ```r\n   test_that(\"interval_coverage works\", {\n     expect_equal(\n       interval_coverage(observed, predicted, quantile_level, interval_range = 50),\n       c(TRUE, FALSE, FALSE)\n     )\n   })\n   ```\n\n2. **Input validation test** (lines 683-688):\n   - Tests that `interval_range` must be a single number\n\n3. **Missing quantile test** (lines 690-703):\n   - Verifies error when required quantiles are missing\n   - Tests for specific error message\n\n### Test Data (Setup File)\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R`\n\nPre-computed test variables:\n```r\nmetrics_no_cov <- get_metrics(\n  example_quantile,\n  exclude = c(\"interval_coverage_50\", \"interval_coverage_90\")\n)\n\nexample_quantile_df <- as.data.frame(na.omit(example_quantile))\n\nscores_quantile <- score(example_quantile)\nscores_sample_continuous <- score(example_sample_continuous)\nscores_sample_discrete <- score(example_sample_discrete)\nscores_point <- score(example_point)\nscores_binary <- score(example_binary)\nscores_nominal <- score(example_nominal)\n```\n\n### Test Datasets Used in `test-metrics-quantile.R`:\n\nGlobal test data (lines 1-16):\n```r\nobserved <- c(1, -15, 22)\npredicted <- rbind(\n  c(-1, 0, 1, 2, 3),\n  c(-2, 1, 2, 2, 4),\n  c(-2, 0, 3, 3, 4)\n)\nquantile_level <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n\n# Also from covidHubUtils tests:\ny <- c(1, -15, 22)\nforecast_quantiles_matrix <- rbind(\n  c(-1, 0, 1, 2, 3),\n  c(-2, 1, 2, 2, 4),\n  c(-2, 0, 3, 3, 4)\n)\nforecast_quantile_probs <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n```\n\n---\n\n## 6. Identified Duplicated Code Patterns\n\n### Pattern 1: Bounds Checking\nBoth functions perform nearly identical bounds checks:\n\n**`get_coverage()`:**\n```r\ninterval_coverage := (observed <= upper) & (observed >= lower)\n```\n\n**`interval_coverage()`:**\n```r\ninterval_coverage := (observed >= lower) & (observed <= upper)\n```\n\nThese are identical in logic (just different order of comparison).\n\n### Pattern 2: `quantile_to_interval()` Usage\nBoth functions call this helper:\n- `get_coverage()`: `quantile_to_interval(forecast, format = \"wide\")`\n- `interval_coverage()`: `quantile_to_interval(observed, predicted, quantile_level)`\n\n### Pattern 3: Data Filtering by `interval_range`\nBoth functions filter results by interval_range (though differently):\n\n**`get_coverage()`:**\n- Merges back to original data to preserve structure\n\n**`interval_coverage()`:**\n```r\nreformatted <- reformatted[interval_range %in% r]\n```\n\n---\n\n## 7. Refactoring Opportunities\n\n1. **Extract bounds check logic** into a shared helper function\n2. **Standardize bounds check order** (currently inconsistent)\n3. **Create a generic interval coverage calculator** that both functions can use\n4. **Consolidate input validation** - both validate quantile inputs, could share logic\n5. **Separate concerns**: \n   - Core computation (bounds checking)\n   - Input validation\n   - Output formatting (vectorized vs. aggregated)\n\n---\n\n## 8. Key Differences Between Functions\n\n| Aspect | `get_coverage()` | `interval_coverage()` |\n|--------|-----------------|----------------------|\n| **Input Type** | data.table forecast object | Numeric vectors |\n| **Output Type** | Aggregated data.table | Logical vector |\n| **Aggregation** | Yes (by grouping variables) | No |\n| **Deviation Calc** | Yes (coverage - expected) | No |\n| **Quantile Coverage** | Yes (computed) | No |\n| **Flexibility** | Fixed to quantile format | Works with any interval_range |\n\n---\n\nThis analysis shows clear opportunities for code consolidation while respecting the different input/output requirements of each function."},{"type":"text","text":"agentId: a7562f9 (for resuming to continue this agent's work if needed)\n<usage>total_tokens: 54230\ntool_uses: 15\nduration_ms: 38323</usage>"}]}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"2cec5722-b93a-400b-92b2-ba9523a7e45d","tool_use_result":{"status":"completed","prompt":"I need to understand the code for issue #454 which is about deduplicating code between `get_coverage()` and `interval_coverage()`. \n\nPlease find and read:\n1. The `get_coverage()` function - likely in `R/get-coverage.R`\n2. The `interval_coverage()` function - likely in `R/metrics-quantile.R` or similar\n3. The `quantile_to_interval()` function (shared helper)\n4. Any existing tests related to coverage: look in `tests/testthat/` for test files containing \"coverage\" \n5. The test helpers/setup files: `tests/testthat/helper-*.R` and `tests/testthat/setup.R`\n\nFor each file, report the full content (or at least the relevant functions) with line numbers. I need to understand:\n- What code is duplicated between `get_coverage()` and `interval_coverage()`\n- What the `quantile_to_interval()` + bounds-check pattern looks like in each\n- How existing tests are structured (testthat conventions, fixtures, example data used)\n- What test data/fixtures are available\n\nWorking directory: /Users/nikos/github-synced/scoringutils-pipeline","agentId":"a7562f9","content":[{"type":"text","text":"Perfect! Now I have a comprehensive understanding of the code. Let me create a detailed report for you.\n\n## Summary Report: Issue #454 - Deduplicating Code Between `get_coverage()` and `interval_coverage()`\n\n### Overview\n\nIssue #454 is about identifying and deduplicating code shared between two functions:\n1. **`get_coverage()`** - `/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R` (lines 58-100)\n2. **`interval_coverage()`** - `/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R` (lines 341-365)\n\nBoth functions use similar patterns with helper functions to transform quantile-based forecasts to interval format and validate bounds.\n\n---\n\n## 1. `get_coverage()` Function\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R` (lines 58-100)\n\n```r\nget_coverage <- function(forecast, by = \"model\") {\n  # input checks\n  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n  assert_subset(get_forecast_type(forecast), \"quantile\")\n  \n  by <- setdiff(by, c(\"quantile_level\", \"interval_range\"))\n  assert_subset(by, names(forecast))\n  \n  # convert to wide interval format and compute interval coverage\n  interval_forecast <- quantile_to_interval(forecast, format = \"wide\")  # <-- SHARED CALL\n  interval_forecast[,\n    interval_coverage := (observed <= upper) & (observed >= lower)  # <-- BOUNDS CHECK\n  ][, c(\"lower\", \"upper\", \"observed\") := NULL]\n  interval_forecast[, interval_coverage_deviation :=\n                      interval_coverage - interval_range / 100]\n  \n  # merge interval range data with original data\n  forecast[, interval_range := get_range_from_quantile(quantile_level)]  # <-- SHARED CALL\n  forecast_cols <- colnames(forecast)\n  forecast_unit <- get_forecast_unit(forecast)\n  \n  forecast <- merge(forecast, interval_forecast,\n                    by = unique(c(forecast_unit, \"interval_range\")))\n  \n  # compute quantile coverage and deviation\n  forecast[, quantile_coverage := observed <= predicted]  # <-- BOUNDS CHECK\n  forecast[, quantile_coverage_deviation := quantile_coverage - quantile_level]\n  \n  # summarise and cleanup\n  new_metrics <- c(\"interval_coverage\", \"interval_coverage_deviation\",\n                   \"quantile_coverage\", \"quantile_coverage_deviation\")\n  setcolorder(forecast, unique(c(forecast_cols, \"interval_range\", new_metrics)))\n  forecast <- as.data.table(forecast)\n  by <- unique(c(by, \"quantile_level\", \"interval_range\"))\n  forecast <- forecast[, lapply(.SD, mean), by = by, .SDcols = new_metrics]\n  return(forecast[])\n}\n```\n\n**Key Operations:**\n- Takes a data.table forecast object in quantile format\n- Calls `quantile_to_interval()` to convert to wide interval format\n- Performs bounds check: `(observed <= upper) & (observed >= lower)`\n- Computes coverage deviation as difference from expected range\n- Summarizes by grouping variables with mean aggregation\n\n---\n\n## 2. `interval_coverage()` Function\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R` (lines 341-365)\n\n```r\ninterval_coverage <- function(observed, predicted,\n                              quantile_level, interval_range = 50) {\n  assert_input_quantile(observed, predicted, quantile_level)\n  assert_number(interval_range)\n  necessary_quantiles <- c(\n    (100 - interval_range) / 2,\n    100 - (100 - interval_range) / 2\n  ) / 100\n  if (!all(necessary_quantiles %in% quantile_level)) {\n    cli_abort(\n      c(\n        \"!\" = \"To compute the interval coverage for an interval range of\n        {.val {interval_range}%}, the {.val {necessary_quantiles}} quantiles\n        are required\"\n      )\n    )\n  }\n  r <- interval_range\n  reformatted <- quantile_to_interval(observed, predicted, quantile_level)  # <-- SHARED CALL\n  reformatted <- reformatted[interval_range %in% r]\n  reformatted[, interval_coverage := (observed >= lower) & (observed <= upper)]  # <-- BOUNDS CHECK\n  return(reformatted$interval_coverage)\n}\n```\n\n**Key Operations:**\n- Takes numeric vectors: `observed`, `predicted`, `quantile_level`\n- Validates that required quantiles exist for the requested interval range\n- Calls `quantile_to_interval()` to convert vectors to interval format\n- Performs bounds check: `(observed >= lower) & (observed <= upper)` (same logic, different order)\n- Returns boolean vector of coverage values\n\n---\n\n## 3. The `quantile_to_interval()` Helper Function\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R` (lines 31-104)\n\nThis is a polymorphic function that delegates based on input type:\n\n```r\nquantile_to_interval <- function(...) {\n  dots <- list(...)\n  if (is.data.frame(dots[[1]])) {\n    do.call(quantile_to_interval_dataframe, dots)\n  } else if (is.numeric(dots[[1]])) {\n    do.call(quantile_to_interval_numeric, dots)\n  } else {\n    cli_abort(\"Input must be either a data.frame or a numeric vector.\")\n  }\n}\n```\n\n**Dataframe version** (lines 62-104):\n- Takes a forecast data.table with `quantile_level` and `predicted` columns\n- Adds `boundary` column (lower/upper based on quantile_level <= 0.5)\n- Calls `get_range_from_quantile()` to compute interval_range\n- Handles median quantile (0.5) specially by duplicating it\n- Can return long or wide format\n\n**Numeric version** (lines 116-134):\n- Takes vectors: `observed`, `predicted`, `quantile_level`\n- Creates a data.table with `forecast_id`, `observed`, `predicted`, `quantile_level`\n- Delegates to dataframe version with `format = \"wide\"`\n- Returns ordered results by `forecast_id` and `interval_range`\n\n---\n\n## 4. The `get_range_from_quantile()` Helper\n\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R` (lines 185-193)\n\n```r\nget_range_from_quantile <- function(quantile_level) {\n  boundary <- ifelse(quantile_level <= 0.5, \"lower\", \"upper\")\n  interval_range <- ifelse(\n    boundary == \"lower\",\n    round((1 - 2 * quantile_level) * 100, digits = 10),\n    round((2 * quantile_level - 1) * 100, digits = 10)\n  )\n  return(interval_range)\n}\n```\n\nConverts a quantile level to the interval range it represents (e.g., 0.25 → 50, 0.75 → 50).\n\n---\n\n## 5. Test Files and Test Data\n\n### Test File: `test-get-coverage.R`\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R`\n\n**Tests for `get_coverage()`:**\n\n1. **Basic functionality test** (lines 6-25):\n   - Verifies correct column names and structure\n   - Uses `example_quantile` dataset\n   - Checks for expected columns: `interval_coverage`, `interval_coverage_deviation`, `quantile_coverage`, `quantile_coverage_deviation`\n\n2. **Class verification test** (lines 27-30):\n   - Ensures output is a data.table\n\n3. **Non-symmetric intervals test** (lines 32-54):\n   - Tests graceful handling of missing quantiles\n   - Missing interval_range values should be `NA`\n   - Tests both explicit removal and NA values in predicted column\n\n**Tests for `plot_interval_coverage()`** (lines 60-76):\n- Generates coverage data and plots it\n- Uses visual regression testing with `vdiffr`\n\n**Tests for `plot_quantile_coverage()`** (lines 82-92):\n- Similar visual regression tests\n\n### Test File: `test-metrics-quantile.R`\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R`\n\n**Tests for `interval_coverage()`** (lines 676-703):\n\n1. **Basic functionality test** (lines 676-681):\n   ```r\n   test_that(\"interval_coverage works\", {\n     expect_equal(\n       interval_coverage(observed, predicted, quantile_level, interval_range = 50),\n       c(TRUE, FALSE, FALSE)\n     )\n   })\n   ```\n\n2. **Input validation test** (lines 683-688):\n   - Tests that `interval_range` must be a single number\n\n3. **Missing quantile test** (lines 690-703):\n   - Verifies error when required quantiles are missing\n   - Tests for specific error message\n\n### Test Data (Setup File)\n**File:** `/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/setup.R`\n\nPre-computed test variables:\n```r\nmetrics_no_cov <- get_metrics(\n  example_quantile,\n  exclude = c(\"interval_coverage_50\", \"interval_coverage_90\")\n)\n\nexample_quantile_df <- as.data.frame(na.omit(example_quantile))\n\nscores_quantile <- score(example_quantile)\nscores_sample_continuous <- score(example_sample_continuous)\nscores_sample_discrete <- score(example_sample_discrete)\nscores_point <- score(example_point)\nscores_binary <- score(example_binary)\nscores_nominal <- score(example_nominal)\n```\n\n### Test Datasets Used in `test-metrics-quantile.R`:\n\nGlobal test data (lines 1-16):\n```r\nobserved <- c(1, -15, 22)\npredicted <- rbind(\n  c(-1, 0, 1, 2, 3),\n  c(-2, 1, 2, 2, 4),\n  c(-2, 0, 3, 3, 4)\n)\nquantile_level <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n\n# Also from covidHubUtils tests:\ny <- c(1, -15, 22)\nforecast_quantiles_matrix <- rbind(\n  c(-1, 0, 1, 2, 3),\n  c(-2, 1, 2, 2, 4),\n  c(-2, 0, 3, 3, 4)\n)\nforecast_quantile_probs <- c(0.1, 0.25, 0.5, 0.75, 0.9)\n```\n\n---\n\n## 6. Identified Duplicated Code Patterns\n\n### Pattern 1: Bounds Checking\nBoth functions perform nearly identical bounds checks:\n\n**`get_coverage()`:**\n```r\ninterval_coverage := (observed <= upper) & (observed >= lower)\n```\n\n**`interval_coverage()`:**\n```r\ninterval_coverage := (observed >= lower) & (observed <= upper)\n```\n\nThese are identical in logic (just different order of comparison).\n\n### Pattern 2: `quantile_to_interval()` Usage\nBoth functions call this helper:\n- `get_coverage()`: `quantile_to_interval(forecast, format = \"wide\")`\n- `interval_coverage()`: `quantile_to_interval(observed, predicted, quantile_level)`\n\n### Pattern 3: Data Filtering by `interval_range`\nBoth functions filter results by interval_range (though differently):\n\n**`get_coverage()`:**\n- Merges back to original data to preserve structure\n\n**`interval_coverage()`:**\n```r\nreformatted <- reformatted[interval_range %in% r]\n```\n\n---\n\n## 7. Refactoring Opportunities\n\n1. **Extract bounds check logic** into a shared helper function\n2. **Standardize bounds check order** (currently inconsistent)\n3. **Create a generic interval coverage calculator** that both functions can use\n4. **Consolidate input validation** - both validate quantile inputs, could share logic\n5. **Separate concerns**: \n   - Core computation (bounds checking)\n   - Input validation\n   - Output formatting (vectorized vs. aggregated)\n\n---\n\n## 8. Key Differences Between Functions\n\n| Aspect | `get_coverage()` | `interval_coverage()` |\n|--------|-----------------|----------------------|\n| **Input Type** | data.table forecast object | Numeric vectors |\n| **Output Type** | Aggregated data.table | Logical vector |\n| **Aggregation** | Yes (by grouping variables) | No |\n| **Deviation Calc** | Yes (coverage - expected) | No |\n| **Quantile Coverage** | Yes (computed) | No |\n| **Flexibility** | Fixed to quantile format | Works with any interval_range |\n\n---\n\nThis analysis shows clear opportunities for code consolidation while respecting the different input/output requirements of each function."}],"totalDurationMs":38323,"totalTokens":54230,"totalToolUseCount":15,"usage":{"input_tokens":7,"cache_creation_input_tokens":17932,"cache_read_input_tokens":32915,"output_tokens":3376,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":17932,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01EWKou7XgRGVv1Q2Eqzq3oZ","type":"message","role":"assistant","content":[{"type":"text","text":"Excellent, now I have a thorough understanding of the code. Let me also check if there's an existing issue file for #454."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4031,"cache_read_input_tokens":49562,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4031},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"b9789833-fccc-4e93-a78c-7d56c13d6de9"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01EWKou7XgRGVv1Q2Eqzq3oZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019WLtJQ5FURCQ4sVxjSGARf","name":"TodoWrite","input":{"todos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"in_progress","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4031,"cache_read_input_tokens":49562,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4031},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"11a6408d-40ed-4e9c-b56e-369ce5217345"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019WLtJQ5FURCQ4sVxjSGARf","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"dac50b45-91dc-4e7d-b5ec-df2f30bc6973","tool_use_result":{"oldTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"in_progress","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"pending","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"pending","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}],"newTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"in_progress","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01FLBqbCWMQLAspX15DUv9J1","type":"message","role":"assistant","content":[{"type":"text","text":"Let me read the key source files to be precise about my test specs."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":321,"cache_read_input_tokens":53593,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":321},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"773ad595-260c-4a00-9cc4-d706fb696855"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01FLBqbCWMQLAspX15DUv9J1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015PW6tsAVD4rnNpF6fiPmhJ","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":321,"cache_read_input_tokens":53593,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":321},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"5818b36f-b919-48de-9a94-5b420df5e9c9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015PW6tsAVD4rnNpF6fiPmhJ","type":"tool_result","content":"     1→#' @title Get quantile and interval coverage values for quantile-based forecasts\n     2→#'\n     3→#' @description\n     4→#' For a validated forecast object in a quantile-based format\n     5→#' (see [as_forecast_quantile()] for more information), this function computes:\n     6→#' - interval coverage of central prediction intervals\n     7→#' - quantile coverage for predictive quantiles\n     8→#' - the deviation between desired and actual coverage (both for interval and\n     9→#' quantile coverage)\n    10→#'\n    11→#' Coverage values are computed for a specific level of grouping, as specified\n    12→#' in the `by` argument. By default, coverage values are computed per model.\n    13→#'\n    14→#' **Interval coverage**\n    15→#'\n    16→#' Interval coverage for a given interval range is defined as the proportion of\n    17→#' observations that fall within the corresponding central prediction intervals.\n    18→#' Central prediction intervals are symmetric around the median and formed\n    19→#' by two quantiles that denote the lower and upper bound. For example, the 50%\n    20→#' central prediction interval is the interval between the 0.25 and 0.75\n    21→#' quantiles of the predictive distribution.\n    22→#'\n    23→#' **Quantile coverage**\n    24→#'\n    25→#' Quantile coverage for a given quantile level is defined as the proportion of\n    26→#' observed values that are smaller than the corresponding predictive quantile.\n    27→#' For example, the 0.5 quantile coverage is the proportion of observed values\n    28→#' that are smaller than the 0.5 quantile of the predictive distribution.\n    29→#' Just as above, for a single observation and the quantile of a single\n    30→#' predictive distribution, the value will either be `TRUE` or `FALSE`.\n    31→#'\n    32→#' **Coverage deviation**\n    33→#'\n    34→#' The coverage deviation is the difference between the desired coverage\n    35→#' (can be either interval or quantile coverage) and the\n    36→#' actual coverage. For example, if the desired coverage is 90% and the actual\n    37→#' coverage is 80%, the coverage deviation is -0.1.\n    38→#' @return\n    39→#' A data.table with columns as specified in `by` and additional\n    40→#' columns for the coverage values described above\n    41→#' @inheritParams score\n    42→#' @param by character vector that denotes the level of grouping for which the\n    43→#'   coverage values should be computed. By default (`\"model\"`), one coverage\n    44→#'   value per model will be returned.\n    45→#' @return\n    46→#' a data.table with columns \"interval_coverage\",\n    47→#' \"interval_coverage_deviation\", \"quantile_coverage\",\n    48→#' \"quantile_coverage_deviation\" and the columns specified in `by`.\n    49→#' @importFrom data.table setcolorder\n    50→#' @importFrom checkmate assert_subset\n    51→#' @examples\n    52→#' example_quantile |>\n    53→#'   as_forecast_quantile() |>\n    54→#'   get_coverage(by = \"model\")\n    55→#' @export\n    56→#' @keywords scoring\n    57→#' @export\n    58→get_coverage <- function(forecast, by = \"model\") {\n    59→  # input checks ---------------------------------------------------------------\n    60→  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n    61→  assert_subset(get_forecast_type(forecast), \"quantile\")\n    62→\n    63→  # remove \"quantile_level\" and \"interval_range\" from `by` if present, as these\n    64→  # are included anyway\n    65→  by <- setdiff(by, c(\"quantile_level\", \"interval_range\"))\n    66→  assert_subset(by, names(forecast))\n    67→\n    68→  # convert to wide interval format and compute interval coverage --------------\n    69→  interval_forecast <- quantile_to_interval(forecast, format = \"wide\")\n    70→  interval_forecast[,\n    71→    interval_coverage := (observed <= upper) & (observed >= lower)\n    72→  ][, c(\"lower\", \"upper\", \"observed\") := NULL]\n    73→  interval_forecast[, interval_coverage_deviation :=\n    74→                      interval_coverage - interval_range / 100]\n    75→\n    76→  # merge interval range data with original data -------------------------------\n    77→  # preparations\n    78→  forecast[, interval_range := get_range_from_quantile(quantile_level)]\n    79→  forecast_cols <- colnames(forecast) # store so we can reset column order later\n    80→  forecast_unit <- get_forecast_unit(forecast)\n    81→\n    82→  forecast <- merge(forecast, interval_forecast,\n    83→                    by = unique(c(forecast_unit, \"interval_range\")))\n    84→\n    85→  # compute quantile coverage and deviation ------------------------------------\n    86→  forecast[, quantile_coverage := observed <= predicted]\n    87→  forecast[, quantile_coverage_deviation := quantile_coverage - quantile_level]\n    88→\n    89→  # summarise coverage values according to `by` and cleanup --------------------\n    90→  # reset column order\n    91→  new_metrics <- c(\"interval_coverage\", \"interval_coverage_deviation\",\n    92→                   \"quantile_coverage\", \"quantile_coverage_deviation\")\n    93→  setcolorder(forecast, unique(c(forecast_cols, \"interval_range\", new_metrics)))\n    94→  # remove forecast class and convert to regular data.table\n    95→  forecast <- as.data.table(forecast)\n    96→  by <- unique(c(by, \"quantile_level\", \"interval_range\"))\n    97→  # summarise\n    98→  forecast <- forecast[, lapply(.SD, mean), by = by, .SDcols = new_metrics]\n    99→  return(forecast[])\n   100→}\n   101→\n   102→\n   103→#' @title Plot interval coverage\n   104→#'\n   105→#' @description\n   106→#' Plot interval coverage values (see [get_coverage()] for more information).\n   107→#'\n   108→#' @param coverage A data frame of coverage values as produced by\n   109→#' [get_coverage()].\n   110→#' @param colour According to which variable shall the graphs be coloured?\n   111→#' Default is \"model\".\n   112→#' @returns ggplot object with a plot of interval coverage\n   113→#' @importFrom ggplot2 ggplot scale_colour_manual scale_fill_manual .data\n   114→#' facet_wrap facet_grid geom_polygon geom_line xlab ylab\n   115→#' @importFrom checkmate assert_subset\n   116→#' @importFrom data.table dcast\n   117→#' @export\n   118→#' @examples\n   119→#' \\dontshow{\n   120→#'   data.table::setDTthreads(2) # restricts number of cores used on CRAN\n   121→#' }\n   122→#' example <- as_forecast_quantile(example_quantile)\n   123→#' coverage <- get_coverage(example, by = \"model\")\n   124→#' plot_interval_coverage(coverage)\n   125→plot_interval_coverage <- function(coverage,\n   126→                                   colour = \"model\") {\n   127→  coverage <- ensure_data.table(coverage)\n   128→  assert_subset(colour, names(coverage))\n   129→\n   130→  # in case quantile columns are present, remove them and then take unique\n   131→  # values. This doesn't visually affect the plot, but prevents lines from being\n   132→  # drawn twice.\n   133→  del <- c(\"quantile_level\", \"quantile_coverage\", \"quantile_coverage_deviation\")\n   134→  suppressWarnings(coverage[, eval(del) := NULL])\n   135→  coverage <- unique(coverage)\n   136→\n   137→  ## overall model calibration - empirical interval coverage\n   138→  p1 <- ggplot(coverage, aes(\n   139→    x = interval_range,\n   140→    colour = .data[[colour]]\n   141→  )) +\n   142→    geom_polygon(\n   143→      data = data.frame(\n   144→        x = c(0, 0, 100),\n   145→        y = c(0, 100, 100),\n   146→        g = c(\"o\", \"o\", \"o\"),\n   147→        stringsAsFactors = TRUE\n   148→      ),\n   149→      aes(\n   150→        x = x, y = y, group = g,\n   151→        fill = g\n   152→      ),\n   153→      alpha = 0.05,\n   154→      colour = \"white\",\n   155→      fill = \"olivedrab3\"\n   156→    ) +\n   157→    geom_line(\n   158→      aes(y = interval_range),\n   159→      colour = \"grey\",\n   160→      linetype = \"dashed\"\n   161→    ) +\n   162→    geom_line(aes(y = interval_coverage * 100)) +\n   163→    theme_scoringutils() +\n   164→    ylab(\"% Obs inside interval\") +\n   165→    xlab(\"Nominal interval coverage\") +\n   166→    coord_cartesian(expand = FALSE)\n   167→\n   168→  return(p1)\n   169→}\n   170→\n   171→\n   172→#' @title Plot quantile coverage\n   173→#'\n   174→#' @description\n   175→#' Plot quantile coverage values (see [get_coverage()] for more information).\n   176→#'\n   177→#' @inheritParams plot_interval_coverage\n   178→#' @param colour String, according to which variable shall the graphs be\n   179→#' coloured? Default is \"model\".\n   180→#' @returns A ggplot object with a plot of interval coverage\n   181→#' @importFrom ggplot2 ggplot scale_colour_manual scale_fill_manual .data aes\n   182→#'   scale_y_continuous geom_line\n   183→#' @importFrom checkmate assert_subset assert_data_frame\n   184→#' @importFrom data.table dcast\n   185→#' @export\n   186→#' @examples\n   187→#' example <- as_forecast_quantile(example_quantile)\n   188→#' coverage <- get_coverage(example, by = \"model\")\n   189→#' plot_quantile_coverage(coverage)\n   190→\n   191→plot_quantile_coverage <- function(coverage,\n   192→                                   colour = \"model\") {\n   193→  coverage <- assert_data_frame(coverage)\n   194→  assert_subset(colour, names(coverage))\n   195→\n   196→  p2 <- ggplot(\n   197→    data = coverage,\n   198→    aes(x = quantile_level, colour = .data[[colour]])\n   199→  ) +\n   200→    geom_polygon(\n   201→      data = data.frame(\n   202→        x = c(\n   203→          0, 0.5, 0.5,\n   204→          0.5, 0.5, 1\n   205→        ),\n   206→        y = c(\n   207→          0, 0, 0.5,\n   208→          0.5, 1, 1\n   209→        ),\n   210→        g = c(\"o\", \"o\", \"o\"),\n   211→        stringsAsFactors = TRUE\n   212→      ),\n   213→      aes(\n   214→        x = x, y = y, group = g,\n   215→        fill = g\n   216→      ),\n   217→      alpha = 0.05,\n   218→      colour = \"white\",\n   219→      fill = \"olivedrab3\"\n   220→    ) +\n   221→    geom_line(\n   222→      aes(y = quantile_level),\n   223→      colour = \"grey\",\n   224→      linetype = \"dashed\"\n   225→    ) +\n   226→    geom_line(aes(y = quantile_coverage)) +\n   227→    theme_scoringutils() +\n   228→    xlab(\"Quantile level\") +\n   229→    ylab(\"% Obs below quantile level\") +\n   230→    scale_y_continuous(\n   231→      labels = function(x) {\n   232→        paste(100 * x)\n   233→      }\n   234→    ) +\n   235→    coord_cartesian(expand = FALSE)\n   236→\n   237→  return(p2)\n   238→}\n   239→"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"4f98843c-56e1-458f-b3fb-ec6b656f9a43","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/R/get-coverage.R","content":"#' @title Get quantile and interval coverage values for quantile-based forecasts\n#'\n#' @description\n#' For a validated forecast object in a quantile-based format\n#' (see [as_forecast_quantile()] for more information), this function computes:\n#' - interval coverage of central prediction intervals\n#' - quantile coverage for predictive quantiles\n#' - the deviation between desired and actual coverage (both for interval and\n#' quantile coverage)\n#'\n#' Coverage values are computed for a specific level of grouping, as specified\n#' in the `by` argument. By default, coverage values are computed per model.\n#'\n#' **Interval coverage**\n#'\n#' Interval coverage for a given interval range is defined as the proportion of\n#' observations that fall within the corresponding central prediction intervals.\n#' Central prediction intervals are symmetric around the median and formed\n#' by two quantiles that denote the lower and upper bound. For example, the 50%\n#' central prediction interval is the interval between the 0.25 and 0.75\n#' quantiles of the predictive distribution.\n#'\n#' **Quantile coverage**\n#'\n#' Quantile coverage for a given quantile level is defined as the proportion of\n#' observed values that are smaller than the corresponding predictive quantile.\n#' For example, the 0.5 quantile coverage is the proportion of observed values\n#' that are smaller than the 0.5 quantile of the predictive distribution.\n#' Just as above, for a single observation and the quantile of a single\n#' predictive distribution, the value will either be `TRUE` or `FALSE`.\n#'\n#' **Coverage deviation**\n#'\n#' The coverage deviation is the difference between the desired coverage\n#' (can be either interval or quantile coverage) and the\n#' actual coverage. For example, if the desired coverage is 90% and the actual\n#' coverage is 80%, the coverage deviation is -0.1.\n#' @return\n#' A data.table with columns as specified in `by` and additional\n#' columns for the coverage values described above\n#' @inheritParams score\n#' @param by character vector that denotes the level of grouping for which the\n#'   coverage values should be computed. By default (`\"model\"`), one coverage\n#'   value per model will be returned.\n#' @return\n#' a data.table with columns \"interval_coverage\",\n#' \"interval_coverage_deviation\", \"quantile_coverage\",\n#' \"quantile_coverage_deviation\" and the columns specified in `by`.\n#' @importFrom data.table setcolorder\n#' @importFrom checkmate assert_subset\n#' @examples\n#' example_quantile |>\n#'   as_forecast_quantile() |>\n#'   get_coverage(by = \"model\")\n#' @export\n#' @keywords scoring\n#' @export\nget_coverage <- function(forecast, by = \"model\") {\n  # input checks ---------------------------------------------------------------\n  forecast <- clean_forecast(forecast, copy = TRUE, na.omit = TRUE)\n  assert_subset(get_forecast_type(forecast), \"quantile\")\n\n  # remove \"quantile_level\" and \"interval_range\" from `by` if present, as these\n  # are included anyway\n  by <- setdiff(by, c(\"quantile_level\", \"interval_range\"))\n  assert_subset(by, names(forecast))\n\n  # convert to wide interval format and compute interval coverage --------------\n  interval_forecast <- quantile_to_interval(forecast, format = \"wide\")\n  interval_forecast[,\n    interval_coverage := (observed <= upper) & (observed >= lower)\n  ][, c(\"lower\", \"upper\", \"observed\") := NULL]\n  interval_forecast[, interval_coverage_deviation :=\n                      interval_coverage - interval_range / 100]\n\n  # merge interval range data with original data -------------------------------\n  # preparations\n  forecast[, interval_range := get_range_from_quantile(quantile_level)]\n  forecast_cols <- colnames(forecast) # store so we can reset column order later\n  forecast_unit <- get_forecast_unit(forecast)\n\n  forecast <- merge(forecast, interval_forecast,\n                    by = unique(c(forecast_unit, \"interval_range\")))\n\n  # compute quantile coverage and deviation ------------------------------------\n  forecast[, quantile_coverage := observed <= predicted]\n  forecast[, quantile_coverage_deviation := quantile_coverage - quantile_level]\n\n  # summarise coverage values according to `by` and cleanup --------------------\n  # reset column order\n  new_metrics <- c(\"interval_coverage\", \"interval_coverage_deviation\",\n                   \"quantile_coverage\", \"quantile_coverage_deviation\")\n  setcolorder(forecast, unique(c(forecast_cols, \"interval_range\", new_metrics)))\n  # remove forecast class and convert to regular data.table\n  forecast <- as.data.table(forecast)\n  by <- unique(c(by, \"quantile_level\", \"interval_range\"))\n  # summarise\n  forecast <- forecast[, lapply(.SD, mean), by = by, .SDcols = new_metrics]\n  return(forecast[])\n}\n\n\n#' @title Plot interval coverage\n#'\n#' @description\n#' Plot interval coverage values (see [get_coverage()] for more information).\n#'\n#' @param coverage A data frame of coverage values as produced by\n#' [get_coverage()].\n#' @param colour According to which variable shall the graphs be coloured?\n#' Default is \"model\".\n#' @returns ggplot object with a plot of interval coverage\n#' @importFrom ggplot2 ggplot scale_colour_manual scale_fill_manual .data\n#' facet_wrap facet_grid geom_polygon geom_line xlab ylab\n#' @importFrom checkmate assert_subset\n#' @importFrom data.table dcast\n#' @export\n#' @examples\n#' \\dontshow{\n#'   data.table::setDTthreads(2) # restricts number of cores used on CRAN\n#' }\n#' example <- as_forecast_quantile(example_quantile)\n#' coverage <- get_coverage(example, by = \"model\")\n#' plot_interval_coverage(coverage)\nplot_interval_coverage <- function(coverage,\n                                   colour = \"model\") {\n  coverage <- ensure_data.table(coverage)\n  assert_subset(colour, names(coverage))\n\n  # in case quantile columns are present, remove them and then take unique\n  # values. This doesn't visually affect the plot, but prevents lines from being\n  # drawn twice.\n  del <- c(\"quantile_level\", \"quantile_coverage\", \"quantile_coverage_deviation\")\n  suppressWarnings(coverage[, eval(del) := NULL])\n  coverage <- unique(coverage)\n\n  ## overall model calibration - empirical interval coverage\n  p1 <- ggplot(coverage, aes(\n    x = interval_range,\n    colour = .data[[colour]]\n  )) +\n    geom_polygon(\n      data = data.frame(\n        x = c(0, 0, 100),\n        y = c(0, 100, 100),\n        g = c(\"o\", \"o\", \"o\"),\n        stringsAsFactors = TRUE\n      ),\n      aes(\n        x = x, y = y, group = g,\n        fill = g\n      ),\n      alpha = 0.05,\n      colour = \"white\",\n      fill = \"olivedrab3\"\n    ) +\n    geom_line(\n      aes(y = interval_range),\n      colour = \"grey\",\n      linetype = \"dashed\"\n    ) +\n    geom_line(aes(y = interval_coverage * 100)) +\n    theme_scoringutils() +\n    ylab(\"% Obs inside interval\") +\n    xlab(\"Nominal interval coverage\") +\n    coord_cartesian(expand = FALSE)\n\n  return(p1)\n}\n\n\n#' @title Plot quantile coverage\n#'\n#' @description\n#' Plot quantile coverage values (see [get_coverage()] for more information).\n#'\n#' @inheritParams plot_interval_coverage\n#' @param colour String, according to which variable shall the graphs be\n#' coloured? Default is \"model\".\n#' @returns A ggplot object with a plot of interval coverage\n#' @importFrom ggplot2 ggplot scale_colour_manual scale_fill_manual .data aes\n#'   scale_y_continuous geom_line\n#' @importFrom checkmate assert_subset assert_data_frame\n#' @importFrom data.table dcast\n#' @export\n#' @examples\n#' example <- as_forecast_quantile(example_quantile)\n#' coverage <- get_coverage(example, by = \"model\")\n#' plot_quantile_coverage(coverage)\n\nplot_quantile_coverage <- function(coverage,\n                                   colour = \"model\") {\n  coverage <- assert_data_frame(coverage)\n  assert_subset(colour, names(coverage))\n\n  p2 <- ggplot(\n    data = coverage,\n    aes(x = quantile_level, colour = .data[[colour]])\n  ) +\n    geom_polygon(\n      data = data.frame(\n        x = c(\n          0, 0.5, 0.5,\n          0.5, 0.5, 1\n        ),\n        y = c(\n          0, 0, 0.5,\n          0.5, 1, 1\n        ),\n        g = c(\"o\", \"o\", \"o\"),\n        stringsAsFactors = TRUE\n      ),\n      aes(\n        x = x, y = y, group = g,\n        fill = g\n      ),\n      alpha = 0.05,\n      colour = \"white\",\n      fill = \"olivedrab3\"\n    ) +\n    geom_line(\n      aes(y = quantile_level),\n      colour = \"grey\",\n      linetype = \"dashed\"\n    ) +\n    geom_line(aes(y = quantile_coverage)) +\n    theme_scoringutils() +\n    xlab(\"Quantile level\") +\n    ylab(\"% Obs below quantile level\") +\n    scale_y_continuous(\n      labels = function(x) {\n        paste(100 * x)\n      }\n    ) +\n    coord_cartesian(expand = FALSE)\n\n  return(p2)\n}\n","numLines":239,"startLine":1,"totalLines":239}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01FLBqbCWMQLAspX15DUv9J1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017pnRzru1AM7KGQocKxHyQG","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R","offset":340,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":321,"cache_read_input_tokens":53593,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":321},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"67afa91a-6138-4618-b62a-c70ca3a8fc85"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017pnRzru1AM7KGQocKxHyQG","type":"tool_result","content":"   340→#' interval_coverage(observed, predicted, quantile_level)\n   341→interval_coverage <- function(observed, predicted,\n   342→                              quantile_level, interval_range = 50) {\n   343→  assert_input_quantile(observed, predicted, quantile_level)\n   344→  assert_number(interval_range)\n   345→  necessary_quantiles <- c(\n   346→    (100 - interval_range) / 2,\n   347→    100 - (100 - interval_range) / 2\n   348→  ) / 100\n   349→  if (!all(necessary_quantiles %in% quantile_level)) {\n   350→    #nolint start: keyword_quote_linter object_usage_linter\n   351→    cli_abort(\n   352→      c(\n   353→        \"!\" = \"To compute the interval coverage for an interval range of\n   354→        {.val {interval_range}%}, the {.val {necessary_quantiles}} quantiles\n   355→        are required\"\n   356→      )\n   357→    )\n   358→    #nolint end\n   359→  }\n   360→  r <- interval_range\n   361→  reformatted <- quantile_to_interval(observed, predicted, quantile_level)\n   362→  reformatted <- reformatted[interval_range %in% r]\n   363→  reformatted[, interval_coverage := (observed >= lower) & (observed <= upper)]\n   364→  return(reformatted$interval_coverage)\n   365→}\n   366→\n   367→\n   368→#' @title Determines bias of quantile forecasts\n   369→#'"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"b1d37c87-99ea-405e-9ee8-2b7319261de6","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/R/metrics-quantile.R","content":"#' interval_coverage(observed, predicted, quantile_level)\ninterval_coverage <- function(observed, predicted,\n                              quantile_level, interval_range = 50) {\n  assert_input_quantile(observed, predicted, quantile_level)\n  assert_number(interval_range)\n  necessary_quantiles <- c(\n    (100 - interval_range) / 2,\n    100 - (100 - interval_range) / 2\n  ) / 100\n  if (!all(necessary_quantiles %in% quantile_level)) {\n    #nolint start: keyword_quote_linter object_usage_linter\n    cli_abort(\n      c(\n        \"!\" = \"To compute the interval coverage for an interval range of\n        {.val {interval_range}%}, the {.val {necessary_quantiles}} quantiles\n        are required\"\n      )\n    )\n    #nolint end\n  }\n  r <- interval_range\n  reformatted <- quantile_to_interval(observed, predicted, quantile_level)\n  reformatted <- reformatted[interval_range %in% r]\n  reformatted[, interval_coverage := (observed >= lower) & (observed <= upper)]\n  return(reformatted$interval_coverage)\n}\n\n\n#' @title Determines bias of quantile forecasts\n#'","numLines":30,"startLine":340,"totalLines":713}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01FLBqbCWMQLAspX15DUv9J1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012deur3vMpuewsRZXNKkbcP","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":321,"cache_read_input_tokens":53593,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":321},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"2660709a-3b6b-433a-8a16-50d5a6032f8a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012deur3vMpuewsRZXNKkbcP","type":"tool_result","content":"     1→# ==============================================================================\n     2→# `get_coverage()` # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→ex_coverage <- example_quantile[model == \"EuroCOVIDhub-ensemble\"]\n     5→\n     6→test_that(\"get_coverage() works as expected\", {\n     7→  cov <- example_quantile |>\n     8→    get_coverage(by = get_forecast_unit(example_quantile))\n     9→\n    10→  expect_identical(\n    11→    sort(colnames(cov)),\n    12→    sort(c(get_forecast_unit(example_quantile), c(\n    13→      \"interval_range\", \"quantile_level\", \"interval_coverage\", \"interval_coverage_deviation\",\n    14→      \"quantile_coverage\", \"quantile_coverage_deviation\"\n    15→    )))\n    16→  )\n    17→\n    18→  expect_identical(nrow(cov), nrow(na.omit(example_quantile)))\n    19→\n    20→  expect_s3_class(\n    21→    cov,\n    22→    c(\"data.table\", \"data.frame\"),\n    23→    exact = TRUE\n    24→  )\n    25→})\n    26→\n    27→test_that(\"get_coverage() outputs an object of class c('data.table', 'data.frame'\", {\n    28→  cov <- get_coverage(example_quantile)\n    29→  expect_s3_class(cov, c(\"data.table\", \"data.frame\"), exact = TRUE)\n    30→})\n    31→\n    32→test_that(\"get_coverage() can deal with non-symmetric prediction intervals\", {\n    33→  # the expected result is that `get_coverage()` just works. However,\n    34→  # all interval coverages with missing values should just be `NA`\n    35→  test <- data.table::copy(example_quantile)\n    36→  test <- test[!quantile_level %in% c(0.2, 0.3, 0.5)]\n    37→\n    38→  cov <- expect_no_condition(get_coverage(test))\n    39→\n    40→  prediction_intervals <- get_range_from_quantile(c(0.2, 0.3, 0.5))\n    41→\n    42→  missing <- cov[interval_range %in% prediction_intervals]\n    43→  not_missing <- cov[!interval_range %in% prediction_intervals]\n    44→\n    45→  expect_true(all(is.na(missing$interval_coverage)))\n    46→  expect_false(anyNA(not_missing))\n    47→\n    48→  # test for a version where values are not missing, but just `NA`\n    49→  # since `get_coverage()` calls `na.omit`, the result should be the same.\n    50→  test <- data.table::copy(example_quantile)\n    51→  test <- test[quantile_level %in% c(0.2, 0.3, 0.5), predicted := NA]\n    52→  cov2 <- get_coverage(test)\n    53→  expect_identical(cov, cov2)\n    54→})\n    55→\n    56→\n    57→# ==============================================================================\n    58→# plot_interval_coverage() # nolint: commented_code_linter\n    59→# ==============================================================================\n    60→test_that(\"plot_interval_coverage() works as expected\", {\n    61→  coverage <- example_quantile |>\n    62→    na.omit() |>\n    63→    as_forecast_quantile() |>\n    64→    get_coverage(by = \"model\")\n    65→  p <- plot_interval_coverage(coverage)\n    66→  expect_s3_class(p, \"ggplot\")\n    67→  skip_on_cran()\n    68→  suppressWarnings(vdiffr::expect_doppelganger(\"plot_interval_coverage\", p))\n    69→\n    70→  # make sure that plot_interval_coverage() doesn't drop column names\n    71→  expect_true(all(c(\n    72→    \"interval_coverage\", \"interval_coverage_deviation\",\n    73→    \"quantile_coverage\", \"quantile_coverage_deviation\"\n    74→  ) %in%\n    75→    names(coverage)))\n    76→})\n    77→\n    78→\n    79→# ==============================================================================\n    80→# plot_quantile_coverage() # nolint: commented_code_linter\n    81→# ==============================================================================\n    82→test_that(\"plot_quantile_coverage() works as expected\", {\n    83→  coverage <- example_quantile |>\n    84→    na.omit() |>\n    85→    as_forecast_quantile() |>\n    86→    get_coverage(by = c(\"model\", \"quantile_level\"))\n    87→\n    88→  p <- plot_quantile_coverage(coverage)\n    89→  expect_s3_class(p, \"ggplot\")\n    90→  skip_on_cran()\n    91→  suppressWarnings(vdiffr::expect_doppelganger(\"plot_quantile_coverage\", p))\n    92→})\n    93→"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"67cceaa7-2f07-4c0b-94f1-6f9b7e661777","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-get-coverage.R","content":"# ==============================================================================\n# `get_coverage()` # nolint: commented_code_linter\n# ==============================================================================\nex_coverage <- example_quantile[model == \"EuroCOVIDhub-ensemble\"]\n\ntest_that(\"get_coverage() works as expected\", {\n  cov <- example_quantile |>\n    get_coverage(by = get_forecast_unit(example_quantile))\n\n  expect_identical(\n    sort(colnames(cov)),\n    sort(c(get_forecast_unit(example_quantile), c(\n      \"interval_range\", \"quantile_level\", \"interval_coverage\", \"interval_coverage_deviation\",\n      \"quantile_coverage\", \"quantile_coverage_deviation\"\n    )))\n  )\n\n  expect_identical(nrow(cov), nrow(na.omit(example_quantile)))\n\n  expect_s3_class(\n    cov,\n    c(\"data.table\", \"data.frame\"),\n    exact = TRUE\n  )\n})\n\ntest_that(\"get_coverage() outputs an object of class c('data.table', 'data.frame'\", {\n  cov <- get_coverage(example_quantile)\n  expect_s3_class(cov, c(\"data.table\", \"data.frame\"), exact = TRUE)\n})\n\ntest_that(\"get_coverage() can deal with non-symmetric prediction intervals\", {\n  # the expected result is that `get_coverage()` just works. However,\n  # all interval coverages with missing values should just be `NA`\n  test <- data.table::copy(example_quantile)\n  test <- test[!quantile_level %in% c(0.2, 0.3, 0.5)]\n\n  cov <- expect_no_condition(get_coverage(test))\n\n  prediction_intervals <- get_range_from_quantile(c(0.2, 0.3, 0.5))\n\n  missing <- cov[interval_range %in% prediction_intervals]\n  not_missing <- cov[!interval_range %in% prediction_intervals]\n\n  expect_true(all(is.na(missing$interval_coverage)))\n  expect_false(anyNA(not_missing))\n\n  # test for a version where values are not missing, but just `NA`\n  # since `get_coverage()` calls `na.omit`, the result should be the same.\n  test <- data.table::copy(example_quantile)\n  test <- test[quantile_level %in% c(0.2, 0.3, 0.5), predicted := NA]\n  cov2 <- get_coverage(test)\n  expect_identical(cov, cov2)\n})\n\n\n# ==============================================================================\n# plot_interval_coverage() # nolint: commented_code_linter\n# ==============================================================================\ntest_that(\"plot_interval_coverage() works as expected\", {\n  coverage <- example_quantile |>\n    na.omit() |>\n    as_forecast_quantile() |>\n    get_coverage(by = \"model\")\n  p <- plot_interval_coverage(coverage)\n  expect_s3_class(p, \"ggplot\")\n  skip_on_cran()\n  suppressWarnings(vdiffr::expect_doppelganger(\"plot_interval_coverage\", p))\n\n  # make sure that plot_interval_coverage() doesn't drop column names\n  expect_true(all(c(\n    \"interval_coverage\", \"interval_coverage_deviation\",\n    \"quantile_coverage\", \"quantile_coverage_deviation\"\n  ) %in%\n    names(coverage)))\n})\n\n\n# ==============================================================================\n# plot_quantile_coverage() # nolint: commented_code_linter\n# ==============================================================================\ntest_that(\"plot_quantile_coverage() works as expected\", {\n  coverage <- example_quantile |>\n    na.omit() |>\n    as_forecast_quantile() |>\n    get_coverage(by = c(\"model\", \"quantile_level\"))\n\n  p <- plot_quantile_coverage(coverage)\n  expect_s3_class(p, \"ggplot\")\n  skip_on_cran()\n  suppressWarnings(vdiffr::expect_doppelganger(\"plot_quantile_coverage\", p))\n})\n","numLines":93,"startLine":1,"totalLines":93}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01FLBqbCWMQLAspX15DUv9J1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018nzXG7ZhjiYhRMxwQJBCMp","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R","offset":670,"limit":40}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":321,"cache_read_input_tokens":53593,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":321},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"48b03eb8-2d2b-4c93-a63c-dea87ae72244"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018nzXG7ZhjiYhRMxwQJBCMp","type":"tool_result","content":"   670→})\n   671→\n   672→\n   673→# ============================================================================ #\n   674→# `interval_coverage` =============================================== #\n   675→# ============================================================================ #\n   676→test_that(\"interval_coverage works\", {\n   677→  expect_equal( # nolint: expect_identical_linter\n   678→    interval_coverage(observed, predicted, quantile_level, interval_range = 50),\n   679→    c(TRUE, FALSE, FALSE)\n   680→  )\n   681→})\n   682→\n   683→test_that(\"interval_coverage rejects wrong inputs\", {\n   684→  expect_error(\n   685→    interval_coverage(observed, predicted, quantile_level, interval_range = c(50, 0)),\n   686→    \"Assertion on 'interval_range' failed: Must have length 1.\"\n   687→  )\n   688→})\n   689→\n   690→test_that(\"interval_coverage_quantile throws a warning when a required quantile is not available\", {\n   691→  dropped_quantile_pred <- predicted[, -4]\n   692→  dropped_quantiles <- quantile_level[-4]\n   693→  expect_error(\n   694→    interval_coverage(\n   695→      observed, dropped_quantile_pred, dropped_quantiles,\n   696→      interval_range = 50\n   697→    ),\n   698→    paste(\n   699→      \"To compute the interval coverage for an interval range of \\\"50%\\\",\",\n   700→      \"the 0.25 and 0.75 quantiles are required\"\n   701→    )\n   702→  )\n   703→})\n   704→\n   705→\n   706→# ============================================================================ #\n   707→# `bias_quantile` ============================================================ #\n   708→# ============================================================================ #\n   709→test_that(\"bias_quantile() works as expected\", {"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"7da3f66b-e43f-4ce3-b364-0f74af9bb874","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-metrics-quantile.R","content":"})\n\n\n# ============================================================================ #\n# `interval_coverage` =============================================== #\n# ============================================================================ #\ntest_that(\"interval_coverage works\", {\n  expect_equal( # nolint: expect_identical_linter\n    interval_coverage(observed, predicted, quantile_level, interval_range = 50),\n    c(TRUE, FALSE, FALSE)\n  )\n})\n\ntest_that(\"interval_coverage rejects wrong inputs\", {\n  expect_error(\n    interval_coverage(observed, predicted, quantile_level, interval_range = c(50, 0)),\n    \"Assertion on 'interval_range' failed: Must have length 1.\"\n  )\n})\n\ntest_that(\"interval_coverage_quantile throws a warning when a required quantile is not available\", {\n  dropped_quantile_pred <- predicted[, -4]\n  dropped_quantiles <- quantile_level[-4]\n  expect_error(\n    interval_coverage(\n      observed, dropped_quantile_pred, dropped_quantiles,\n      interval_range = 50\n    ),\n    paste(\n      \"To compute the interval coverage for an interval range of \\\"50%\\\",\",\n      \"the 0.25 and 0.75 quantiles are required\"\n    )\n  )\n})\n\n\n# ============================================================================ #\n# `bias_quantile` ============================================================ #\n# ============================================================================ #\ntest_that(\"bias_quantile() works as expected\", {","numLines":40,"startLine":670,"totalLines":978}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01DyDEedxdmYXvpF2e8U6Q3C","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have all the context I need. Let me design the test cases. The key insight is that this issue is about **refactoring** — extracting the shared bounds-check and quantile-to-interval pattern into a helper. The tests need to ensure that after the refactoring:\n\n1. Both `get_coverage()` and `interval_coverage()` produce identical results to before\n2. The shared helper (if created) works correctly\n3. Edge cases still work\n\nLet me also quickly check the `quantile_to_interval` helper to understand the full chain."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5946,"cache_read_input_tokens":53914,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5946},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"2011f6aa-5e62-4f7b-b39e-e0431824e4b1"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01DyDEedxdmYXvpF2e8U6Q3C","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014qN43XxSaPKsUHvR9bNwTW","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5946,"cache_read_input_tokens":53914,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5946},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"4016e600-51b9-44e6-92ed-644a318fa916"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014qN43XxSaPKsUHvR9bNwTW","type":"tool_result","content":"     1→# ==================== Functions internally used for scoring ===================\n     2→# These functions would ideally be replaced in the future\n     3→\n     4→#' Transform from a quantile format to an interval format\n     5→#' @description\n     6→#' Internal helper function to transform from a quantile format to an interval\n     7→#' format (which is no longer a supported forecast format, but still used\n     8→#' internally. The function mimics an S3 generic, but is not actually an S3\n     9→#' generic, as we want the functions to be internal and not exported.)\n    10→#'\n    11→#' **Quantile format**\n    12→#' In a quantile format, a prediction is characterised by one or multiple\n    13→#' predicted values and the corresponding quantile levels. For example, a\n    14→#' prediction in a quantile format could be represented by the 0.05, 0.25, 0.5,\n    15→#' 0.75 and 0.95 quantiles of the predictive distribution.\n    16→#'\n    17→#' **Interval format**\n    18→#' In the interval format, two quantiles are assumed to form a prediction\n    19→#' interval. Prediction intervals need to be symmetric around the median and\n    20→#' are characterised by a lower and an upper bound. The lower bound is defined\n    21→#' by the lower quantile and the upper bound is defined by the upper quantile.\n    22→#' A 90% prediction interval, for example, covers 90% of the probability mass\n    23→#' and is defined by the 5% and 95% quantiles. A forecast could therefore\n    24→#' be characterised by one or multiple prediction intervals, e.g. the lower\n    25→#' and upper bounds of the 50% and 90% prediction intervals (corresponding to\n    26→#' the 0.25 and 0.75 as well as the 0.05 and 0.095 quantiles).\n    27→#' @param ... Arguments\n    28→#' @returns A data.table with forecasts in an interval format.\n    29→#' @keywords internal\n    30→#' @importFrom cli cli_abort\n    31→quantile_to_interval <- function(...) {\n    32→  dots <- list(...)\n    33→  if (is.data.frame(dots[[1]])) {\n    34→    do.call(quantile_to_interval_dataframe, dots)\n    35→  } else if (is.numeric(dots[[1]])) {\n    36→    do.call(quantile_to_interval_numeric, dots)\n    37→  } else {\n    38→    cli_abort(\"Input must be either a data.frame or a numeric vector.\")\n    39→  }\n    40→}\n    41→\n    42→\n    43→#' @param forecast A data.table with forecasts in a quantile-based format (see\n    44→#'   [as_forecast_quantile()]).\n    45→#' @param format The format of the output. Either \"long\" or \"wide\". If \"long\"\n    46→#'   (the default), there will be a column `boundary` (with values either\n    47→#'   \"upper\" or \"lower\" and a column `interval_range` that contains the range of\n    48→#'   the interval. If \"wide\", there will be a column `interval_range` and two\n    49→#'   columns `lower` and `upper` that contain the lower and upper bounds of the\n    50→#'   prediction interval, respectively.\n    51→#' @param keep_quantile_col keep the `quantile_level` column in the final\n    52→#'   output after transformation (default is FALSE). This only works if\n    53→#'   `format = \"long\"`. If `format = \"wide\"`, the `quantile_level` column will\n    54→#'   always be dropped.\n    55→#' @return\n    56→#' *quantile_to_interval_dataframe*:\n    57→#'   a data.table in an interval format (either \"long\" or \"wide\"), with or\n    58→#'   without a `quantile_level` column. Rows will not be reordered.\n    59→#' @importFrom data.table copy\n    60→#' @rdname quantile_to_interval\n    61→#' @keywords internal\n    62→quantile_to_interval_dataframe <- function(forecast,\n    63→                                           format = \"long\",\n    64→                                           keep_quantile_col = FALSE,\n    65→                                           ...) {\n    66→  # After this transformation, the object will no longer be a valid forecast\n    67→  # object so we unclass it\n    68→  forecast <- as.data.table(forecast)\n    69→\n    70→  forecast[, boundary := ifelse(quantile_level <= 0.5, \"lower\", \"upper\")]\n    71→  forecast[, interval_range := get_range_from_quantile(quantile_level)]\n    72→\n    73→  # add median quantile\n    74→  median <- forecast[quantile_level == 0.5, ]\n    75→  median[, boundary := \"upper\"]\n    76→\n    77→  forecast <- data.table::rbindlist(list(forecast, median))\n    78→  if (!keep_quantile_col) {\n    79→    forecast[, quantile_level := NULL]\n    80→  }\n    81→\n    82→  if (length(unique(forecast$boundary)) < 2) {\n    83→    cli_abort(\n    84→      c(\n    85→        #nolint start: keyword_quote_linter\n    86→        `!` = \"No valid forecast intervals found.\",\n    87→        `i` = \"A forecast interval comprises two\n    88→      quantiles with quantile levels symmetric around the median\n    89→      (e.g. 0.25 and 0.75)\"\n    90→        #nolint end\n    91→      )\n    92→    )\n    93→  }\n    94→\n    95→  if (format == \"wide\") {\n    96→    suppressWarnings(forecast[, \"quantile_level\" := NULL])\n    97→    forecast <- dcast(forecast, ... ~ boundary, value.var = \"predicted\")\n    98→    # if there are NA values in `predicted`, this introduces a column \"NA\"\n    99→    if (\"NA\" %in% colnames(forecast) && all(is.na(forecast[[\"NA\"]]))) {\n   100→      forecast[, \"NA\" := NULL]\n   101→    }\n   102→  }\n   103→  return(forecast[])\n   104→}\n   105→\n   106→\n   107→#' @inheritParams wis\n   108→#' @return\n   109→#' *quantile_to_interval.numeric*:\n   110→#' a data.table in a wide interval format with columns `forecast_id`,\n   111→#' `observed`, `lower`, `upper`, and `interval_range`. The `forecast_id` column\n   112→#' is a unique identifier for each forecast. Rows will be reordered according to\n   113→#' `forecast_id` and `interval_range`.\n   114→#' @rdname quantile_to_interval\n   115→#' @keywords internal\n   116→quantile_to_interval_numeric <- function(observed,\n   117→                                         predicted,\n   118→                                         quantile_level,\n   119→                                         ...) {\n   120→  assert_input_quantile(observed, predicted, quantile_level)\n   121→\n   122→  n <- length(observed)\n   123→  N <- length(quantile_level)\n   124→\n   125→  dt <- data.table(\n   126→    forecast_id = rep(1:n, each = N),\n   127→    observed = rep(observed, each = N),\n   128→    predicted = as.vector(t(predicted)),\n   129→    quantile_level = quantile_level\n   130→  )\n   131→  out <- quantile_to_interval(dt, format = \"wide\")\n   132→  out <- out[order(forecast_id, interval_range)]\n   133→  return(out)\n   134→}\n   135→\n   136→\n   137→#' @title Change data from a sample-based format to a long interval range format\n   138→#'\n   139→#' @description\n   140→#'\n   141→#' Transform data from a format that is based on predictive samples to a format\n   142→#' based on interval ranges.\n   143→#'\n   144→#' @inheritParams as_forecast_quantile\n   145→#' @param keep_quantile_col keep quantile_level column, default is TRUE\n   146→#' @returns A data.table in a long interval interval range format\n   147→#' @importFrom data.table as.data.table\n   148→#' @importFrom stats quantile\n   149→#' @keywords internal\n   150→\n   151→sample_to_interval_long <- function(data,\n   152→                                    interval_range = c(0, 50, 90),\n   153→                                    type = 7,\n   154→                                    keep_quantile_col = TRUE) {\n   155→  data <- ensure_data.table(data)\n   156→\n   157→  lower_quantiles <- (100 - interval_range) / 200\n   158→  upper_quantiles <- 1 - lower_quantiles\n   159→  quantile_levels <- sort(unique(c(lower_quantiles, upper_quantiles)))\n   160→\n   161→  data <- as_forecast_quantile(\n   162→    data,\n   163→    probs = quantile_levels,\n   164→    type = type\n   165→  )\n   166→\n   167→  data <- quantile_to_interval(data, keep_quantile_col = keep_quantile_col)\n   168→\n   169→  return(data[])\n   170→}\n   171→\n   172→#' Get interval range belonging to a quantile\n   173→#' @description\n   174→#' Every quantile can be thought of either as the lower or the\n   175→#' upper bound of a symmetric central prediction interval. This helper function\n   176→#' returns the range of the central prediction interval to which the quantile\n   177→#' belongs.\n   178→#'\n   179→#' Due to numeric instability that sometimes occurred in the past, ranges are\n   180→#' rounded to 10 decimal places. This is not a problem for the vast majority of\n   181→#' use cases, but it is something to be aware of.\n   182→#' @param quantile_level A numeric vector of quantile levels of size N.\n   183→#' @returns a numeric vector of interval ranges of size N\n   184→#' @keywords internal\n   185→get_range_from_quantile <- function(quantile_level) {\n   186→  boundary <- ifelse(quantile_level <= 0.5, \"lower\", \"upper\")\n   187→  interval_range <- ifelse(\n   188→    boundary == \"lower\",\n   189→    round((1 - 2 * quantile_level) * 100, digits = 10),\n   190→    round((2 * quantile_level - 1) * 100, digits = 10)\n   191→  )\n   192→  return(interval_range)\n   193→}\n   194→"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"73cadfa3-ec74-4ae6-bdba-297d0f26f815","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/R/helper-quantile-interval-range.R","content":"# ==================== Functions internally used for scoring ===================\n# These functions would ideally be replaced in the future\n\n#' Transform from a quantile format to an interval format\n#' @description\n#' Internal helper function to transform from a quantile format to an interval\n#' format (which is no longer a supported forecast format, but still used\n#' internally. The function mimics an S3 generic, but is not actually an S3\n#' generic, as we want the functions to be internal and not exported.)\n#'\n#' **Quantile format**\n#' In a quantile format, a prediction is characterised by one or multiple\n#' predicted values and the corresponding quantile levels. For example, a\n#' prediction in a quantile format could be represented by the 0.05, 0.25, 0.5,\n#' 0.75 and 0.95 quantiles of the predictive distribution.\n#'\n#' **Interval format**\n#' In the interval format, two quantiles are assumed to form a prediction\n#' interval. Prediction intervals need to be symmetric around the median and\n#' are characterised by a lower and an upper bound. The lower bound is defined\n#' by the lower quantile and the upper bound is defined by the upper quantile.\n#' A 90% prediction interval, for example, covers 90% of the probability mass\n#' and is defined by the 5% and 95% quantiles. A forecast could therefore\n#' be characterised by one or multiple prediction intervals, e.g. the lower\n#' and upper bounds of the 50% and 90% prediction intervals (corresponding to\n#' the 0.25 and 0.75 as well as the 0.05 and 0.095 quantiles).\n#' @param ... Arguments\n#' @returns A data.table with forecasts in an interval format.\n#' @keywords internal\n#' @importFrom cli cli_abort\nquantile_to_interval <- function(...) {\n  dots <- list(...)\n  if (is.data.frame(dots[[1]])) {\n    do.call(quantile_to_interval_dataframe, dots)\n  } else if (is.numeric(dots[[1]])) {\n    do.call(quantile_to_interval_numeric, dots)\n  } else {\n    cli_abort(\"Input must be either a data.frame or a numeric vector.\")\n  }\n}\n\n\n#' @param forecast A data.table with forecasts in a quantile-based format (see\n#'   [as_forecast_quantile()]).\n#' @param format The format of the output. Either \"long\" or \"wide\". If \"long\"\n#'   (the default), there will be a column `boundary` (with values either\n#'   \"upper\" or \"lower\" and a column `interval_range` that contains the range of\n#'   the interval. If \"wide\", there will be a column `interval_range` and two\n#'   columns `lower` and `upper` that contain the lower and upper bounds of the\n#'   prediction interval, respectively.\n#' @param keep_quantile_col keep the `quantile_level` column in the final\n#'   output after transformation (default is FALSE). This only works if\n#'   `format = \"long\"`. If `format = \"wide\"`, the `quantile_level` column will\n#'   always be dropped.\n#' @return\n#' *quantile_to_interval_dataframe*:\n#'   a data.table in an interval format (either \"long\" or \"wide\"), with or\n#'   without a `quantile_level` column. Rows will not be reordered.\n#' @importFrom data.table copy\n#' @rdname quantile_to_interval\n#' @keywords internal\nquantile_to_interval_dataframe <- function(forecast,\n                                           format = \"long\",\n                                           keep_quantile_col = FALSE,\n                                           ...) {\n  # After this transformation, the object will no longer be a valid forecast\n  # object so we unclass it\n  forecast <- as.data.table(forecast)\n\n  forecast[, boundary := ifelse(quantile_level <= 0.5, \"lower\", \"upper\")]\n  forecast[, interval_range := get_range_from_quantile(quantile_level)]\n\n  # add median quantile\n  median <- forecast[quantile_level == 0.5, ]\n  median[, boundary := \"upper\"]\n\n  forecast <- data.table::rbindlist(list(forecast, median))\n  if (!keep_quantile_col) {\n    forecast[, quantile_level := NULL]\n  }\n\n  if (length(unique(forecast$boundary)) < 2) {\n    cli_abort(\n      c(\n        #nolint start: keyword_quote_linter\n        `!` = \"No valid forecast intervals found.\",\n        `i` = \"A forecast interval comprises two\n      quantiles with quantile levels symmetric around the median\n      (e.g. 0.25 and 0.75)\"\n        #nolint end\n      )\n    )\n  }\n\n  if (format == \"wide\") {\n    suppressWarnings(forecast[, \"quantile_level\" := NULL])\n    forecast <- dcast(forecast, ... ~ boundary, value.var = \"predicted\")\n    # if there are NA values in `predicted`, this introduces a column \"NA\"\n    if (\"NA\" %in% colnames(forecast) && all(is.na(forecast[[\"NA\"]]))) {\n      forecast[, \"NA\" := NULL]\n    }\n  }\n  return(forecast[])\n}\n\n\n#' @inheritParams wis\n#' @return\n#' *quantile_to_interval.numeric*:\n#' a data.table in a wide interval format with columns `forecast_id`,\n#' `observed`, `lower`, `upper`, and `interval_range`. The `forecast_id` column\n#' is a unique identifier for each forecast. Rows will be reordered according to\n#' `forecast_id` and `interval_range`.\n#' @rdname quantile_to_interval\n#' @keywords internal\nquantile_to_interval_numeric <- function(observed,\n                                         predicted,\n                                         quantile_level,\n                                         ...) {\n  assert_input_quantile(observed, predicted, quantile_level)\n\n  n <- length(observed)\n  N <- length(quantile_level)\n\n  dt <- data.table(\n    forecast_id = rep(1:n, each = N),\n    observed = rep(observed, each = N),\n    predicted = as.vector(t(predicted)),\n    quantile_level = quantile_level\n  )\n  out <- quantile_to_interval(dt, format = \"wide\")\n  out <- out[order(forecast_id, interval_range)]\n  return(out)\n}\n\n\n#' @title Change data from a sample-based format to a long interval range format\n#'\n#' @description\n#'\n#' Transform data from a format that is based on predictive samples to a format\n#' based on interval ranges.\n#'\n#' @inheritParams as_forecast_quantile\n#' @param keep_quantile_col keep quantile_level column, default is TRUE\n#' @returns A data.table in a long interval interval range format\n#' @importFrom data.table as.data.table\n#' @importFrom stats quantile\n#' @keywords internal\n\nsample_to_interval_long <- function(data,\n                                    interval_range = c(0, 50, 90),\n                                    type = 7,\n                                    keep_quantile_col = TRUE) {\n  data <- ensure_data.table(data)\n\n  lower_quantiles <- (100 - interval_range) / 200\n  upper_quantiles <- 1 - lower_quantiles\n  quantile_levels <- sort(unique(c(lower_quantiles, upper_quantiles)))\n\n  data <- as_forecast_quantile(\n    data,\n    probs = quantile_levels,\n    type = type\n  )\n\n  data <- quantile_to_interval(data, keep_quantile_col = keep_quantile_col)\n\n  return(data[])\n}\n\n#' Get interval range belonging to a quantile\n#' @description\n#' Every quantile can be thought of either as the lower or the\n#' upper bound of a symmetric central prediction interval. This helper function\n#' returns the range of the central prediction interval to which the quantile\n#' belongs.\n#'\n#' Due to numeric instability that sometimes occurred in the past, ranges are\n#' rounded to 10 decimal places. This is not a problem for the vast majority of\n#' use cases, but it is something to be aware of.\n#' @param quantile_level A numeric vector of quantile levels of size N.\n#' @returns a numeric vector of interval ranges of size N\n#' @keywords internal\nget_range_from_quantile <- function(quantile_level) {\n  boundary <- ifelse(quantile_level <= 0.5, \"lower\", \"upper\")\n  interval_range <- ifelse(\n    boundary == \"lower\",\n    round((1 - 2 * quantile_level) * 100, digits = 10),\n    round((2 * quantile_level - 1) * 100, digits = 10)\n  )\n  return(interval_range)\n}\n","numLines":194,"startLine":1,"totalLines":194}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01ByJrJXLUKEVe7MRLMunfG7","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have complete understanding. Let me also check if the issue file already exists."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3064,"cache_read_input_tokens":59860,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3064},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"e19ee24f-977a-413d-8871-043f8585d25e"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01ByJrJXLUKEVe7MRLMunfG7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016TaiQUwA2nSiaY9eNUPQft","name":"Glob","input":{"pattern":".ralph/issues/454*.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3064,"cache_read_input_tokens":59860,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3064},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"8c03c96c-725d-489d-8baa-e25ce70939e3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016TaiQUwA2nSiaY9eNUPQft","type":"tool_result","content":"No files found"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"eec2f67f-2bd5-405b-99fb-136cc9c544d5","tool_use_result":{"filenames":[],"durationMs":317,"numFiles":0,"truncated":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01W9CbCabPWz62Ukk6WJEjwe","type":"message","role":"assistant","content":[{"type":"text","text":"Good, no existing file. Now I can design the tests and write the spec. Let me also check the test file for `quantile_to_interval` for completeness."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":92,"cache_read_input_tokens":62924,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":92},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"a858009d-bd5e-4067-8715-209a1ddaf3a2"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01W9CbCabPWz62Ukk6WJEjwe","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01JZb1yr8HmSJR96yq6CgXNJ","name":"Grep","input":{"pattern":"quantile_to_interval","path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat","output_mode":"files_with_matches"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":92,"cache_read_input_tokens":62924,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":92},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"49364bac-ba00-4f4f-990e-c6da21f28726"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01JZb1yr8HmSJR96yq6CgXNJ","type":"tool_result","content":"Found 1 file\ntests/testthat/test-helper-quantile-interval-range.R"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"28e7dcac-8c85-40dd-a995-21924fa7405b","tool_use_result":{"mode":"files_with_matches","filenames":["tests/testthat/test-helper-quantile-interval-range.R"],"numFiles":1}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_018xwYX5D8bXpTKGfSraB2pT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WdA4cxgDQgv1wVx5XVt1Yp","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-helper-quantile-interval-range.R"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":389,"cache_read_input_tokens":63016,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":389},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"34f7ce18-0b0a-4270-bb27-20574e8b5e00"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WdA4cxgDQgv1wVx5XVt1Yp","type":"tool_result","content":"     1→# ==============================================================================\n     2→# quantile_to_interval() # nolint: commented_code_linter\n     3→# ==============================================================================\n     4→\n     5→test_that(\"quantile_to_interval_dataframe() works\", {\n     6→  quantile <- data.frame(\n     7→    date = as.Date(\"2020-01-01\") + 1:10,\n     8→    model = \"model1\",\n     9→    observed = 1:10,\n    10→    predicted = c(2:11, 4:13),\n    11→    quantile_level = rep(c(0.25, 0.75), each = 10),\n    12→    stringsAsFactors = FALSE\n    13→  )\n    14→  long <- data.frame(\n    15→    date = as.Date(\"2020-01-01\") + 1:10,\n    16→    model = \"model1\",\n    17→    observed = 1:10,\n    18→    predicted = c(2:11, 4:13),\n    19→    interval_range = 50,\n    20→    boundary = rep(c(\"lower\", \"upper\"), each = 10),\n    21→    stringsAsFactors = FALSE\n    22→  )\n    23→  long2 <- as.data.frame(quantile_to_interval(\n    24→    quantile,\n    25→    keep_quantile_col = FALSE\n    26→  ))\n    27→  data.table::setcolorder(long2, names(long))\n    28→  # for some reason this is needed to pass the unit tests on gh actions\n    29→  long2$boundary <- as.character(long2$boundary)\n    30→  long$boundary <- as.character(long$boundary)\n    31→  expect_identical(long, as.data.frame(long2))\n    32→\n    33→  # check that it handles NA values\n    34→  setDT(quantile)\n    35→  quantile[c(1, 3, 11, 13), c(\"observed\", \"predicted\", \"quantile_level\") := NA]\n    36→  # in this instance, a problem appears because there is an NA value both\n    37→  # for the upper and lower bound.\n    38→\n    39→  # the data.table behavior differs before/after v1.16.0\n    40→  #  - before, it's a 'message'\n    41→  #  - after, it's a 'warning'\n    42→  #  - the conditionMessage() also differs\n    43→  expected_condition <- tryCatch(\n    44→    dcast(data.table(a = c(1, 1), b = 2, c = 3), a ~ b, value.var = \"c\"),\n    45→    condition = identity\n    46→  )\n    47→  expect_condition(\n    48→    quantile_to_interval(\n    49→      quantile,\n    50→      keep_quantile_col = FALSE,\n    51→      format = \"wide\"\n    52→    ),\n    53→    class = class(expected_condition)[1L], # testthat 3e requires exactly one class\n    54→    regexp = \"[Aa]ggregate.*default.*length\"\n    55→  )\n    56→  quantile <- quantile[-c(1, 3), ]\n    57→  wide2 <- scoringutils:::quantile_to_interval( # nolint: undesirable_operator_linter\n    58→    quantile,\n    59→    keep_quantile_col = FALSE,\n    60→    format = \"wide\"\n    61→  )\n    62→  expect_identical(nrow(wide2), 10L)\n    63→  expect_false((\"NA\") %in% colnames(wide2))\n    64→  expect_equal(sum(wide2$lower, na.rm = TRUE), 59) # nolint: expect_identical_linter\n    65→})\n    66→\n    67→\n    68→test_that(\"sample_to_range_long works\", {\n    69→  samples <- data.frame(\n    70→    date = as.Date(\"2020-01-01\") + 1:10,\n    71→    model = \"model1\",\n    72→    observed = 1:10,\n    73→    predicted = c(rep(0, 10), 2:11, 3:12, 4:13, rep(100, 10)),\n    74→    sample_id = rep(1:5, each = 10),\n    75→    stringsAsFactors = FALSE\n    76→  )\n    77→\n    78→  long <- data.frame(\n    79→    date = as.Date(\"2020-01-01\") + 1:10,\n    80→    model = \"model1\",\n    81→    observed = 1:10,\n    82→    predicted = c(2:11, 4:13),\n    83→    interval_range = 50,\n    84→    boundary = rep(c(\"lower\", \"upper\"), each = 10),\n    85→    stringsAsFactors = FALSE\n    86→  )\n    87→\n    88→  long2 <- scoringutils:::sample_to_interval_long(as_forecast_sample(samples), # nolint: undesirable_operator_linter\n    89→    interval_range = 50,\n    90→    keep_quantile_col = FALSE\n    91→  )\n    92→  long2 <- long2[order(model, boundary, date)]\n    93→  data.table::setcolorder(long2, names(long))\n    94→\n    95→  # for some reason this is needed to pass the unit tests on gh actions\n    96→  long2$boundary <- as.character(long2$boundary)\n    97→  long$boundary <- as.character(long$boundary)\n    98→\n    99→  expect_equal(long, as.data.frame(long2)) # nolint: expect_identical_linter\n   100→})\n   101→\n   102→test_that(\"quantile_to_range works - scalar and vector case\", {\n   103→  predicted <- 9:1\n   104→  quantile <- rev(seq(0.1, 0.9, 0.1))\n   105→  observed <- 5\n   106→\n   107→  # check output is produced\n   108→  out1 <- quantile_to_interval(observed, predicted, quantile)\n   109→  expect_snapshot(out1)\n   110→\n   111→  # check order of predictions doesn't matter\n   112→  predicted <- 1:9\n   113→  quantile <- seq(0.1, 0.9, 0.1)\n   114→  out2 <- quantile_to_interval(observed, predicted, quantile)\n   115→  expect_identical(out1, out2)\n   116→\n   117→  # check error if observed is a vector and predicted is a vector as well\n   118→  expect_error(\n   119→    quantile_to_interval(\n   120→      observed = c(1, 2), predicted = c(1, 2), quantile_level = c(0.1, 0.9)\n   121→    ),\n   122→    \"Assertion on 'predicted' failed: Must be of type 'matrix', not 'double'.\"\n   123→  )\n   124→\n   125→  # check NA values are handled gracefully - there should just be NA in the output\n   126→  predicted <- c(1:8, NA)\n   127→  quantile <- seq(0.1, 0.9, 0.1)\n   128→  observed <- 5\n   129→  out3 <- quantile_to_interval(observed, predicted, quantile)\n   130→  expect_snapshot(out3)\n   131→\n   132→  # check non-symmetrical intervals are handled gracefully\n   133→  # result should be newly introduced ranges where one value is NA\n   134→  predicted <- 1:9\n   135→  quantile <- c(seq(0.1, 0.8, 0.1), 0.95)\n   136→  observed <- 5\n   137→  out4 <- quantile_to_interval(observed, predicted, quantile)\n   138→  expect_snapshot(out4)\n   139→\n   140→  # check function works without a median\n   141→  predicted <- 1:8\n   142→  quantile <- c(seq(0.1, 0.4, 0.1), seq(0.6, 0.9, 0.1))\n   143→  observed <- 5\n   144→  expect_no_condition(\n   145→    quantile_to_interval(observed, predicted, quantile)\n   146→  )\n   147→\n   148→  # check a one-dimensional matrix works fine\n   149→  predicted <- matrix(1:9, nrow = 1)\n   150→  quantile <- seq(0.1, 0.9, 0.1)\n   151→  expect_no_condition(\n   152→    quantile_to_interval(observed, predicted, quantile)\n   153→  )\n   154→})\n   155→\n   156→test_that(\"quantile_to_range works - matrix case\", {\n   157→  n <- 5\n   158→  N <- 9\n   159→  predicted <- matrix(1:45, nrow = n, ncol = N)\n   160→  quantile <- seq(0.1, 0.9, 0.1)\n   161→  observed <- seq(21, 25, 1)\n   162→\n   163→  # check output is produced\n   164→  out1 <- quantile_to_interval(observed, predicted, quantile)\n   165→  expect_snapshot(out1)\n   166→\n   167→  # check order of predictions doesn't matter\n   168→  predicted <- matrix(\n   169→    c(41:45, 36:40, 31:35, 26:30, 21:25, 16:20, 11:15, 6:10, 1:5),\n   170→    nrow = n,\n   171→    ncol = N\n   172→  )\n   173→  quantile <- rev(seq(0.1, 0.9, 0.1))\n   174→  out2 <- quantile_to_interval(observed, predicted, quantile)\n   175→  expect_identical(out1, out2)\n   176→\n   177→  # check NA values are fine\n   178→  predicted[1, 1] <- NA\n   179→  expect_no_condition(\n   180→    quantile_to_interval(observed, predicted, quantile)\n   181→  )\n   182→})\n   183→\n   184→\n   185→test_that(\"quantile_to_interval works - data.frame case\", {\n   186→  dt <- data.table(\n   187→    observed = 5,\n   188→    predicted = 1:9,\n   189→    quantile_level = seq(0.1, 0.9, 0.1)\n   190→  )\n   191→\n   192→  expect_no_condition(\n   193→    quantile_to_interval(dt)\n   194→  )\n   195→\n   196→  expect_no_condition(\n   197→    quantile_to_interval(dt, format = \"wide\")\n   198→  )\n   199→\n   200→  # check that the number of rows after transformation is equal to the number\n   201→  # of rows plus the number of medians added (as upper boundary of a 0%\n   202→  # prediction interval)\n   203→  ex <- example_quantile[!is.na(predicted)]\n   204→  n_preds <- nrow(ex)\n   205→  n_medians <- nrow(ex[quantile_level == 0.5])\n   206→  ex_interval <- quantile_to_interval(ex, keep_quantile_col = TRUE)\n   207→  expect_identical(\n   208→    nrow(ex_interval),\n   209→    n_preds + n_medians\n   210→  )\n   211→\n   212→  expect_identical(\n   213→    colnames(ex_interval),\n   214→    c(colnames(ex), \"boundary\", \"interval_range\")\n   215→  )\n   216→\n   217→  expect_error(\n   218→    quantile_to_interval(x = \"not working\"),\n   219→    \"Input must be either a data.frame or a numeric vector.\"\n   220→  )\n   221→})\n   222→"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"c537222c-c395-4dee-8024-217630ab43eb","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/tests/testthat/test-helper-quantile-interval-range.R","content":"# ==============================================================================\n# quantile_to_interval() # nolint: commented_code_linter\n# ==============================================================================\n\ntest_that(\"quantile_to_interval_dataframe() works\", {\n  quantile <- data.frame(\n    date = as.Date(\"2020-01-01\") + 1:10,\n    model = \"model1\",\n    observed = 1:10,\n    predicted = c(2:11, 4:13),\n    quantile_level = rep(c(0.25, 0.75), each = 10),\n    stringsAsFactors = FALSE\n  )\n  long <- data.frame(\n    date = as.Date(\"2020-01-01\") + 1:10,\n    model = \"model1\",\n    observed = 1:10,\n    predicted = c(2:11, 4:13),\n    interval_range = 50,\n    boundary = rep(c(\"lower\", \"upper\"), each = 10),\n    stringsAsFactors = FALSE\n  )\n  long2 <- as.data.frame(quantile_to_interval(\n    quantile,\n    keep_quantile_col = FALSE\n  ))\n  data.table::setcolorder(long2, names(long))\n  # for some reason this is needed to pass the unit tests on gh actions\n  long2$boundary <- as.character(long2$boundary)\n  long$boundary <- as.character(long$boundary)\n  expect_identical(long, as.data.frame(long2))\n\n  # check that it handles NA values\n  setDT(quantile)\n  quantile[c(1, 3, 11, 13), c(\"observed\", \"predicted\", \"quantile_level\") := NA]\n  # in this instance, a problem appears because there is an NA value both\n  # for the upper and lower bound.\n\n  # the data.table behavior differs before/after v1.16.0\n  #  - before, it's a 'message'\n  #  - after, it's a 'warning'\n  #  - the conditionMessage() also differs\n  expected_condition <- tryCatch(\n    dcast(data.table(a = c(1, 1), b = 2, c = 3), a ~ b, value.var = \"c\"),\n    condition = identity\n  )\n  expect_condition(\n    quantile_to_interval(\n      quantile,\n      keep_quantile_col = FALSE,\n      format = \"wide\"\n    ),\n    class = class(expected_condition)[1L], # testthat 3e requires exactly one class\n    regexp = \"[Aa]ggregate.*default.*length\"\n  )\n  quantile <- quantile[-c(1, 3), ]\n  wide2 <- scoringutils:::quantile_to_interval( # nolint: undesirable_operator_linter\n    quantile,\n    keep_quantile_col = FALSE,\n    format = \"wide\"\n  )\n  expect_identical(nrow(wide2), 10L)\n  expect_false((\"NA\") %in% colnames(wide2))\n  expect_equal(sum(wide2$lower, na.rm = TRUE), 59) # nolint: expect_identical_linter\n})\n\n\ntest_that(\"sample_to_range_long works\", {\n  samples <- data.frame(\n    date = as.Date(\"2020-01-01\") + 1:10,\n    model = \"model1\",\n    observed = 1:10,\n    predicted = c(rep(0, 10), 2:11, 3:12, 4:13, rep(100, 10)),\n    sample_id = rep(1:5, each = 10),\n    stringsAsFactors = FALSE\n  )\n\n  long <- data.frame(\n    date = as.Date(\"2020-01-01\") + 1:10,\n    model = \"model1\",\n    observed = 1:10,\n    predicted = c(2:11, 4:13),\n    interval_range = 50,\n    boundary = rep(c(\"lower\", \"upper\"), each = 10),\n    stringsAsFactors = FALSE\n  )\n\n  long2 <- scoringutils:::sample_to_interval_long(as_forecast_sample(samples), # nolint: undesirable_operator_linter\n    interval_range = 50,\n    keep_quantile_col = FALSE\n  )\n  long2 <- long2[order(model, boundary, date)]\n  data.table::setcolorder(long2, names(long))\n\n  # for some reason this is needed to pass the unit tests on gh actions\n  long2$boundary <- as.character(long2$boundary)\n  long$boundary <- as.character(long$boundary)\n\n  expect_equal(long, as.data.frame(long2)) # nolint: expect_identical_linter\n})\n\ntest_that(\"quantile_to_range works - scalar and vector case\", {\n  predicted <- 9:1\n  quantile <- rev(seq(0.1, 0.9, 0.1))\n  observed <- 5\n\n  # check output is produced\n  out1 <- quantile_to_interval(observed, predicted, quantile)\n  expect_snapshot(out1)\n\n  # check order of predictions doesn't matter\n  predicted <- 1:9\n  quantile <- seq(0.1, 0.9, 0.1)\n  out2 <- quantile_to_interval(observed, predicted, quantile)\n  expect_identical(out1, out2)\n\n  # check error if observed is a vector and predicted is a vector as well\n  expect_error(\n    quantile_to_interval(\n      observed = c(1, 2), predicted = c(1, 2), quantile_level = c(0.1, 0.9)\n    ),\n    \"Assertion on 'predicted' failed: Must be of type 'matrix', not 'double'.\"\n  )\n\n  # check NA values are handled gracefully - there should just be NA in the output\n  predicted <- c(1:8, NA)\n  quantile <- seq(0.1, 0.9, 0.1)\n  observed <- 5\n  out3 <- quantile_to_interval(observed, predicted, quantile)\n  expect_snapshot(out3)\n\n  # check non-symmetrical intervals are handled gracefully\n  # result should be newly introduced ranges where one value is NA\n  predicted <- 1:9\n  quantile <- c(seq(0.1, 0.8, 0.1), 0.95)\n  observed <- 5\n  out4 <- quantile_to_interval(observed, predicted, quantile)\n  expect_snapshot(out4)\n\n  # check function works without a median\n  predicted <- 1:8\n  quantile <- c(seq(0.1, 0.4, 0.1), seq(0.6, 0.9, 0.1))\n  observed <- 5\n  expect_no_condition(\n    quantile_to_interval(observed, predicted, quantile)\n  )\n\n  # check a one-dimensional matrix works fine\n  predicted <- matrix(1:9, nrow = 1)\n  quantile <- seq(0.1, 0.9, 0.1)\n  expect_no_condition(\n    quantile_to_interval(observed, predicted, quantile)\n  )\n})\n\ntest_that(\"quantile_to_range works - matrix case\", {\n  n <- 5\n  N <- 9\n  predicted <- matrix(1:45, nrow = n, ncol = N)\n  quantile <- seq(0.1, 0.9, 0.1)\n  observed <- seq(21, 25, 1)\n\n  # check output is produced\n  out1 <- quantile_to_interval(observed, predicted, quantile)\n  expect_snapshot(out1)\n\n  # check order of predictions doesn't matter\n  predicted <- matrix(\n    c(41:45, 36:40, 31:35, 26:30, 21:25, 16:20, 11:15, 6:10, 1:5),\n    nrow = n,\n    ncol = N\n  )\n  quantile <- rev(seq(0.1, 0.9, 0.1))\n  out2 <- quantile_to_interval(observed, predicted, quantile)\n  expect_identical(out1, out2)\n\n  # check NA values are fine\n  predicted[1, 1] <- NA\n  expect_no_condition(\n    quantile_to_interval(observed, predicted, quantile)\n  )\n})\n\n\ntest_that(\"quantile_to_interval works - data.frame case\", {\n  dt <- data.table(\n    observed = 5,\n    predicted = 1:9,\n    quantile_level = seq(0.1, 0.9, 0.1)\n  )\n\n  expect_no_condition(\n    quantile_to_interval(dt)\n  )\n\n  expect_no_condition(\n    quantile_to_interval(dt, format = \"wide\")\n  )\n\n  # check that the number of rows after transformation is equal to the number\n  # of rows plus the number of medians added (as upper boundary of a 0%\n  # prediction interval)\n  ex <- example_quantile[!is.na(predicted)]\n  n_preds <- nrow(ex)\n  n_medians <- nrow(ex[quantile_level == 0.5])\n  ex_interval <- quantile_to_interval(ex, keep_quantile_col = TRUE)\n  expect_identical(\n    nrow(ex_interval),\n    n_preds + n_medians\n  )\n\n  expect_identical(\n    colnames(ex_interval),\n    c(colnames(ex), \"boundary\", \"interval_range\")\n  )\n\n  expect_error(\n    quantile_to_interval(x = \"not working\"),\n    \"Input must be either a data.frame or a numeric vector.\"\n  )\n})\n","numLines":222,"startLine":1,"totalLines":222}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01GMeRkwhQQ51Rono6Mb5gGS","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have a complete picture. Let me design the test specifications and write the issue file."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3232,"cache_read_input_tokens":63405,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3232},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"1134aa8e-b409-4c10-94f3-54396c6e5960"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01GMeRkwhQQ51Rono6Mb5gGS","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01JR29GZDwd9E2RWLSwemf5o","name":"TodoWrite","input":{"todos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"in_progress","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3232,"cache_read_input_tokens":63405,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3232},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"9ce53fa3-a00c-4f6b-be6b-f35c4273107b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01JR29GZDwd9E2RWLSwemf5o","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"67fdb12a-8270-422f-a3de-9744d1dd38ac","tool_use_result":{"oldTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"in_progress","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"pending","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}],"newTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"in_progress","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_011diLi1ZzCKhJ4USAHoTGc8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013T8uvZzMHp3REzc2MTs6vL","name":"Write","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/issues/454-deduplicate-coverage.md","content":"# Issue #454: Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage()`\n\n## Source\n\n- **GitHub**: https://github.com/epiforecasts/scoringutils/issues/454\n- **Action Summary**: Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern.\n- **Status Note**: Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic).\n\n## Test Specifications\n\n**Designed**: 2026-02-13\n**Test file**: `tests/testthat/test-get-coverage.R` (existing) and `tests/testthat/test-metrics-quantile.R` (existing)\n**Testing patterns observed**: testthat 3e style, data.table fixtures, `example_quantile` built-in dataset, `setup.R` pre-computes scored objects, numeric vector test data defined at top of `test-metrics-quantile.R` (`observed = c(1, -15, 22)`, `predicted` as 3x5 matrix, `quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9)`), `vdiffr` for visual regression, `expect_no_condition()` for clean execution, `get_range_from_quantile()` as helper.\n\n## Duplicated Code Analysis\n\nBoth functions share the same pattern:\n1. Call `quantile_to_interval()` to convert quantile format to interval format with `lower`/`upper` columns\n2. Perform bounds check: `(observed >= lower) & (observed <= upper)` to compute `interval_coverage`\n\n**`get_coverage()`** (R/get-coverage.R:69-71): calls `quantile_to_interval(forecast, format = \"wide\")` on a data.table, then `interval_coverage := (observed <= upper) & (observed >= lower)`\n\n**`interval_coverage()`** (R/metrics-quantile.R:361-363): calls `quantile_to_interval(observed, predicted, quantile_level)` on numeric vectors, then `interval_coverage := (observed >= lower) & (observed <= upper)`\n\nThe refactoring should extract the shared logic into a helper, or have `get_coverage()` delegate to `interval_coverage()` for the bounds-check step. Tests must verify that both functions produce identical results before and after refactoring.\n\n### Test 1: test_that(\"get_coverage() interval coverage matches interval_coverage() for same data\", { ... })\n\n**Reproduces**: the core duplication — both functions independently compute the same bounds check, and we need to verify they agree\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- Use `example_quantile` dataset, filtered to a single model (`\"EuroCOVIDhub-ensemble\"`) and a single target date to keep data small\n- Call `as_forecast_quantile()` on the filtered data\n- Extract the numeric vectors `observed`, `predicted` (as matrix), and `quantile_level` from the same filtered data\n**Action**:\n- Call `get_coverage(forecast, by = get_forecast_unit(forecast))` to get per-row interval coverage\n- For the same forecast rows, call `interval_coverage(observed, predicted, quantile_level, interval_range = 50)` (and repeat for interval_range = 80 if those quantiles exist)\n- Compare the interval_coverage column from `get_coverage()` against the boolean vector from `interval_coverage()` for matching interval ranges\n**Expected**:\n- The `interval_coverage` values from `get_coverage()` for a given `interval_range` must exactly equal the values returned by `interval_coverage()` for the same `interval_range` and same input data\n- Use `expect_equal()` to compare the matched values\n**Fails now because**:\n- This test should PASS now — both implementations compute the same thing. This is a regression guard that ensures the refactored version remains equivalent. If the refactoring introduces a bug (e.g., off-by-one in bounds, wrong filtering), this test will catch it.\n\n### Test 2: test_that(\"get_coverage() produces correct interval_coverage for known inputs\", { ... })\n\n**Reproduces**: verifies the bounds-check logic with hand-crafted data where the expected coverage values can be computed manually\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- Create a minimal `forecast_quantile` object with known values:\n  ```\n  observed = 5 for all rows\n  model = \"m1\", target = \"t1\", date = \"2020-01-01\", location = \"loc1\"\n  quantile_level = c(0.25, 0.5, 0.75)\n  predicted = c(3, 5, 7)   # observation IS inside the 50% interval [3, 7]\n  ```\n- Create a second forecast where `observed = 10` (OUTSIDE the interval [3, 7])\n- Combine into a single `forecast_quantile` object with 2 forecast units\n**Action**:\n- Call `get_coverage()` with `by = get_forecast_unit(forecast)` to get per-forecast coverage\n**Expected**:\n- For observed=5, predicted=[3,5,7]: `interval_coverage` for the 50% interval should be `TRUE` (5 >= 3 and 5 <= 7)\n- For observed=10, predicted=[3,5,7]: `interval_coverage` for the 50% interval should be `FALSE` (10 > 7)\n- `quantile_coverage` for quantile_level=0.5 should be `TRUE` for observed=5 (5 <= 5) and `FALSE` for observed=10 (10 > 5)\n- Use `expect_equal()` on specific rows\n**Fails now because**:\n- This test should PASS now. It is a regression guard to ensure the extracted/shared bounds-check logic computes the same values after refactoring.\n\n### Test 3: test_that(\"interval_coverage() produces correct results for boundary cases\", { ... })\n\n**Reproduces**: boundary cases in the bounds check (observation exactly on lower/upper bound)\n**File**: `tests/testthat/test-metrics-quantile.R`\n**Setup**:\n- Use numeric vectors:\n  ```\n  observed = c(3, 7, 5)   # exactly on lower, exactly on upper, inside\n  predicted = matrix(c(3, 5, 7), nrow = 3, ncol = 3, byrow = TRUE)  # each row: [3, 5, 7]\n  quantile_level = c(0.25, 0.5, 0.75)\n  ```\n**Action**:\n- Call `interval_coverage(observed, predicted, quantile_level, interval_range = 50)`\n**Expected**:\n- All three should be `TRUE`: observation on lower bound (3 >= 3), on upper bound (7 <= 7), and inside (5 >= 3 and 5 <= 7)\n- `expect_equal(result, c(TRUE, TRUE, TRUE))`\n**Fails now because**:\n- This test should PASS now. It guards against a refactoring that accidentally changes `>=`/`<=` to `>`/`<` in the bounds check.\n\n### Test 4: test_that(\"interval_coverage() handles multiple interval ranges correctly\", { ... })\n\n**Reproduces**: verifying that when the shared helper handles multiple interval ranges, results are consistent\n**File**: `tests/testthat/test-metrics-quantile.R`\n**Setup**:\n- Use numeric vectors with 5 quantile levels:\n  ```\n  observed = c(5)\n  predicted = matrix(c(1, 3, 5, 7, 9), nrow = 1)  # quantiles for [0.1, 0.25, 0.5, 0.75, 0.9]\n  quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9)\n  ```\n**Action**:\n- Call `interval_coverage(observed, predicted, quantile_level, interval_range = 50)` — tests the 50% interval [3, 7]\n- Call `interval_coverage(observed, predicted, quantile_level, interval_range = 80)` — tests the 80% interval [1, 9]\n**Expected**:\n- 50% interval: `TRUE` (5 is inside [3, 7])\n- 80% interval: `TRUE` (5 is inside [1, 9])\n- Use `expect_equal(result, TRUE)` for each\n**Fails now because**:\n- This test should PASS now. It ensures the refactored code correctly filters by interval_range and computes bounds on the right interval.\n\n### Test 5: test_that(\"get_coverage() and interval_coverage() agree on edge case: observation outside all intervals\", { ... })\n\n**Reproduces**: regression guard for the case where the observation is outside all prediction intervals — both functions must agree\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- Create a `forecast_quantile` object:\n  ```\n  observed = 100 for all rows\n  model = \"m1\", target = \"t1\", date = \"2020-01-01\", location = \"loc1\"\n  quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9)\n  predicted = c(1, 3, 5, 7, 9)  # observation 100 is far outside all intervals\n  ```\n**Action**:\n- Call `get_coverage()` with `by = get_forecast_unit(forecast)` and extract `interval_coverage` for all `interval_range` values\n- Call `interval_coverage()` with same numeric data for `interval_range = 50` and `interval_range = 80`\n**Expected**:\n- All `interval_coverage` values should be `FALSE` (100 is outside both the 50% interval [3, 7] and 80% interval [1, 9])\n- Both functions must agree: `expect_false()` for each\n**Fails now because**:\n- This test should PASS now. It guards against a refactoring that would incorrectly handle the case where the observation is completely outside all intervals.\n\n### Test 6: test_that(\"get_coverage() works after refactoring with non-symmetric intervals\", { ... })\n\n**Reproduces**: the existing test for non-symmetric intervals continues to pass after refactoring\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- This test already exists (test-get-coverage.R:32-54) but should be explicitly verified to still pass after refactoring\n- Use `example_quantile` with some quantile levels removed (`0.2, 0.3, 0.5`)\n**Action**:\n- Call `get_coverage(test)` on the modified data\n**Expected**:\n- Missing interval_range values should have `NA` for `interval_coverage`\n- Non-missing interval_range values should have no NAs\n- `expect_no_condition()` — should not error or warn\n**Fails now because**:\n- This test PASSES now and already exists. It is listed here as a reminder that the refactoring must preserve NA-handling behavior when quantile pairs are incomplete. If the shared helper doesn't properly propagate NAs for missing quantile pairs, this test will catch it.\n\n### Test 7: test_that(\"refactored interval coverage helper produces identical output to original implementations\", { ... })\n\n**Reproduces**: if a new shared internal helper is created (e.g., `compute_interval_coverage()`), this test verifies it directly\n**File**: `tests/testthat/test-get-coverage.R` or `tests/testthat/test-helper-quantile-interval-range.R`\n**Setup**:\n- Use the full `example_quantile` dataset (all models, all dates)\n- Compute coverage using the full `get_coverage()` pipeline\n- Also score the same data using `score()` which calls `interval_coverage` under the hood\n**Action**:\n- Call `get_coverage(example_quantile, by = get_forecast_unit(example_quantile))` and capture the `interval_coverage` column\n- Call `score(example_quantile)` and extract the `interval_coverage_50` and `interval_coverage_90` columns\n- Compare these values at matching forecast unit + interval_range combinations\n**Expected**:\n- The `interval_coverage` from `get_coverage()` for `interval_range == 50` should match `interval_coverage_50` from `score()` across all forecast units\n- The `interval_coverage` from `get_coverage()` for `interval_range == 90` should match `interval_coverage_90` from `score()` across all forecast units\n- Use `expect_equal()` for comparison\n**Fails now because**:\n- This test should PASS now. It is a comprehensive regression guard using the full example dataset to ensure that the refactored shared logic produces results consistent with both the `get_coverage()` pathway and the `score()` → `interval_coverage()` pathway. Any bug in the shared helper would be caught by a disagreement between the two pathways.\n\n## Notes for Implementation\n\n- The refactoring is small: ~5 lines of core logic (call `quantile_to_interval()`, compute bounds check). The primary approaches are:\n  1. **Have `get_coverage()` call `interval_coverage()` internally** for the bounds-check step, adapting the data format as needed\n  2. **Extract a shared internal helper** (e.g., `compute_interval_coverage_from_bounds()`) that both functions call\n- The bounds check in `get_coverage()` uses `(observed <= upper) & (observed >= lower)` while `interval_coverage()` uses `(observed >= lower) & (observed <= upper)` — these are logically identical but the order differs. The refactoring should standardize to one order.\n- `get_coverage()` operates on data.table forecast objects; `interval_coverage()` operates on numeric vectors. The `quantile_to_interval()` dispatcher already handles both input types, so the shared helper should work at the data.table level (after `quantile_to_interval()` has been called).\n- Existing tests in `test-get-coverage.R` and `test-metrics-quantile.R` already cover the basic functionality. The new tests above focus on cross-validation between the two code paths and boundary cases to guard against regression.\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":311,"cache_read_input_tokens":66637,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":311},"output_tokens":18,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"a85a5cc5-2162-439d-ac35-f26a490aa34a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013T8uvZzMHp3REzc2MTs6vL","type":"tool_result","content":"File created successfully at: /Users/nikos/github-synced/scoringutils-pipeline/.ralph/issues/454-deduplicate-coverage.md"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"6bcb3661-56fd-43eb-8a59-29c6ffc37fb5","tool_use_result":{"type":"create","filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/issues/454-deduplicate-coverage.md","content":"# Issue #454: Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage()`\n\n## Source\n\n- **GitHub**: https://github.com/epiforecasts/scoringutils/issues/454\n- **Action Summary**: Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern.\n- **Status Note**: Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic).\n\n## Test Specifications\n\n**Designed**: 2026-02-13\n**Test file**: `tests/testthat/test-get-coverage.R` (existing) and `tests/testthat/test-metrics-quantile.R` (existing)\n**Testing patterns observed**: testthat 3e style, data.table fixtures, `example_quantile` built-in dataset, `setup.R` pre-computes scored objects, numeric vector test data defined at top of `test-metrics-quantile.R` (`observed = c(1, -15, 22)`, `predicted` as 3x5 matrix, `quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9)`), `vdiffr` for visual regression, `expect_no_condition()` for clean execution, `get_range_from_quantile()` as helper.\n\n## Duplicated Code Analysis\n\nBoth functions share the same pattern:\n1. Call `quantile_to_interval()` to convert quantile format to interval format with `lower`/`upper` columns\n2. Perform bounds check: `(observed >= lower) & (observed <= upper)` to compute `interval_coverage`\n\n**`get_coverage()`** (R/get-coverage.R:69-71): calls `quantile_to_interval(forecast, format = \"wide\")` on a data.table, then `interval_coverage := (observed <= upper) & (observed >= lower)`\n\n**`interval_coverage()`** (R/metrics-quantile.R:361-363): calls `quantile_to_interval(observed, predicted, quantile_level)` on numeric vectors, then `interval_coverage := (observed >= lower) & (observed <= upper)`\n\nThe refactoring should extract the shared logic into a helper, or have `get_coverage()` delegate to `interval_coverage()` for the bounds-check step. Tests must verify that both functions produce identical results before and after refactoring.\n\n### Test 1: test_that(\"get_coverage() interval coverage matches interval_coverage() for same data\", { ... })\n\n**Reproduces**: the core duplication — both functions independently compute the same bounds check, and we need to verify they agree\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- Use `example_quantile` dataset, filtered to a single model (`\"EuroCOVIDhub-ensemble\"`) and a single target date to keep data small\n- Call `as_forecast_quantile()` on the filtered data\n- Extract the numeric vectors `observed`, `predicted` (as matrix), and `quantile_level` from the same filtered data\n**Action**:\n- Call `get_coverage(forecast, by = get_forecast_unit(forecast))` to get per-row interval coverage\n- For the same forecast rows, call `interval_coverage(observed, predicted, quantile_level, interval_range = 50)` (and repeat for interval_range = 80 if those quantiles exist)\n- Compare the interval_coverage column from `get_coverage()` against the boolean vector from `interval_coverage()` for matching interval ranges\n**Expected**:\n- The `interval_coverage` values from `get_coverage()` for a given `interval_range` must exactly equal the values returned by `interval_coverage()` for the same `interval_range` and same input data\n- Use `expect_equal()` to compare the matched values\n**Fails now because**:\n- This test should PASS now — both implementations compute the same thing. This is a regression guard that ensures the refactored version remains equivalent. If the refactoring introduces a bug (e.g., off-by-one in bounds, wrong filtering), this test will catch it.\n\n### Test 2: test_that(\"get_coverage() produces correct interval_coverage for known inputs\", { ... })\n\n**Reproduces**: verifies the bounds-check logic with hand-crafted data where the expected coverage values can be computed manually\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- Create a minimal `forecast_quantile` object with known values:\n  ```\n  observed = 5 for all rows\n  model = \"m1\", target = \"t1\", date = \"2020-01-01\", location = \"loc1\"\n  quantile_level = c(0.25, 0.5, 0.75)\n  predicted = c(3, 5, 7)   # observation IS inside the 50% interval [3, 7]\n  ```\n- Create a second forecast where `observed = 10` (OUTSIDE the interval [3, 7])\n- Combine into a single `forecast_quantile` object with 2 forecast units\n**Action**:\n- Call `get_coverage()` with `by = get_forecast_unit(forecast)` to get per-forecast coverage\n**Expected**:\n- For observed=5, predicted=[3,5,7]: `interval_coverage` for the 50% interval should be `TRUE` (5 >= 3 and 5 <= 7)\n- For observed=10, predicted=[3,5,7]: `interval_coverage` for the 50% interval should be `FALSE` (10 > 7)\n- `quantile_coverage` for quantile_level=0.5 should be `TRUE` for observed=5 (5 <= 5) and `FALSE` for observed=10 (10 > 5)\n- Use `expect_equal()` on specific rows\n**Fails now because**:\n- This test should PASS now. It is a regression guard to ensure the extracted/shared bounds-check logic computes the same values after refactoring.\n\n### Test 3: test_that(\"interval_coverage() produces correct results for boundary cases\", { ... })\n\n**Reproduces**: boundary cases in the bounds check (observation exactly on lower/upper bound)\n**File**: `tests/testthat/test-metrics-quantile.R`\n**Setup**:\n- Use numeric vectors:\n  ```\n  observed = c(3, 7, 5)   # exactly on lower, exactly on upper, inside\n  predicted = matrix(c(3, 5, 7), nrow = 3, ncol = 3, byrow = TRUE)  # each row: [3, 5, 7]\n  quantile_level = c(0.25, 0.5, 0.75)\n  ```\n**Action**:\n- Call `interval_coverage(observed, predicted, quantile_level, interval_range = 50)`\n**Expected**:\n- All three should be `TRUE`: observation on lower bound (3 >= 3), on upper bound (7 <= 7), and inside (5 >= 3 and 5 <= 7)\n- `expect_equal(result, c(TRUE, TRUE, TRUE))`\n**Fails now because**:\n- This test should PASS now. It guards against a refactoring that accidentally changes `>=`/`<=` to `>`/`<` in the bounds check.\n\n### Test 4: test_that(\"interval_coverage() handles multiple interval ranges correctly\", { ... })\n\n**Reproduces**: verifying that when the shared helper handles multiple interval ranges, results are consistent\n**File**: `tests/testthat/test-metrics-quantile.R`\n**Setup**:\n- Use numeric vectors with 5 quantile levels:\n  ```\n  observed = c(5)\n  predicted = matrix(c(1, 3, 5, 7, 9), nrow = 1)  # quantiles for [0.1, 0.25, 0.5, 0.75, 0.9]\n  quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9)\n  ```\n**Action**:\n- Call `interval_coverage(observed, predicted, quantile_level, interval_range = 50)` — tests the 50% interval [3, 7]\n- Call `interval_coverage(observed, predicted, quantile_level, interval_range = 80)` — tests the 80% interval [1, 9]\n**Expected**:\n- 50% interval: `TRUE` (5 is inside [3, 7])\n- 80% interval: `TRUE` (5 is inside [1, 9])\n- Use `expect_equal(result, TRUE)` for each\n**Fails now because**:\n- This test should PASS now. It ensures the refactored code correctly filters by interval_range and computes bounds on the right interval.\n\n### Test 5: test_that(\"get_coverage() and interval_coverage() agree on edge case: observation outside all intervals\", { ... })\n\n**Reproduces**: regression guard for the case where the observation is outside all prediction intervals — both functions must agree\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- Create a `forecast_quantile` object:\n  ```\n  observed = 100 for all rows\n  model = \"m1\", target = \"t1\", date = \"2020-01-01\", location = \"loc1\"\n  quantile_level = c(0.1, 0.25, 0.5, 0.75, 0.9)\n  predicted = c(1, 3, 5, 7, 9)  # observation 100 is far outside all intervals\n  ```\n**Action**:\n- Call `get_coverage()` with `by = get_forecast_unit(forecast)` and extract `interval_coverage` for all `interval_range` values\n- Call `interval_coverage()` with same numeric data for `interval_range = 50` and `interval_range = 80`\n**Expected**:\n- All `interval_coverage` values should be `FALSE` (100 is outside both the 50% interval [3, 7] and 80% interval [1, 9])\n- Both functions must agree: `expect_false()` for each\n**Fails now because**:\n- This test should PASS now. It guards against a refactoring that would incorrectly handle the case where the observation is completely outside all intervals.\n\n### Test 6: test_that(\"get_coverage() works after refactoring with non-symmetric intervals\", { ... })\n\n**Reproduces**: the existing test for non-symmetric intervals continues to pass after refactoring\n**File**: `tests/testthat/test-get-coverage.R`\n**Setup**:\n- This test already exists (test-get-coverage.R:32-54) but should be explicitly verified to still pass after refactoring\n- Use `example_quantile` with some quantile levels removed (`0.2, 0.3, 0.5`)\n**Action**:\n- Call `get_coverage(test)` on the modified data\n**Expected**:\n- Missing interval_range values should have `NA` for `interval_coverage`\n- Non-missing interval_range values should have no NAs\n- `expect_no_condition()` — should not error or warn\n**Fails now because**:\n- This test PASSES now and already exists. It is listed here as a reminder that the refactoring must preserve NA-handling behavior when quantile pairs are incomplete. If the shared helper doesn't properly propagate NAs for missing quantile pairs, this test will catch it.\n\n### Test 7: test_that(\"refactored interval coverage helper produces identical output to original implementations\", { ... })\n\n**Reproduces**: if a new shared internal helper is created (e.g., `compute_interval_coverage()`), this test verifies it directly\n**File**: `tests/testthat/test-get-coverage.R` or `tests/testthat/test-helper-quantile-interval-range.R`\n**Setup**:\n- Use the full `example_quantile` dataset (all models, all dates)\n- Compute coverage using the full `get_coverage()` pipeline\n- Also score the same data using `score()` which calls `interval_coverage` under the hood\n**Action**:\n- Call `get_coverage(example_quantile, by = get_forecast_unit(example_quantile))` and capture the `interval_coverage` column\n- Call `score(example_quantile)` and extract the `interval_coverage_50` and `interval_coverage_90` columns\n- Compare these values at matching forecast unit + interval_range combinations\n**Expected**:\n- The `interval_coverage` from `get_coverage()` for `interval_range == 50` should match `interval_coverage_50` from `score()` across all forecast units\n- The `interval_coverage` from `get_coverage()` for `interval_range == 90` should match `interval_coverage_90` from `score()` across all forecast units\n- Use `expect_equal()` for comparison\n**Fails now because**:\n- This test should PASS now. It is a comprehensive regression guard using the full example dataset to ensure that the refactored shared logic produces results consistent with both the `get_coverage()` pathway and the `score()` → `interval_coverage()` pathway. Any bug in the shared helper would be caught by a disagreement between the two pathways.\n\n## Notes for Implementation\n\n- The refactoring is small: ~5 lines of core logic (call `quantile_to_interval()`, compute bounds check). The primary approaches are:\n  1. **Have `get_coverage()` call `interval_coverage()` internally** for the bounds-check step, adapting the data format as needed\n  2. **Extract a shared internal helper** (e.g., `compute_interval_coverage_from_bounds()`) that both functions call\n- The bounds check in `get_coverage()` uses `(observed <= upper) & (observed >= lower)` while `interval_coverage()` uses `(observed >= lower) & (observed <= upper)` — these are logically identical but the order differs. The refactoring should standardize to one order.\n- `get_coverage()` operates on data.table forecast objects; `interval_coverage()` operates on numeric vectors. The `quantile_to_interval()` dispatcher already handles both input types, so the shared helper should work at the data.table level (after `quantile_to_interval()` has been called).\n- Existing tests in `test-get-coverage.R` and `test-metrics-quantile.R` already cover the basic functionality. The new tests above focus on cross-validation between the two code paths and boundary cases to guard against regression.\n","structuredPatch":[],"originalFile":null}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01L3aHec84fFqkm6We4cFd4r","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me update the triage table."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3769,"cache_read_input_tokens":66948,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3769},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"458810c5-e9fc-4912-ac5e-957b39ffe7ed"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01L3aHec84fFqkm6We4cFd4r","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017iLksoN1t785Exnv69LDmD","name":"TodoWrite","input":{"todos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"in_progress","activeForm":"Updating triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3769,"cache_read_input_tokens":66948,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3769},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"f786b520-6beb-4a9a-b161-894a2f1be53f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017iLksoN1t785Exnv69LDmD","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"dca9bbac-3d8b-4f11-a413-99effe1a5521","tool_use_result":{"oldTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"in_progress","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"pending","activeForm":"Updating triage table"}],"newTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"in_progress","activeForm":"Updating triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_011Ny2euRVkGnkg8Si4H9TpN","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UiUdTKd1z55hb5gKqU12va","name":"Edit","input":{"replace_all":false,"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","old_string":"| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |","new_string":"| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [x] | [ ] | — | [ ] |"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":301,"cache_read_input_tokens":70717,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":301},"output_tokens":18,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"42e9ea5e-faec-43cf-999d-66be1c1ba7d6"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UiUdTKd1z55hb5gKqU12va","type":"tool_result","content":"The file /Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"8279929d-e1a7-4592-89cf-f6fc2d3dce0c","tool_use_result":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","oldString":"| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |","newString":"| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [x] | [ ] | — | [ ] |","originalFile":"# scoringutils Issue Triage\n\n> Auto-generated by Ralph triage loop. Do not edit manually during the loop.\n\n## Summary\n\n| Metric | Count |\n|--------|-------|\n| Total open issues | 106 |\n| Triaged | 106 |\n| Remaining | 0 |\n\n## Pipeline\n\n- [ ] **Test design** — Test specs written for automatable issues\n- [ ] **Implement** — Fixes implemented, PRs created\n- [ ] **Review** — All PRs independently reviewed\n- [ ] **Summary** — Human summary generated\n\n## Issues\n\n| # | Title | Labels | Opened | Category | Priority | Complexity | Automatable | Status Note | Dependencies | Action Summary | Has Tests | Implemented | PR | Reviewed |\n|---|-------|--------|--------|----------|----------|------------|-------------|-------------|--------------|----------------|-----------|-------------|-----|----------|\n| 1065 | Release reminder - February 2026 | | 2026-02-02 | meta | medium | medium | partially | Automated reminder. Dev version (2.1.2.9000) has multivariate feature + bug fix accumulated over 161 days. No blocking bugs. Two open PRs (#1060, #1062) are infrastructure changes. | Maintainers should decide whether to merge #1060/#1062 before releasing. | Decide whether to cut a new release (likely v2.2.0 given multivariate feature). If yes, run `usethis::use_release_issue()` and proceed with CRAN submission. If not, close the issue. | N/A | N/A | — | N/A |\n| 1064 | Pre-ranking for multivariate calibration assessment | | 2026-01-30 | feature | medium | large | no | No pre-ranking or multivariate calibration diagnostics exist in the codebase. The `forecast_sample_multivariate` class and energy score are implemented and stable. Design decision needed: integrate into `score()` (Option A) or standalone `get_multivariate_ranks()` function (Option B). MST method may need `igraph` dependency. | None blocking. Builds on existing `forecast_sample_multivariate` infrastructure. | Implement pre-ranking methods (multivariate rank, optionally MST rank) from Gneiting et al. (2008) as calibration diagnostics for multivariate forecasts, after resolving the API design choice with maintainers. | N/A | N/A | — | N/A |\n| 1057 | Revise README to remove news about the package rewrite | documentation | 2026-01-29 | documentation | low | medium | partially | Open, no assignee. The rewrite messaging and Google Drive paper links appear in 4 locations in README.Rmd. Mechanical edits are straightforward but replacement text requires editorial decisions about where to point users for documentation. | Loosely related to #810 (pkgdown for dev docs) and #759 (future of scoringutils.Rmd vignette). | Edit `README.Rmd` to remove/soften the \"major re-write\" paragraph and replace Google Drive paper links with pointers to package vignettes/docs, then re-render README.md. Confirm preferred documentation hierarchy with maintainers first. | N/A | N/A | — | N/A |\n| 1055 | Model confidence sets | feature request | 2026-01-19 | feature | medium | large | no | Design phase — methodology not yet agreed upon. 6 comments with active debate. Naive pairwise p-value approach acknowledged as incomplete. Three alternatives discussed: Hansen et al. MCS, novel extension for incomplete sets, Bayesian approach. No consensus reached. | Relates to #750 (p-value computation refactor), #769 (pairwise comparison arguments), #37 (Diebold-Mariano test), #1022 (two-model pairwise). | Resolve design consensus on which MCS methodology to implement (Hansen et al. on complete sets, novel incomplete-set extension, or Bayesian), then implement `get_model_confidence_set()` building on existing pairwise comparison infrastructure. | N/A | N/A | — | N/A |\n| 1046 | multivariate scoring in scoring rules vignette | documentation | 2025-11-04 | documentation | low | small | partially | Open, agreed upon by maintainers. The scoring-rules vignette has zero mentions of multivariate scoring. A dedicated multivariate vignette already exists. Only a cross-reference and brief section are missing. | None. All underlying code and the dedicated multivariate vignette already exist. | Add a short section to `vignettes/scoring-rules.Rmd` describing the energy score for multivariate forecasts and cross-linking to the existing `scoring-multivariate-forecasts` vignette. No code changes needed. | N/A | N/A | — | N/A |\n| 1044 | Remove the commented out vignette code. | | 2025-09-23 | cleanup | low | small | yes | Still relevant. Two HTML comment blocks remain: a WIP \"Details on the grouping\" section in `scoring-multivariate-forecasts.Rmd` (lines 88-102) and a ~50-line PIT/Calibration section in `scoring-rules.Rmd` (lines 444-493). Originated from PR #998 review comment. | None. | Delete the two HTML-commented-out code blocks from `vignettes/scoring-multivariate-forecasts.Rmd` and `vignettes/scoring-rules.Rmd`. Optionally confirm with maintainers whether the PIT/Calibration text should be preserved elsewhere. | N/A | [ ] | — | [ ] |\n| 1043 | Update print method for multivariate forecasts to show `joint_across` | | 2025-09-21 | enhancement | low | small | yes | Still relevant. No `print.forecast_sample_multivariate` method exists; the generic `print.forecast` handles all types. `joint_across` is not stored as an attribute — `set_grouping()` computes `.mv_group_id` but discards the original `joint_across` value. | Builds on multivariate infrastructure from merged PR #998. | Add a `print.forecast_sample_multivariate()` method that displays `joint_across` columns. Requires either storing `joint_across` as an attribute during object construction or computing it as the set difference between forecast unit and `get_grouping()` output. Update test snapshots. | [x] | [ ] | — | [ ] |\n| 1026 | as_forecast_sample.forecast_quantile | enhancement | 2025-07-13 | enhancement | medium | medium | partially | Still relevant. The reverse conversion (`as_forecast_quantile.forecast_sample`) exists, but `as_forecast_sample.forecast_quantile` does not. No distribution-fitting dependencies in DESCRIPTION. Author plans to borrow from hubverse approach. | Related to #557 (sample_to_quantile.numeric). May introduce new dependency (e.g., `distfromq`). | Implement `as_forecast_sample.forecast_quantile()` S3 method that fits a distribution or uses interpolation to approximate the predictive CDF from quantiles and draws samples. Requires design decision on approximation method and potential new package dependency. | N/A | N/A | — | N/A |\n| 1022 | Let `get_pairwise_comparison()` run with two models? | | 2025-07-03 | enhancement | low | small | yes | Still relevant. The check at line 162 of `R/pairwise-comparisons.R` calls `cli_abort()` when `length(setdiff(comparators, baseline)) < 2`, blocking the 2-models-with-baseline case. Maintainer has expressed agreement to change to a warning. Also has typo \"compairisons\". | None. Self-contained change. | Change the `cli_abort()` to `cli_warn()` at line 162 of `R/pairwise-comparisons.R` so that 2 models with one as baseline emits a warning instead of an error. Update test expectations and fix the \"compairisons\" typo. | [x] | [ ] | — | [ ] |\n| 1002 | LaTeX not rendering in function reference | | 2025-05-20 | documentation | medium | small | yes | Still relevant. pkgdown defaults to `mathml` which doesn't handle the 63 `\\eqn{}`/`\\deqn{}` occurrences in 4 metrics files. Upstream r-lib/pkgdown#2704 is closed (won't fix mathml); recommended fix is to use `mathjax` or `katex`. No `math-rendering` setting exists in `_pkgdown.yml`. | None internally. CRAN release needed for release site fix. | Add `math-rendering: katex` (or `mathjax`) under the `template:` key in `_pkgdown.yml` and rebuild the pkgdown site. One-line config change fixes all 63 LaTeX expressions across 4 documentation files. | [x] | [ ] | — | [ ] |\n| 986 | Test scores are correct | | 2025-02-21 | enhancement | medium | medium | partially | Still relevant. WIS has reference tests (CovidHubUtils), `logs_binary` validates against `Metrics::ll`, `rps_ordinal` against `scoringRules::rps_probs`. But CRPS/DSS/log_score for sample-based forecasts, dispersion, overprediction, underprediction lack absolute-value tests. No snapshot tests pin numerical outputs on example datasets. | None | Add (1) snapshot tests pinning `score()` output on each built-in example dataset, and (2) targeted correctness tests validating key sample-based scoring functions against known analytical solutions or external reference implementations. | N/A | N/A | — | N/A |\n| 983 | Create `.data.frame` methods for `as_forecast<type>` | | 2025-01-14 | refactor | low | small | yes | Still relevant. Prerequisite #981 (making `as_forecast_*` S3 generics) is done. Currently all 7 types only have `.default` methods; no `.data.frame` methods exist. The `.default` methods silently accept any type and rely on `as.data.table()` coercion deep inside. | #981 (done). No blockers. | Rename the 7 existing `.default` methods to `.data.frame` (since they assume data.frame-like input), then create new `.default` methods that error with a helpful message about unsupported types. Update NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |\n| 975 | Add link to score definitions in `summarise_score()` | | 2024-11-05 | enhancement | low | small | yes | Still relevant. No `print.scores` method exists anywhere in the codebase. Scores objects fall through to data.table's default print. The scoring rules vignette exists at the expected URL. Maintainers favor option 1 (link to docs) over printing definitions. | Closely related to #667 (create `print.scores` method). Should be aware of #935 (print side-effects on column mutation). | Create a `print.scores()` S3 method in `R/class-scores.R` that calls `NextMethod()` then appends a `cli_text()` link to the scoring rules vignette. Ideally coordinate with #667 to also show the metrics used. | [x] | [ ] | — | [ ] |\n| 944 | Make `get_metrics()` more customisable | enhancement, feature request | 2024-10-08 | enhancement | low | medium | partially | Still relevant. `get_metrics()` is an S3 generic with `select`/`exclude` args (via #832/#903), but the proposed `replace` argument for inline metric customization has not been implemented. The `add_args` idea from the issue body is less well-defined. | #832 (done). Related to #789 (documenting customization workflow). | Add a `replace` argument to `select_metrics()` and propagate through all 7 `get_metrics.forecast_*` methods, accepting a named list of functions to substitute by name. Evaluate whether `add_args` is worth implementing or should be dropped. | N/A | N/A | — | N/A |\n| 942 | Discrimination plots for binary forecasts | enhancement, feature request | 2024-10-07 | feature | low | small | yes | Still relevant. No binary-specific plot functions exist in the codebase. No discrimination plot, reliability diagram, or any binary visualization exists. The `forecast_binary` class and `example_binary` dataset provide exactly the data structures needed. | None. Thematically adjacent to #49 (calibration plotting) and #286 (diagnostics) but no hard dependency. | Implement a new `plot_discrimination()` function that accepts a `forecast_binary` object, groups predicted probabilities by the two levels of observed, and renders overlapping density plots via ggplot2, with optional faceting by model. Add tests and docs. | [x] | [ ] | — | [ ] |\n| 941 | Rethink `score()` and maybe make it not S3 anymore | refactor | 2024-10-06 | refactor | medium | medium | no | Still relevant. All 7 `score.forecast_*` S3 methods share near-identical boilerplate (clean, validate, reshape, apply_metrics, as_scores). Only the reshape step differs per type. No refactoring has been done since the issue was opened. Both opener and reviewer agree on the direction. | Related to #338 (cross-type metrics). Benefits from #982 (done). | Refactor `score()` from 7 S3 methods into a single function by extracting the type-specific data reshaping into a new internal S3 generic (e.g., `prepare_for_scoring()`). Requires design decision on the preparation step's interface. | N/A | N/A | — | N/A |\n| 939 | remove `check_input_<forecast_type>` functions | refactor | 2024-10-03 | refactor | low | small | partially | Still relevant. All 5 `check_input_*` functions exist. 3 (sample, quantile, interval) are unused in production code — only tested. 2 (binary, point) are used in `assert_forecast` methods. Other types (sample, quantile) already skip the check pattern, showing inconsistency. | Supersedes #684 (subset of this issue). | Delete all 5 `check_input_*` functions and their man pages. For the 2 in use (binary, point), refactor call sites to call `assert_input_*` directly or inline `check_try()`, matching the pattern already used by sample/quantile. Close #684 as duplicate. | N/A | N/A | — | N/A |\n| 938 | Rename/refactor? `assert_dims_ok_point` | refactor, low-priority | 2024-10-03 | refactor | low | small | partially | Still relevant. `assert_dims_ok_point` is used by both `assert_input_point` and `assert_input_binary`, making the `_point` suffix misleading. The function is a general-purpose dimension check. Companion `check_dims_ok_point` also exists. ~5 files to update. | None. | Rename `assert_dims_ok_point` (and `check_dims_ok_point`) to a type-agnostic name (e.g., `assert_dims_ok_scalar` or `assert_dims_match`). Update definition, 2 call sites, tests, and docs. Optionally move out of `metrics-point.R` into a shared file. | N/A | N/A | — | N/A |\n| 935 | `forecast` and `scores` objects print output every time a column is changed | high-priority | 2024-09-29 | bug | medium | large | no | Still relevant. Core bug unfixed: `print.forecast` dispatches to `print.data.table` via `NextMethod()`, which breaks data.table's internal `:=` print-suppression (call stack too deep for `shouldPrint()`). PR #956 only worked around nominal forecast validation errors with a `nrow > 30` hack. The `scores` side may no longer be affected (no `print.scores` exists). | Related to #667 (print.scores — would reintroduce problem), #685 (suppress printing in tests), #1043 (multivariate print). | Design decision needed: (a) accept cosmetic annoyance, (b) reimplement `print.forecast` without dispatching to `print.data.table` (e.g., use `format.data.table` + manipulate `.global$print`), or (c) pursue upstream data.table fix per @MichaelChirico's suggestion. | N/A | N/A | — | N/A |\n| 934 | Use illustration of local and global scores | documentation | 2024-09-28 | documentation | low | small | partially | Still relevant. `score-locality.png` already exists in `vignettes/` but is orphaned — not referenced by any vignette. The scoring-rules vignette mentions local/global in text only, with no visual. Code to regenerate is in the issue body. | Sibling issues: #929, #930, #933 (all repurpose manuscript illustrations). Consider batching. | Add the existing `score-locality.png` to `scoring-rules.Rmd` (or a new comparative vignette) with an explanatory paragraph on local vs. global scoring rules. Coordinate with #929/#930/#933 for a unified approach. | N/A | N/A | — | N/A |\n| 933 | Use nice illustration of CRPS/DSS/Log score + the number of samples needed for scores to converge | documentation | 2024-09-28 | documentation | low | medium | partially | Still relevant. `score-convergence-outliers.png` already exists in `vignettes/` but is orphaned (not referenced by any vignette). Issue #758 contains a detailed draft vignette stub (\"Choosing a scoring rule\") that already references this PNG with prose. Should be batched with siblings #929, #930, #934. | Sibling issues #929, #930, #934. Parent issue #758 (scoring rules explanation vignette). Related to #759 (vignette future). | Create a new \"Choosing a scoring rule\" vignette (per #758's stub) or integrate `score-convergence-outliers.png` into `scoring-rules.Rmd` with explanatory text on sample convergence and score properties. Best addressed as a batch with #929/#930/#934 under #758. | N/A | N/A | — | N/A |\n| 930 | Use nice illustration of the effect of increasing the mean on increasing average scores | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. `illustration-effect-scale.png` already exists in `vignettes/` but is orphaned. A deleted stub vignette (`Choosing-a-score.Rmd`) once referenced it. Author expressed uncertainty (\"maybe we don't want it anymore\"). Should be batched with siblings #929, #933, #934. | Sibling issues #929, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future). | Decide whether to integrate `illustration-effect-scale.png` into `scoring-rules.Rmd` or a new \"Choosing a score\" vignette, with text explaining how average scores scale with forecast target magnitude. If unwanted, close and delete orphaned PNG. Address with #929/#933/#934 as batch. | N/A | N/A | — | N/A |\n| 929 | Use nice illustration of Sharpness and calibration | documentation | 2024-09-27 | documentation | low | medium | partially | Still relevant. Unlike siblings (#930, #933, #934), the calibration/sharpness PNG does **not** exist in the repo — it was never generated. Code to create it is in the issue body. The `scoring-rules.Rmd` has a commented-out calibration section (lines ~449-492) where this illustration would naturally fit. | Sibling issues #930, #933, #934. Related to #758 (scoring rules vignette), #759 (vignette future), #785 (vignette updates). | Generate `calibration-sharpness-illustration.png` from the code in the issue body, place in `vignettes/`, and integrate into `scoring-rules.Rmd` (or a new \"Choosing a score\" vignette). Coordinate with #930/#933/#934 batch. | N/A | N/A | — | N/A |\n| 927 | Make all internal columns, in particular `forecast_id`, more robust by prefixing it with \"scoringutils_\" | good first issue, high-priority | 2024-09-26 | refactor | high | medium | partially | Still relevant. Unprefixed internal columns (`forecast_id`, `weight`, `boundary`) risk collision with user data columns. Codebase already mixes two conventions: `.scoringutils_*` (e.g., `.scoringutils_N`) and `scoringutils_Internal*`. ~3 source files, 1 globalVariables file, 1 snapshot to update for `forecast_id` alone. Not a breaking change — all affected columns are internal. | None blocking. Self-contained refactor. | Rename unprefixed internal temp columns (`forecast_id`, `weight`, `boundary`) to `.scoringutils_` prefix, matching existing convention. Update `R/helper-quantile-interval-range.R`, `R/metrics-quantile.R`, `R/z-globalVariables.R`, and test snapshots. Also unify the two naming conventions (`.scoringutils_*` vs `scoringutils_Internal*`). | N/A | N/A | — | N/A |\n| 922 | remove `check_columns_present` | | 2024-09-25 | cleanup | low | small | yes | Still relevant. `check_columns_present` (and `test_columns_present`) are internal functions with 10 call sites across 9 files. All can be replaced by `checkmate::assert_subset`/`check_subset`/`test_subset` (already a dependency). One call site in `pairwise-comparisons.R:143` uses a custom error message and needs slightly different handling. No user-facing API impact. | Related to #839 (remove `test_columns_not_present`) — both modify `R/check-input-helpers.R`, should be batched. | Replace all 10 call sites of `check_columns_present` (and 1 of `test_columns_present`) with equivalent `checkmate` functions, then delete both functions and their tests. Consider batching with #839. | [x] | [ ] | — | [ ] |\n| 909 | Compact version of `get_duplicate_forecasts()` | enhancement, good first issue, help wanted | 2024-09-13 | enhancement | low | small | yes | Already implemented. The `counts` parameter on `get_duplicate_forecasts()` provides exactly the compact output requested. Tests exist. Issue can be closed. | None | Close this issue — the `counts` argument already provides compact/aggregated duplicate output. No further code changes needed. | N/A | N/A | — | N/A |\n| 907 | coverage of function factory | enhancement | 2024-09-12 | enhancement | low | small | partially | Still relevant. No factory helper exists. Users must manually use `purrr::partial()` (documented in `score()` docs). Only `interval_coverage` benefits currently. Hard-coded 50% and 90% variants in `get_metrics.forecast_quantile()`. Discussion stalled after 5 comments with no consensus on approach. | Related to #451 (quantile/interval metric organization), #668 (coverage flexibility). | Add either a generic `expand_metric()` utility or a `coverage_ranges` argument to `get_metrics.forecast_quantile()` to programmatically generate multiple parameterized coverage metric variants. Needs design decision on approach given only one metric currently benefits. | N/A | N/A | — | N/A |\n| 898 | Support for \"Mixture of Parameters\" scoring | feature request | 2024-09-06 | feature | low | large | partially | Still relevant but low urgency. No `forecast_distribution` class exists. Prerequisites #832 (metrics S3 rework) and #846 (ordinal class) are done. Original requester has deprioritized. Would require new class, input format design, dispatch to `scoringRules` parametric functions, and mixture handling. | #888 (S3 rework of `get_duplicate_forecasts()` — minor blocker for clean extensibility). Builds on completed #832, #846. | Implement a new `forecast_distribution` class for scoring forecasts expressed as parametric distributions or mixtures, dispatching to `scoringRules`' closed-form scoring functions. Deferred until active demand — keep on `scoringutils-2.x` milestone. | N/A | N/A | — | N/A |\n| 895 | Check quantile prediction order helper function | enhancement | 2024-08-19 | enhancement | medium | small | yes | Still relevant. Monotonicity check only exists deep in `bias_quantile_single_vector()` (metrics-quantile.R:495) as a hard error. No early warning in `assert_forecast.forecast_quantile()`. No user-facing diagnostic function. Issue #412 is the maintainer-created parent with an agreed plan (steps 1 and 3 not done). | Duplicate of / should be resolved together with #412. | Create a user-facing helper (e.g., `get_non_monotonic_forecasts()`) to identify rows where predictions decrease with increasing quantile level, and add a warning in `assert_forecast.forecast_quantile()`. Resolves both #895 and #412. | [x] | [ ] | — | [ ] |\n| 889 | Make `get_protected_columns()` S3 to avoid hard-coding things | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. `get_protected_columns()` is a plain function with a hard-coded superset of all column names for all forecast types. Not S3. Only 2 call sites (both in `forecast-unit.R`). The `intersect()` filter masks the problem but prevents extensibility for new forecast classes. No progress made. | Coordinate with #888 (same S3 pattern for `get_duplicate_forecasts()`). Unblocks #852 (new forecast class workflow). Largely resolves #740 (`get_forecast_unit` customization). | Convert `get_protected_columns()` into an S3 generic with per-class methods so each forecast type declares its own protected columns. Coordinate with #888 for consistent pattern. | N/A | N/A | — | N/A |\n| 888 | Rework `get_duplicate_forecasts()` to S3 to avoid hard-coding columns and types | high-priority | 2024-08-10 | refactor | high | medium | partially | Still relevant. Function hard-codes `c(\"sample_id\", \"quantile_level\", \"predicted_label\")` in two places instead of dispatching on S3 class. No work done. Codebase has many S3 precedents now. | Coordinate with #889 (same pattern for `get_protected_columns()`). Partially blocks #898, #852. Related to #740. | Convert `get_duplicate_forecasts()` into an S3 generic with per-class methods where each method knows its own type-specific columns. Retain a `.data.frame`/`.default` method with current column-sniffing logic for backward compatibility. | N/A | N/A | — | N/A |\n| 885 | Functions to export or make internal | refactor, high-priority | 2024-08-10 | refactor | medium | medium | no | Partially done. `validate_forecast()` already deleted (PR #921). Remaining: `as_forecast_generic()`, `as_scores()`, `apply_metrics()` still internal; `new_forecast()` ambiguously exported with `@keywords internal`. Design decision needed on which functions to export for custom forecast class authors. | Depends on #746 (closed). Blocks custom forecast class creation. Related to #852 (design workflow for new classes). | Decide final export list for custom forecast class API: export `as_forecast_generic()`, `as_scores()`, `apply_metrics()`, resolve ambiguous `new_forecast()` status, and update NAMESPACE. Needs maintainer decision on naming/prefixing. | N/A | N/A | — | N/A |\n| 880 | Allow users to specify the quantile in `as_forecast_point.forecast_quantile()? | | 2024-07-30 | enhancement | low | small | yes | Still relevant. Function hard-codes `0.5` on two lines (107, 115) in `R/class-forecast-quantile.R`. Design decisions already resolved in PR #876 discussion: parameter name `quantile_level`, default `0.5`, passed via `...`. No work done since issue opened. | None. Self-contained enhancement. | Add a `quantile_level` parameter (default `0.5`) to `as_forecast_point.forecast_quantile()`, replacing the two hard-coded `0.5` references, then add tests and documentation. | [x] | [ ] | — | [ ] |\n| 872 | Add a reference for CRPS decomposition approach. | | 2024-07-25 | documentation | low | small | partially | Still relevant. `crps_sample()` in `R/metrics-sample.R` has only a `scoringRules` package reference, not one for the decomposition approach. `wis()` already properly cites Bracher et al. 2021. Minor decision needed on which paper(s) to cite: Bracher et al. 2021 and/or Arnold et al. 2023. | Related to #870 (CRPS/WIS decomposition refactor). Could be addressed in same PR. | Add `@references` entries to `crps_sample()` roxygen block citing paper(s) formalizing the overprediction/underprediction/dispersion decomposition (likely Bracher et al. 2021 and/or Arnold et al. 2023). | N/A | N/A | — | N/A |\n| 870 | Refactor CRPS and WIS decomposition | refactor | 2024-07-22 | refactor | low | medium | partially | Partially addressed. PR #985 (Feb 2025) simplified `crps_sample()` decomposition internally, but the structural duplication remains: `crps_sample()`, `wis()`, and `interval_score()` each independently implement `separate_results` branching, and 6 wrapper functions (3 sample + 3 quantile) repeat identical extraction patterns. | Related to #872 (CRPS decomposition reference). Filed during PR #854 review. On `scoringutils-2.x` milestone. | Extract the decomposition logic (`separate_results` branching and wrapper function boilerplate) into a shared internal helper or factory pattern so `crps_sample()`, `wis()`, and `interval_score()` delegate to the same mechanism instead of each implementing it independently. | N/A | N/A | — | N/A |\n| 852 | Design workflow for creating new forecast classes | enhancement | 2024-07-10 | enhancement | high | large | no | Still relevant. The building blocks exist (`new_forecast()` exported, `as_forecast_generic()` internal, pattern visible in 7 class files) but no documentation, vignette, or guide for third-party class creation exists. Three prerequisite S3 refactors (#889, #888, #740) remain open and block clean extensibility because `get_protected_columns()` and `get_duplicate_forecasts()` hard-code column names, preventing new classes from working seamlessly. `as_forecast_generic()`, `apply_metrics()`, and `as_scores()` are still unexported (#885). | #889 (S3 `get_protected_columns`), #888 (S3 `get_duplicate_forecasts`), #740 (`get_forecast_unit` customization), #885 (export decision for helper functions). | Complete prerequisite S3 refactors (#889, #888), decide export list for class-creation helpers (#885), then write a vignette or article documenting the step-by-step workflow for creating a custom `forecast_*` class (define `as_forecast_<type>`, `assert_forecast.<type>`, `score.<type>`, `get_metrics.<type>`, `get_protected_columns.<type>`, `get_duplicate_forecasts.<type>`). | N/A | N/A | — | N/A |\n| 840 | Clean up input checks | | 2024-06-07 | refactor | low | medium | partially | Still relevant. Input checks are applied inconsistently: `assert_forecast.forecast_binary` and `assert_forecast.forecast_point` call `check_input_*` wrappers (double-validating with scoring functions), while sample/quantile/ordinal/nominal types skip this. `check-input-helpers.R` has a comment marking `check_try()` and wrappers for future deletion. Author leans toward pushing all checks to the scoring-function level. No work done. | Related to #939 (remove `check_input_*` functions), #839, #922. On `scoringutils-2.x` milestone. | Decide where input validation should live (scoring-function level vs. `assert_forecast` level vs. both-consistently). Then either remove `check_input_binary`/`check_input_point` calls from `assert_forecast` methods and delete wrappers + `check_try()`, or add missing checks to sample/quantile `assert_forecast` methods for consistency. | N/A | N/A | — | N/A |\n| 839 | Remove `test_columns_not_present` | | 2024-06-07 | cleanup | low | small | yes | Still relevant. `test_columns_not_present()` is a trivial 1-line wrapper around `checkmate::test_names(..., disjunct.from = ...)` with exactly 1 production call site (`R/class-forecast-binary.R:68`) and 1 test block. `checkmate::test_names` is already imported. Straightforward removal. | Related to #922 (remove `check_columns_present`) and #840 (clean up input checks). Can be batched but has no blocking dependency. | Replace the single call in `R/class-forecast-binary.R:68` with `test_names(colnames(forecast), disjunct.from = ...)`, delete the function definition in `R/check-input-helpers.R`, delete the test block, and remove the generated man page. Optionally bundle with #922. | [x] | [ ] | — | [ ] |\n| 826 | Update `plot_heatmap()` to be a table instead of a plot | | 2024-05-19 | enhancement | low | medium | partially | Still relevant. Accessibility concern from @Bisaloo is valid: current ggplot2 heatmap renders values as image text, inaccessible to screen readers. Conversation stalled — Bisaloo never replied to follow-up about output format (HTML/LaTeX). `kableExtra` is already in `Suggests`. This is a breaking change: function returns `ggplot` object, used with `+ facet_wrap()` in `inst/manuscript/`. Not used in any current vignette. Design decision needed on migration strategy. | None blocking. Thematically adjacent to #713 (visualization vignette). | Resolve stalled design conversation on output format and migration strategy (deprecate + new function vs. breaking change). Implement colored-table version using `kableExtra` (already in Suggests) or `gt` (new dep), update manuscript, replace test/snapshot, and deprecate or replace `plot_heatmap()`. | N/A | N/A | — | N/A |\n| 825 | Package scaffolding | | 2024-05-19 | refactor | low | small | yes | Still relevant. Neither recommendation acted upon: (1) `Metrics` package still imported for 3 trivial functions (`ae`, `se`, `ape`) used only in `R/class-forecast-point.R` — these are one-line base-R replacements. `Metrics` did receive a 2025 CRAN update, reducing urgency. (2) `matrixStats` not adopted; 3 `apply()` calls in `R/metrics-sample.R` could benefit. Suggest splitting into two sub-tasks, prioritizing `Metrics` removal as a quick win. | None blocking. Removing `Metrics` requires updating vignettes (`scoring-rules.Rmd`) and tests (`test-metrics-binary.R`) that reference `Metrics::` functions. | (1) Replace `Metrics::ae/se/ape` with internal one-liners in `R/class-forecast-point.R`, remove from DESCRIPTION/NAMESPACE, update vignettes and tests. (2) Optionally evaluate adding `matrixStats` for 3 `apply()` calls — lower value, separate decision. | [x] | [ ] | — | [ ] |\n| 823 | Reduce number of `try()`, `suppressMessages()`, `suppressWarnings()` used in the package | | 2024-05-19 | refactor | medium | medium | no | Still relevant. 8 direct `try()` calls + 6 indirect via `check_try()` + 1 `suppressWarnings(suppressMessages(...))` in R/ code. Usage has slightly increased since filing due to new `[.forecast` subsetting operators (PR #884). Each `try()` exists for a reason; removal requires architectural rethinking of validation flow, not simple find-and-replace. | Deeply linked to #935 (print on column change), #840 (input check cleanup), #939 (remove `check_input_*`), #885 (export decisions), #811 (batch warnings). | Address incrementally: (1) Replace blanket `try()` in subsetting operators with targeted `tryCatch()`, (2) add `verbose` params to internal validation paths to eliminate `suppress*()`, (3) replace `check_try()` with proper `check_*()` functions as part of #840. | N/A | N/A | — | N/A |\n| 811 | Should `score()`/`apply_metrics()` return warning about failed metrics in a batch? | enhancement | 2024-05-07 | enhancement | low | small | yes | Still relevant. `run_safely()` in `R/score.R:210` uses `try()` + immediate `cli_warn()` per failed metric. No batching/deferring mechanism exists. Failed metrics silently dropped from `scores` object via `as_scores()` filtering. Blog post on deferred errors referenced in issue. Only 1 comment from maintainer (\"I like this idea\"). | None blocking. Tangentially related to #823 (reduce `try()`) and #382 (metric/column name clash warnings). | Modify `apply_metrics()`/`run_safely()` to collect error messages in a list during the `lapply` loop, then emit a single batched `cli_warn()` summarizing all failed metrics at the end. Update test expectations in `test-score.R`. | [x] | [ ] | — | [ ] |\n| 810 | Clean up readme by using `pkgdown` to render dev version docs | | 2024-05-05 | documentation | low | small | yes | CLOSED (completed 2026-02-09). PR #1060 merged today, replacing R-based `packageVersion()` detection with pkgdown selective HTML (`.pkgdown-devel`/`.pkgdown-release` fenced divs). `_pkgdown.yml` already had `development: mode: auto`. Dev docs deployed at `/dev/` on gh-pages. | None. Fully resolved. | No action needed — issue closed via merged PR #1060. | N/A | N/A | — | N/A |\n| 808 | Run lintr everywhere | | 2024-05-05 | cleanup | low | medium | partially | Active work in progress. PR #1062 (\"enable lintr for tests, inst, and vignettes\") is open but not yet merged. `.lintr` excludes all `tests/`, `inst/`, and `vignettes/*.R` files. CI only lints changed files on PRs. Main scope is `tests/` (~5,400 lines across 35 files). `inst/` has only 2 R files. `vignettes/*.R` not tracked in git (auto-generated tangles). ~90 `nolint` annotations already exist in `R/` source. | #807 (cli nolint cleanup) should be done first to establish pattern. #513 (vignette .R scripts) related to vignette exclusion. PR #1062 is in flight. | Merge or rebase PR #1062 once reviews pass. May need to fix remaining lint violations or add targeted `# nolint` annotations. Address `tests/` directory-wide lint cleanup separately from `inst/` and `vignettes/`. | N/A | N/A | — | N/A |\n| 807 | Clean up linter statements around `cli` conditions | good first issue, implementation-ready | 2024-05-05 | cleanup | low | medium | yes | Still relevant. ~43 `#nolint start: keyword_quote_linter` blocks across R/ wrap `cli_abort()`/`cli_warn()` calls that use `\"!\" =`, `\"i\" =` named vector syntax. Only 12 instances use Hugo's backtick fix (`\\`!\\` =`). The fix is mechanical: replace `\"!\" =` with `` `!` = `` inside cli condition calls, then remove the `nolint` annotations. Additional `object_usage_linter` suppressions (14 blocks) exist for `.data[...]` patterns in data.table code — these need separate handling. No work done on this issue since filing. | Prerequisite for #808 (run lintr everywhere). Related to PR #583 (which introduced the cli pattern). | Replace all `\"!\" =`, `\"i\" =`, `\"x\" =`, `\"v\" =` with backtick-quoted equivalents (`` `!` = ``, etc.) in `cli_abort()`/`cli_warn()` calls across ~20 R/ files, then delete the corresponding `#nolint start`/`#nolint end` blocks. Handle `object_usage_linter` suppressions separately. | [x] | [ ] | — | [ ] |\n| 793 | How should we document methods that have different arguments than the default method? | | 2024-04-19 | documentation | low | small | yes | Still relevant. Consensus reached (separate pages per @seabbs), but not implemented. Three generics currently lump methods with differing arguments onto one `@rdname` page: `as_forecast_quantile` (`.forecast_sample` has different args), `as_forecast_point` (`.forecast_quantile`), and `get_pit_histogram` (`.forecast_sample` has extra args). Renders a confusing flat argument list. | None. | Remove shared `@rdname` tags for S3 methods whose arguments differ from the generic (`as_forecast_quantile.forecast_sample`, `as_forecast_point.forecast_quantile`, `get_pit_histogram.forecast_sample`) and give each its own documentation page with proper title and description. | [x] | [ ] | — | [ ] |\n| 789 | Add documentation about how to customise scores | documentation | 2024-04-13 | documentation | medium | medium | partially | Partially addressed. The `score()` roxygen docs now have a \"Customising metrics\" section with `purrr::partial()` examples (PR #874). But the core ask — a vignette walkthrough — remains undone. No vignette covers `get_metrics()` → `select_metrics()` → `purrr::partial()` → `score()` workflow. On `scoringutils-2.x` milestone. | Related to #546 (vignette restructuring — decides where customization docs go), #666 (manuscript update). | Create a vignette section (or standalone vignette) walking users through the full metric customization workflow: `get_metrics()`, `select_metrics()`, `purrr::partial()`, custom metric functions, and passing to `score()`. Coordinate with vignette restructuring in #546. | N/A | N/A | — | N/A |\n| 785 | Small updates to the scoring rules Vignette | documentation | 2024-04-10 | documentation | low | small | yes | Still relevant. None of the 7 suggestions from @seabbs's review of PR #762 have been implemented: author field still present, Google Drive link still used, no citations for proper scoring rules or transformed scores paper, `get_metrics` wording not improved for website readers, no introductory paragraph on different probabilistic forecast types. Nearly 2 years open with no progress on these items. | Loosely related to #666 (manuscript update re: Google Drive link), #758 (scoring rules vignette parent issue), #1046 (multivariate scoring in same vignette). | Implement the 7 small documentation edits in `vignettes/scoring-rules.Rmd`: remove author field, replace Google Drive link with proper reference, add citations for proper scoring rules and transformed scores, improve `get_metrics` wording, clean whitespace, and add intro paragraph on probabilistic forecast types. | [x] | [ ] | — | [ ] |\n| 780 | Add a `verbose` argument to `as_forecast()` | | 2024-04-05 | enhancement | low | small | yes | Still relevant. `verbose` already exists on `assert_forecast()` and `assert_forecast_generic()` (PR #778) but has not been threaded through to any `as_forecast_<type>()` constructor. ~11 test lines and 1 internal R source line use `suppressMessages`/`suppressWarnings` wrappers around `as_forecast_*` calls, confirming ongoing need. Author acknowledged low urgency but wants it for test cleanup. | None. PR #778 (verbose on `assert_forecast`) already merged; plumbing is in place. | Add `verbose = TRUE` parameter to each `as_forecast_<type>.default()` method, pass it to their `assert_forecast()` calls, guard direct `cli_warn()`/`cli_inform()` calls behind the flag, and replace `suppressMessages`/`suppressWarnings` wrappers in tests and internal code with `verbose = FALSE`. | [x] | [ ] | — | [ ] |\n| 769 | Rethink the way arguments are passed in `get_pairwise_comparisons()` | package improvement | 2024-04-03 | refactor | low | small | yes | Still relevant. `test_type`, `one_sided`, and `n_permutations` are still passed via `...` through 3 levels (`get_pairwise_comparisons` → `pairwise_comparison_one_group` → `compare_forecasts`). Users must read internal `compare_forecasts` docs to discover options. PR #994 added `test_type = NULL` but didn't address the `...` pattern. #750 (split out p-values) was deferred. | Loosely related to #750 (if p-values split out, this becomes moot — but #750 deferred indefinitely). No hard blockers. | Promote `test_type`, `one_sided`, and `n_permutations` from `...` to explicit named arguments in `get_pairwise_comparisons()` and `add_relative_skill()`, improving API discoverability. Update roxygen `@param` tags accordingly. | [x] | [ ] | — | [ ] |\n| 763 | We should check for consistency when scoring binary forecasts | feature request, high-priority | 2024-03-30 | enhancement | medium | small | yes | Still relevant. No code warns when 0/1 factor levels are in counterintuitive order (e.g., `levels = c(1, 0)` making `0` the reference level). `assert_input_binary()` only validates factor with 2 levels and predicted in [0,1]. No helper for safe factor conversion exists. @seabbs questioned whether this is must-have for 2.0 or nice-to-have. | Loosely related to #840 (input check cleanup) and #939 (remove `check_input_*` functions). | Add a check in `assert_input_binary()` that warns when observed values are \"0\"/\"1\" and level order is `c(1, 0)`. Optionally create `as_binary_observed()` helper with a `reference_level` argument for safe factor conversion. | [x] | [ ] | — | [ ] |\n| 759 | Future of the current `scoringutils.Rmd` vignette | documentation, question | 2024-03-27 | documentation | medium | medium | partially | Partially addressed. The outdated `scoringutils.Rmd` was removed in PR #836 (May 2024). However, the promised replacement \"Get started\" vignette was never written — @seabbs volunteered June 2024 but no PR materialized. Package has no introductory vignette. README also has dead links to deleted vignettes (`metric-details`, `scoring-forecasts-directly`). | Related to #546 (simplify getting started). Completing this would likely close #546. Dead README links should be fixed regardless. | Write a new \"Get started\" / quick-start vignette to replace the deleted `scoringutils.Rmd`. Fix dead vignette links in README.Rmd. Consider merging #546 into this issue. | N/A | N/A | — | N/A |\n| 758 | Create a vignette that explains scoring rules | documentation | 2024-03-27 | documentation | medium | large | partially | Partially addressed. Part 1 (explain each scoring rule) is covered by the existing `scoring-rules.Rmd` vignette. Part 2 (practical guidance on choosing a scoring rule — convergence, overconfidence, locality, scale sensitivity) remains unaddressed. The issue body contains a detailed vignette stub. Three illustration PNGs exist in `vignettes/` but are orphaned (not referenced by any vignette). | Parent issue for #929, #930, #933, #934 (illustration issues). Related to #785 (scoring rules vignette updates), #1046 (multivariate content). | Narrow scope to creating a \"Choosing a scoring rule\" vignette covering comparative guidance (convergence, overconfidence, locality, scale sensitivity). Coordinate with #929/#930/#933/#934 to incorporate the orphaned illustrations. | N/A | N/A | — | N/A |\n| 752 | Split up `plot_pit()` into a method for vectors and one for data.frames | | 2024-03-27 | refactor | low | small | yes | Fully resolved. `plot_pit()` was removed entirely in PRs #940 and #949. Replaced by `get_pit_histogram()` (S3 generic with methods for `forecast_sample` and `forecast_quantile`) and `pit_histogram_sample()` for low-level computation. Plotting is now via standard ggplot2 calls. The dual-purpose if/else concern no longer exists. | None. | Close as completed. The issue's concern was superseded by a complete removal of `plot_pit()` in favor of `get_pit_histogram()` with proper S3 dispatch. | N/A | N/A | — | N/A |\n| 750 | Move computation of p-values out of `get_pairwise_comparisons()`? | question | 2024-03-26 | discussion | low | medium | no | Partially addressed. PR #994 added `test_type = NULL` to skip p-value computation, resolving the main pain point (spurious warnings from `add_relative_skill()`). The broader architectural question of separating p-value from score ratio computation remains open but contributor consensus was option 1 (do nothing). | #769 (explicit test_type argument) should be considered together. #1022 (two-model case) tangentially related. | Consider closing as won't-fix or converting to long-term design discussion. The practical problem was solved by PR #994. If kept open, the most impactful remaining change would be defaulting `add_relative_skill()` to `test_type = NULL` since it discards p-values anyway. | N/A | N/A | — | N/A |\n| 740 | Make `get_forecast_unit()` and friends customisable such that they can work with new forecast classes | enhancement | 2024-03-24 | refactor | medium | medium | partially | Still relevant. No implementation work done. `get_protected_columns()` remains a plain function with hard-coded column lists (not S3). `get_duplicate_forecasts()` still hard-codes type-specific columns. Related S3 conversions (e.g., `get_forecast_type`, `get_pit`, `as_forecast`) establish the pattern. On `scoringutils-2.x` milestone. | Near-duplicate of #889 (S3 `get_protected_columns`) and #888 (S3 `get_duplicate_forecasts`). Blocks #852 (custom forecast class workflow). | Convert `get_protected_columns()` into an S3 generic with per-class methods (addressing #889), similarly make `get_duplicate_forecasts()` S3 (#888), so each forecast class declares its own columns. Follow established pattern from completed S3 conversions. | N/A | N/A | — | N/A |\n| 731 | Should `wis()` warn about missing values? | question | 2024-03-23 | discussion | low | small | no | Still relevant. Current behavior is correct — `as_forecast()` emits a vague NA message, `clean_forecast()` strips NA rows before `wis()` sees them. Discussion converged on improving the `as_forecast()` message (#537) as the primary fix, with `wis()`-specific warning being secondary. No consensus reached on implementation. | Related to #537 (high-priority: improve NA-removal message). Related to #542 (debug `score()`). Resolving #537 first would clarify what remains here. | Resolve #537 first (improve NA-removal message in `as_forecast()` to report affected row count). Then decide whether `wis()` needs its own warning for direct (non-`score()` pipeline) usage. May be closeable once #537 is done. | N/A | N/A | — | N/A |\n| 718 | Testing fails locally when done repeatedly | enhancement | 2024-03-20 | bug | low | small | yes | Still relevant and unfixed. Root cause: `cli_warn()` with `.frequency = \"once\"` and `.frequency_id = \"small_interval_range\"` suppresses warnings after first test run in same R session. 4 instances of `.frequency = \"once\"` in codebase. Tests pass on CI (fresh process) and on first local run. | No blocking dependencies. Tangentially related to #935 (surprising output behavior). | Add `rlang::reset_message_verbosity(\"small_interval_range\")` before the `expect_warning()` call in `test-metrics-interval-range.R`. Audit the other 3 `.frequency = \"once\"` sites to ensure their tests are similarly resilient. | [x] | [ ] | — | [ ] |\n| 713 | Create vignette that showcases different ways to visualise forecasts and scores | documentation | 2024-03-11 | documentation | medium | medium | no | Still relevant. Package has 7 exported plot functions (`plot_wis`, `plot_heatmap`, `plot_interval_coverage`, `plot_quantile_coverage`, `plot_pairwise_comparisons`, `plot_forecast_counts`, `plot_correlations`) plus `theme_scoringutils()` but no dedicated visualization vignette. Only a deprecated-visualisations vignette exists. Issue requests updated `plot_score_table()` and mention of `hubVis`. | Related to #826 (plot_heatmap accessibility), #49 (calibration plotting), #942 (discrimination plots), #356 (interactions vignette). Does not block other issues. | Create a new `vignettes/visualising-scores.Rmd` with worked examples for all exported plot functions using example data. Include mention of `hubVis` as alternative for forecast visualization (per closed #812), and an updated `plot_score_table()` equivalent. | N/A | N/A | — | N/A |\n| 702 | Update docs to conform to tidyverse styleguide | good first issue | 2024-03-08 | documentation | low | small | partially | Still relevant. ~85% of `@param` tags already conform to sentence case. ~21 clearly non-conforming (lowercase start) and ~21 special-character-starting descriptions across ~15 R files. Blocker PR #705 (file reorg) was merged March 2024. Assigned to @jamesmbaazam but no PR submitted. | No blocking dependencies. Standalone cleanup task. PR #705 dependency resolved. | Fix ~21 `@param` descriptions across ~15 R files to start with capital letter and end with full stop per tidyverse style guide. Also audit ~21 special-character-starting descriptions and all `@return`/`@returns` tags for same conformance. | N/A | N/A | — | N/A |\n| 686 | Improve code coverage | | 2024-03-06 | meta | low | small | partially | Largely resolved. Coverage is at 98%. The specific sub-issue (#703, misnamed test file) was fixed in PR #704. Only `theme_scoringutils()` (18 lines) lacks any test coverage. The remaining 2% is edge-case branches. Could be closed as-is. | Sub-issue #703 resolved. No blockers. | Optionally add a test for `theme_scoringutils()` and a `.codecov.yml` with a coverage target to prevent regressions, then close this issue given the excellent 98% coverage level. | N/A | N/A | — | N/A |\n| 685 | Suppress printing in tests | enhancement, implementation-ready | 2024-03-06 | cleanup | medium | medium | partially | Still relevant. 29 `suppressMessages()` and 16 `suppressWarnings()` remain across 13 test files, silently swallowing conditions instead of asserting them. Partially addressed: 80+ `expect_no_condition()` calls already exist. Root causes include `as_forecast_*()` verbose messaging and the #935 print bug. | Partially depends on #935 (unwanted printing on `:=`). Overlaps significantly with #823 (reduce `suppress*()` in package). | Audit all 29 `suppressMessages()` and 16 `suppressWarnings()` in tests: replace with `expect_message()`/`expect_warning()` for intentional conditions, pass `verbose = FALSE` where available, or use `expect_no_condition()` after fixing the source. Also review `setup.R` score calls. | N/A | N/A | — | N/A |\n| 684 | Delete functions `check_input_sample()`, `check_input_quantile()`, `check_input_interval()` | | 2024-03-06 | cleanup | low | small | yes | Still relevant but fully superseded by #939, which covers all 5 `check_input_*` functions. These 3 functions are dead code: never called in production, not exported, only tested. A comment in `check-input-helpers.R` explicitly marks them for future deletion. Should be closed as duplicate of #939. | Superseded by #939. Related to #840 (input check cleanup) and #823 (reduce `try()`). | Close as duplicate of #939. If handled independently, delete the 3 function definitions, their man pages, roxygen cross-references, and corresponding test blocks. | [x] | [ ] | — | [ ] |\n| 668 | Add argument to control what kind of coverage is computed by `get_coverage()` | feature request | 2024-02-29 | enhancement | low | small | yes | Still relevant and unimplemented. `get_coverage()` always computes all 4 coverage metrics with no user control. The issue body contains a near-complete code proposal (with a minor bug to fix). The function structure already cleanly separates interval and quantile computation paths. On `scoringutils-2.x` milestone. | No blocking dependencies. Standalone enhancement. | Add a `type = c(\"quantile\", \"interval\")` argument to `get_coverage()` that conditionally computes interval coverage, quantile coverage, or both. Update downstream column handling, summarisation, and tests. | [x] | [ ] | — | [ ] |\n| 667 | Create print method for `scores()` objects | feature request | 2024-02-29 | feature | low | small | partially | Still relevant but effectively blocked by #935. No `print.scores` method exists; scores objects fall through to `print.data.table`. Adding a custom print method would exacerbate the unwanted-printing-on-`:=` bug documented in #935. Both maintainers expressed lukewarm interest, deferring to post-release. | Blocked by #935 (print on column change bug). Related to #975 (link to score definitions in print). | Implement `print.scores()` S3 method displaying computed metric names (from `metrics` attribute) before delegating to `NextMethod()`, following the `print.forecast` pattern. Wait until #935 is resolved first to avoid spurious printing on `:=` operations. | N/A | N/A | — | N/A |\n| 666 | Update the scoringutils paper that accompanies the package | question | 2024-02-29 | documentation | medium | large | no | Still relevant but partially stale. Only 2 of ~15 checklist items done. Manuscript in `inst/manuscript/` still references `magrittr`, CITATION still points to 2022 arXiv preprint. README links to Google Drive PDF. Some checklist items reference functions no longer exported. Should be broken into smaller actionable issues. | Related to #759, #789, #546, #758, #1057. | Audit the to-do list against current API to remove stale items, break remaining unchecked items into individual issues, and decide whether to prioritize the \"make a vignette out of the paper\" item before the next CRAN release. Requires human authorship decisions. | N/A | N/A | — | N/A |\n| 646 | Stop converting errors to warnings in `score()`? | help wanted, question, Breaking change | 2024-02-23 | refactor | medium | medium | partially | Still relevant. `run_safely()` in `R/score.R` still converts all metric errors to warnings via `try()` + `cli_warn()` and returns `NULL`, silently dropping failed metrics. Deferred to `scoringutils-2.x` milestone. No consensus on approach: let errors propagate (breaking), add `on_error` parameter (compatible), or batch warnings (#811). | Related to #811 (batch warnings), #941 (`score()` redesign), #823 (reduce `try()`). | Design decision needed: (a) let errors propagate as errors, (b) add `on_error = c(\"warn\", \"error\")` param to `score()`, or (c) combine with #811's batch-warning approach. Implementation touches `run_safely()` in `R/score.R` and test expectations. | N/A | N/A | — | N/A |\n| 625 | Implement a `get_unique_values()` function | implementation-ready | 2024-02-10 | enhancement | low | small | yes | Still relevant. No implementation work done. Function does not exist. Would compute `length(unique(values))` per forecast-unit column (via `get_forecast_unit()`), optionally grouped by model. Similar diagnostic purpose to `get_forecast_counts()` but different output. | Loosely related to #581 (NA in print), #935 (print bug), #621 (closed, prettier print). No hard blockers for standalone function. | Implement a new exported `get_unique_values()` function that takes a forecast object, calls `get_forecast_unit()` to identify relevant columns, and returns a summary table of unique value counts per column. Optionally integrate into `print.forecast()`. | [x] | [ ] | — | [ ] |\n| 608 | Define input format for categorical forecasts | implementation-ready | 2024-01-17 | discussion | low | small | no | Largely resolved. Input format fully defined and implemented in `forecast_nominal` (PR #837) and `forecast_ordinal` (PR #977). Column named `predicted_label`. Strict completeness enforced. Only unimplemented item is a helper function to auto-expand incomplete forecasts with zero-probability rows, which has no dedicated tracking issue. Can be closed. | #604 (parent, closed), #607 (naming, closed). No blockers. | Close this issue as completed. Optionally open a new targeted issue for the helper function that auto-expands incomplete forecasts with zero-probability rows if still desired. | N/A | N/A | — | N/A |\n| 581 | Add the number of `NA` values to the planned print method for `forecast_*` objects, rather than throw a message? | feature request | 2024-01-05 | enhancement | medium | medium | partially | Still relevant. None of the three requested features implemented: `print.forecast()` shows no NA info, NA message in `assert_forecast_generic()` is still vague (no row count), no `get_NA()` helper exists. Issue #537 is a near-duplicate and both should be addressed together. | Near-duplicate of #537 (should be resolved together). Tangentially related to #731, #542, #935 (print bug). | Enhance `print.forecast()` to display NA row count, improve the message in `assert_forecast_generic()` to report exact number of NA rows, and consider adding a `get_na_summary()` helper. Consolidate with #537. | N/A | N/A | — | N/A |\n| 557 | sample_to_quantile.numeric method | low-priority | 2024-01-05 | enhancement | low | small | yes | Effectively resolved. `sample_to_quantile()` was replaced by `as_forecast_quantile.forecast_sample()` in PR #876. The motivating code (`interval_coverage_sample()`) was deleted entirely. No current code needs a `.numeric` method. Can be closed. | Tangentially related to #1026 (reverse conversion). No blockers. | Close this issue as resolved. The refactoring in PR #876 and deletion of `interval_coverage_sample()` eliminated the code pattern that motivated this request. | N/A | N/A | — | N/A |\n| 546 | Simplify \"Getting Started\" \\| Split into smaller chunks | documentation | 2024-01-05 | documentation | medium | medium | no | Still relevant. The old `scoringutils.Rmd` was deleted (PR #836) but the promised replacement \"Get started\" vignette was never written. Package currently has no introductory vignette at all. README has a \"Quick start\" section but no pkgdown \"Get started\" page. @seabbs volunteered in #759 but no PR materialized. | Tightly coupled with #759 (future of scoringutils.Rmd) — should be resolved together. Related to #789 (metric customization docs). | Write a new introductory \"Getting Started\" vignette (as `vignettes/scoringutils.Rmd` for pkgdown) providing a concise end-to-end walkthrough. Can close both #546 and #759 simultaneously. Requires editorial decisions on scope and content. | N/A | N/A | — | N/A |\n| 542 | Improve ways to debug `score()` | question | 2024-01-04 | enhancement | medium | medium | partially | Still relevant. Neither action item is done: (1) no vignette documents `debugonce()` for scoring rule debugging, (2) `run_safely()` still only reports the metric name in warnings, not which forecast unit/row caused the failure. The `{cli}` migration (PR #583) improved formatting but not informativeness. | Closely related to #646 (stop converting errors to warnings — open, breaking change). Related to closed #391, #418, #543, #583. | Two tasks remain: (1) add documentation showing `debugonce()` usage for debugging scoring failures, and (2) enhance `run_safely()`/`apply_metrics()` to include forecast-unit context in warning messages. Coordinate error-vs-warning design with #646. | N/A | N/A | — | N/A |\n| 537 | Inform users about the number of rows that have NA values | good first issue, question, implementation-ready, high-priority | 2024-01-03 | enhancement | medium | medium | partially | Still relevant. The message in `assert_forecast_generic()` (line 150 of `R/class-forecast.R`) still says the generic \"Some rows containing NA values may be removed\" without a row count. No NA info in `print.forecast`, no `get_na()` helper. The `anyNA()` check and `na.omit()` call are already in place, so computing the count is trivial. | Near-duplicate of #581 (NA count in print method) — should be resolved together. Related to #731 (wis NA warnings). | Minimal fix: add NA row count to the existing `cli_inform` message in `assert_forecast_generic()`. For a more complete resolution, combine with #581 to also add NA summary to `print.forecast` and potentially create a `get_na()` diagnostic helper. | N/A | N/A | — | N/A |\n| 522 | Create a way to test user-provided scoring functions | feature request | 2023-12-06 | feature | medium | medium | partially | Still relevant. No progress made. `validate_metrics()` only checks that items are functions (via `checkmate::check_function`), not signature compatibility, output shape, or runtime behavior. Custom metric errors are silently converted to warnings by `run_safely()`, making debugging hard. Each of the 6+ forecast types passes different arguments to metrics. | Loosely related to #362 (non-standard metric I/O), #823 (reduce `try()`). | Implement an exported `test_metrics()` function that runs each metric on a small representative subset of a forecast object and reports informative diagnostics: signature compatibility, runtime errors, and output shape validation. Needs design decision on API. | N/A | N/A | — | N/A |\n| 520 | Create documentation/vignette for the naming scheme of our functions | help wanted, question | 2023-11-30 | documentation | low | medium | partially | Still relevant. The issue body already describes the naming scheme in detail but no vignette or formal documentation exists. CONTRIBUTING.md only references tidyverse style guide. Issue #948 (closed) debated `get_` prefix but decided to keep it. 65 exported functions follow conventions consistently. ~2 years old with no implementation. | Related to closed #948, #610, #476, #357. No blockers. | Create a developer-facing vignette or CONTRIBUTING.md section formalizing the naming conventions (scoring rules: `<name>_<type>`, `get_*`, `as_forecast_*`, `is_forecast_*`, `plot_*`, `add_*`, `assert_`/`check_`/`test_`). Update to reflect current classes including `forecast_nominal`, `forecast_ordinal`, `forecast_sample_multivariate`. | N/A | N/A | — | N/A |\n| 513 | Investigate R scripts that get generated based on our vignettes | | 2023-11-28 | cleanup | low | small | yes | Already resolved. The `.R` and `.html` vignette artifacts (generated by `knitr` tangle) are gitignored via `vignettes/.gitignore`, excluded from linting via `.lintr`, and not tracked in git. All three mitigations from the discussion are in place. Issue can be closed. | None. | Close this issue — all requested investigation and mitigations (gitignore, buildignore, lint-ignore for vignette-generated `.R` files) are already implemented. No code changes needed. | N/A | N/A | — | N/A |\n| 492 | Expose `get_range_from_quantile()` to the user | question | 2023-11-22 | enhancement | low | small | yes | Still relevant. Function exists at `R/helper-quantile-interval-range.R:185` as `@keywords internal`, not exported. Maintainer consensus reached: rename to `get_interval_range()` and create `add_interval_range()` wrapper. Neither implemented. Original motivating function `plot_ranges()` was deleted, reducing urgency. 3 internal call sites. | Parent meta-issue #484. Tangentially related to #885 (export decisions). | Rename `get_range_from_quantile()` to `get_interval_range()`, export it, and create a new exported `add_interval_range()` that adds an `interval_range` column to a quantile-format data.table. Update 3 internal call sites, NAMESPACE, docs, and tests. | [x] | [ ] | — | [ ] |\n| 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |\n| 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |\n| 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |\n| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |\n| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n\nTRIAGE_COMPLETE","structuredPatch":[{"oldStart":102,"oldLines":7,"newStart":102,"newLines":7,"lines":[" | 484 | Discussion: Functions to expose to users | | 2023-11-19 | meta | low | small | partially | Partially done. 1 of 3 tracked items complete (`get_metrics()` is already exported). Remaining: `get_protected_columns()` still internal (`@keywords internal` in `R/get-protected-columns.R`), `get_range_from_quantile()` still internal (blocked on naming decision in #492). ~2 years old. | #492 (naming decision blocks `get_range_from_quantile` export). No blockers for `get_protected_columns`. | Update checkboxes: tick `get_metrics()` as done. Export `get_protected_columns()` (add `@export`, remove `@keywords internal`). Resolve #492 naming decision before exporting `get_range_from_quantile`. Consider closing in favor of tracking remaining items individually. | N/A | N/A | — | N/A |"," | 459 | Meta-issue: Code refactoring | refactor | 2023-11-17 | meta | low | medium | partially | 2 of 5 tracked sub-issues done (#446, #469). Of remaining 3: #411 is stale (function `interval_coverage_sample` removed), #454 needs updating (named function gone but duplication between `get_coverage()` and `interval_coverage()` persists), #445 still actionable (64 `::` occurrences across 18 R files). Untracked dimension-checking item partially addressed by existing `assert_input_*` pattern. 2+ years old. | #938 (rename `assert_dims_ok_point`) related to untracked item. #870 (CRPS/WIS refactor) is related open refactoring work not tracked here. | Update the issue body: close #411 as stale, update #454's description to reference current duplication, and decide whether to close this meta-issue in favor of tracking remaining refactoring (#445, #870, #938) as standalone issues. | N/A | N/A | — | N/A |"," | 455 | Discussion: Should scoring rules always return a vector? | Breaking change | 2023-11-15 | discussion | low | large | no | Stale discussion from Nov 2023 with only 2 comments and no concrete proposal. The current design (separate functions per metric component, e.g., `overprediction_quantile()`) already addresses the motivating example. All metrics consistently return numeric vectors; `apply_metrics()` requires this. API is settled. | None | Close as \"won't fix\". The vector-return contract is consistent across all metrics and well-integrated with `apply_metrics()`. The separate-function-per-component pattern already solves the motivating WIS use case. | N/A | N/A | — | N/A |","-| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [ ] | [ ] | — | [ ] |","+| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [x] | [ ] | — | [ ] |"," | 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |"," | 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |"," | 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01JayNCb46jZPeSbxRP1yB9R","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check if all automatable=yes issues with Has Tests not N/A are now `[x]`. Let me verify the remaining ones (#445, #412, #406, #382, #363, #360, #193)."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":646,"cache_read_input_tokens":71018,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":646},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"e60a12a2-62cb-499a-89f4-4fc9973d511e"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01JayNCb46jZPeSbxRP1yB9R","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01E8yXxwZ5qUZ1ty2BLQoYxk","name":"Read","input":{"file_path":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","offset":105,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":646,"cache_read_input_tokens":71018,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":646},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"fe5745fd-f6bd-44ec-820a-4b61d400ce54"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01E8yXxwZ5qUZ1ty2BLQoYxk","type":"tool_result","content":"   105→| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [x] | [ ] | — | [ ] |\n   106→| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n   107→| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n   108→| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n   109→| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n   110→| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n   111→| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n   112→| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n   113→| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n   114→| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n   115→| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n   116→| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n   117→| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n   118→| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n   119→| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n   120→| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n   121→| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n   122→| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n   123→| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n   124→| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n   125→| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n   126→| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n   127→| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n   128→| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n   129→| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n   130→\n   131→TRIAGE_COMPLETE"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"83f84abe-53ff-4b7a-9ff0-9b6b22d99b00","tool_use_result":{"type":"text","file":{"filePath":"/Users/nikos/github-synced/scoringutils-pipeline/.ralph/ISSUE_TRIAGE.md","content":"| 454 | Code refactoring: Deduplicate code in `get_coverage()` and `interval_coverage_deviation_quantile()` | refactor, implementation-ready | 2023-11-15 | refactor | low | small | yes | Original function `interval_coverage_deviation_quantile()` was removed. Issue title is outdated. Remaining duplication is between `get_coverage()` (data.table-level) and `interval_coverage()` (numeric scoring metric) — both call `quantile_to_interval()` + bounds check independently. Duplication is small (~5 lines of core logic). | Tracked in meta-issue #459. Related to #668 (coverage type argument). | Update issue title to reference `interval_coverage()`. Refactor `get_coverage()` to delegate interval coverage computation to `interval_coverage()` or a shared internal helper, eliminating duplicated `quantile_to_interval()` + bounds-check pattern. | [x] | [ ] | — | [ ] |\n| 453 | Create function `quantile_coverage_deviation()` | feature request, implementation-ready | 2023-11-15 | feature | low | small | no | Superseded. The requested functionality is already provided by `get_coverage()` (line 88 of `R/get-coverage.R` computes `quantile_coverage_deviation`). The analogous standalone function (`interval_coverage_dev_quantile()`) was removed rather than duplicated. Can be closed. | Related to #454 (also likely closeable) and meta-issue #459. | Close as superseded. Quantile coverage deviation is already computed in `get_coverage()`. The design decision was to consolidate coverage metrics there rather than create standalone functions. | N/A | N/A | — | N/A |\n| 451 | Deal with one-to-one functions related to quantile and interval metrics | question | 2023-11-15 | discussion | low | medium | no | Still relevant. The issue concerns the one-to-one correspondence between quantile-format and interval-format metric functions. Currently: `interval_score()` is internal (not exported), called by `wis()` after `quantile_to_interval()` conversion. Exported quantile metrics: `wis()`, `quantile_score()`, `interval_coverage()`, `ae_median_quantile()`, `bias_quantile()`, `dispersion_quantile()`, `overprediction_quantile()`, `underprediction_quantile()`. No `forecast_interval` class exists — interval format is internal only. The comment from the maintainer (\"not expose functions to users for now and rethink later\") is the current policy. Coverage code in `get_coverage()` still has its own inline computation rather than reusing `interval_coverage()`. | Related to #668 (coverage type argument), #492 (expose `get_range_from_quantile`), #870 (CRPS/WIS decomposition refactor), #907 (coverage function factory), #454 (deduplicate coverage code), #484 (functions to expose). | Keep open as a design discussion for future releases. The immediate question (whether to expose interval-format functions) was deferred. The broader question of code reuse between `get_coverage()` and `interval_coverage()` overlaps with #454 and #668. Revisit when planning the next API refactor cycle. | N/A | N/A | — | N/A |\n| 445 | Cleanup: delete calls that use `::` or `:::` and add to imports instead | refactor, implementation-ready | 2023-11-15 | cleanup | low | small | yes | Still relevant. 25 redundant `pkg::fn()` calls across 9 R files where functions are already imported in NAMESPACE (13 data.table, 5 scoringRules, 3 Metrics, 2 cli, 1 purrr, 1 utils). The oft-cited \"64 occurrences\" overstates scope — 37 are roxygen/comment references that should stay. Purely cosmetic cleanup. | None. Standalone cleanup. Tracked in meta-issue #459. | Remove the `pkg::` prefix from 25 redundant function calls across 9 R files. Leave roxygen documentation cross-references and cli-formatting self-references unchanged. | [ ] | [ ] | — | [ ] |\n| 434 | Discussion: composing score data.frames | help wanted, question | 2023-11-14 | discussion | low | medium | partially | Still relevant. No `c.scores()` or score composition method exists. Scores objects are data.tables with a `metrics` attribute. A `c.scores()` method would need `rbindlist(..., fill = TRUE)` and union of `metrics` attributes. No comments on the issue; design not discussed beyond the example in the issue body. ~2.5 years old with no progress. | Sub-issue of #433. Depends on #432 (as_point) being resolved first to enable the full workflow. | Design and implement a `c.scores()` (or `merge_scores()`) method that combines multiple scores objects via `rbindlist(fill = TRUE)`, merges `metrics` attributes, and validates that forecast unit columns are compatible. Needs maintainer input on API design. | N/A | N/A | — | N/A |\n| 433 | Meta-issue - Feature: scoring probabilistic and point forecasts together / composing scores | help wanted | 2023-11-14 | meta | low | medium | no | Neither sub-issue resolved. PR #790 (implementing `as_point`/`as_quantile`) was closed without merging in May 2024. Only `as_forecast_point.forecast_quantile()` exists (hard-coded to median). No `as_forecast_point.forecast_sample()` method. No score composition mechanism. ~2.5 years old with stalled progress. | Tracks #432 (as_point) and #434 (composing scores). Related to #880 (customizable quantile in as_forecast_point). | Complete both sub-issues: (1) implement `as_forecast_point.forecast_sample()` and optionally a user-friendly `as_point()` wrapper, (2) implement score composition via `c.scores()` or similar. Needs fresh design decision given closed PR #790. | N/A | N/A | — | N/A |\n| 432 | Create function `as_point()` that converts sample- or quantile-based forecasts into point forecasts | help wanted | 2023-11-14 | feature | medium | medium | partially | Partially addressed. `as_forecast_point.forecast_quantile()` exists (hard-coded to median, lines 105-120 of `R/class-forecast-quantile.R`). `as_forecast_point.forecast_sample()` does NOT exist. PR #790 attempted full `as_point()`/`as_quantile()` implementation but was closed without merging (May 2024). @seabbs deprioritized as \"nice to have\". | Sub-issue of #433. Related to #880 (customizable quantile_level in as_forecast_point). | Implement `as_forecast_point.forecast_sample()` (summarize samples via median/mean/custom function). Optionally create a convenience `as_point()` generic wrapping `as_forecast_point()`. Revisit closed PR #790 for design reference. | N/A | N/A | — | N/A |\n| 412 | Add check to `validate_forecast()` that predictions must not decrease with quantile | implementation-ready | 2023-11-10 | enhancement | medium | small | yes | Still relevant. Monotonicity check exists only as a hard error deep in `bias_quantile_single_vector()` (R/metrics-quantile.R:495). `assert_forecast.forecast_quantile()` does NOT check monotonicity — it only validates column presence, quantile range [0,1], and type. `validate_forecast()` was deleted and replaced by `assert_forecast()`. Agreed plan: helper function + warning in `assert_forecast`. No user-facing diagnostic function exists. | Near-duplicate of #895 (user-facing `get_non_monotonic_forecasts()` helper). Related to #840 (input check cleanup). | Add monotonicity warning to `assert_forecast.forecast_quantile()` checking that predictions don't decrease with increasing quantile_level. Create a user-facing `get_non_monotonic_forecasts()` helper to identify affected rows (resolving both #412 and #895). | [ ] | [ ] | — | [ ] |\n| 411 | Rework `interval_coverage_sample()` | refactor | 2023-11-10 | refactor | low | small | yes | Fully resolved. `interval_coverage_sample()` was deleted in commit cc41a0e5 (Jan 5, 2024) as part of the v2.0.0 redesign. NEWS.md documents: \"Removed `interval_coverage_sample()` as users are now expected to convert to a quantile format first before scoring.\" Coverage is now handled by `interval_coverage()` (quantile-based) and `get_coverage()`. Issue can be closed. | None. | Close this issue — `interval_coverage_sample()` was intentionally removed in v2.0.0. The underlying concerns (code simplification, reuse of `interval_coverage_quantile`) were resolved by deleting the function and unifying the API around quantile-format coverage computation. | N/A | N/A | — | N/A |\n| 406 | Maybe expose `get_type()`. | | 2023-11-10 | enhancement | low | small | yes | Still relevant. `get_type()` (vector type detection: classification/integer/continuous) and `get_forecast_type()` (forecast class type) both exist in `R/get-forecast-type.R` but are internal (`@keywords internal`), not exported. The proposed renames (`get_vector_type`, `get_observed_type`, `get_predicted_type`) were never implemented. No comments, no progress. | Related to #484 (functions to expose) and #885 (export decisions). No blockers. | Rename `get_type()` to `get_vector_type()`, export it, and create `get_observed_type()` / `get_predicted_type()` wrappers that extract the relevant column from a forecast and call `get_vector_type()`. Decide whether to also export `get_forecast_type()` (tracked in #484/#885). | [ ] | [ ] | — | [ ] |\n| 385 | Feature: Add data transformation functions to work with other packages | help wanted | 2023-11-03 | feature | low | large | no | Still relevant but aspirational. No conversion functions for external packages (Stan, Odin, yardstick, hubverse) exist. The `as_forecast_*` S3 generic system provides extension points for external packages to write their own methods. Hubverse integration was explicitly moved to external `hubEvals` package (#605 closed). Maintainer consensus: package-specific conversions are too varied and dependency-heavy for scoringutils. Yardstick conversion code in comments never formalized. 2+ years old with no progress. | Related to #356 (interoperability vignette), #363 (external function interface), #852 (custom forecast class workflow), #983 (`.data.frame` methods), #1026 (`as_forecast_sample.forecast_quantile`). | Consider narrowing scope or closing in favor of more specific issues. The broad goal is better served by: (1) completing S3 extensibility infrastructure (#852, #983, #889, #888), (2) writing an interoperability vignette (#356), and (3) possibly adding 1-2 zero-dependency convenience converters if demand warrants. Too broad to be directly actionable as stated. | N/A | N/A | — | N/A |\n| 382 | Feature: Input check to notify about clashes with metrics and column names | good first issue, package improvement, implementation-ready | 2023-11-02 | enhancement | medium | small | yes | Still relevant and unresolved. A pre-v2.0 implementation existed in `check_clean_data()` but was removed during the v2.0 rewrite. A commented-out test skeleton exists in `test-score.R` (lines 51-62). `apply_metrics()` silently overwrites any existing column whose name matches a metric name via `forecast[, (metric_name) := result]`. No pre-scoring clash detection exists. | Tangentially related to #927 (prefix internal columns). Could coordinate with #811 (batch warnings). No blockers. | Add a check after `validate_metrics(metrics)` in each `score.forecast_*` method comparing `names(metrics)` against `colnames(forecast)` and emitting `cli_warn()` on overlap. Uncomment and update the existing test skeleton in `test-score.R`. | [ ] | [ ] | — | [ ] |\n| 363 | Discussion: design interface for external functions with different input/output formats | help wanted, question | 2023-10-26 | enhancement | low | small | yes | Still relevant. No wrapper/adapter functions exist. Users must manually wrap external metrics with non-standard signatures (e.g., reversed arg order, numeric 0/1 instead of factor observed). Two small wrapper proposals in the issue body (`wrap_binary_numeric`, `wrap_switch_order`) were never implemented. The `purrr::partial()` mechanism alone cannot solve argument reordering or type conversion. Zero comments, no design consensus. 2+ years old. | Related to #362 (non-standard I/O formats), #522 (test user-provided scoring functions), #385 (data transformation), #789 (customization docs). No blockers. | Implement 2-3 small exported adapter functions (e.g., `metric_adapt_binary_numeric()` to convert factor to 0/1, `metric_adapt_swap_args()` to reverse observed/predicted order). Document usage in `score()`'s \"Customising metrics\" section. Resolve naming convention with maintainers first. | [ ] | [ ] | — | [ ] |\n| 362 | Discussion: How to handle scores with input / output formats not currently covered | question | 2023-10-26 | discussion | low | large | no | Still relevant. `apply_metrics()` requires each metric to return a numeric vector of length n (one per forecast unit). Metrics with non-standard I/O (many-to-one like AUC, many-to-many like PIT) cannot fit into `score()`. The PIT case is solved outside `score()` via `get_pit_histogram()`. Zero comments, no design consensus. The multivariate sample class introduced a partial precedent (multiple rows collapse into groups) but by restructuring data before `apply_metrics()`, not by changing it. 2+ years old. | Related to #455 (data frame returns — recommended for closure), #363 (wrapper functions), #338 (cross-type metrics), #941 (rethink `score()` S3), #852 (custom class workflow). | Long-term design question with no clear path forward. The pragmatic pattern (non-standard metrics live outside `score()` as standalone functions) is established. Consider closing as won't-fix or consolidating with #455, #363, #338 into a single milestone-level tracking issue. Any solution requires fundamental changes to `apply_metrics()`. | N/A | N/A | — | N/A |\n| 360 | Create a wrapper around log score to warn about its use for integer-valued forecasts | implementation-ready | 2023-10-26 | enhancement | medium | small | yes | Still relevant and unaddressed. `logs_sample()` is a thin wrapper with no integer detection or warning. `get_type()` already exists and is used by `bias_sample()` and `pit_histogram_sample()` for the same purpose. Consensus reached on MVP: add a warning when `get_type(predicted) == \"integer\"`. Discussion also suggested possibly removing `log_score` from default metrics for sample forecasts. | None. All building blocks exist (`get_type()`, `cli_warn()`, pattern established in `bias_sample()`). | Add `get_type(predicted)` check to `logs_sample()` that emits `cli_warn()` when predictions are integer-valued. Optionally move `log_score` out of the default metric list in `get_metrics.forecast_sample()`. Add targeted tests. | [ ] | [ ] | — | [ ] |\n| 356 | Documentation: Create a vignette for interactions with other packages | documentation | 2023-10-26 | documentation | low | medium | partially | Still relevant. No interoperability vignette exists. Deprecated-visualisations vignette mentions hubVis. Hubverse integration moved to external `hubEvals` package (#605, closed). No conversion functions for external packages exist in scoringutils. `as_forecast_*` S3 generics provide extension points for external packages. ~2.5 years old with no progress. | Related to #385 (data transformation for other packages), #852 (custom forecast class workflow), #983 (`.data.frame` methods). | Create a vignette documenting how to use scoringutils with common external packages (hubverse via hubEvals, Stan output, yardstick). Focus on showing the `as_forecast_*()` S3 extension mechanism and practical conversion patterns. Requires editorial decisions on scope and which packages to feature. | N/A | N/A | — | N/A |\n| 353 | Documentation: Create a vignette that details the different inputs and internal workings of scoringutils | documentation | 2023-10-26 | documentation | medium | large | partially | Still relevant but heavily overlapping with #546, #759, #852. The deleted `scoringutils.Rmd` vignette (PR #836) covered 3 of 4 bullet points; replacement was never written. README \"Quick start\" section partially addresses input formats and workflow. The \"create your own class\" bullet is blocked by prerequisite S3 refactors (#889, #888, #885). Package currently has no introductory or architecture vignette. | Tightly coupled with #546 (simplify getting started) and #759 (replace deleted vignette). \"Create your own class\" aspect depends on #852, blocked by #889, #888, #885. | Consider consolidating with #546 and #759 into a single effort to write a new introductory vignette covering input formats, the `as_forecast_*()` / `score()` workflow, and forecast object anatomy. Defer \"create your own class\" section until S3 extensibility refactors are complete. | N/A | N/A | — | N/A |\n| 338 | Discussion: Define implementation for metrics which work for different forecast types | help wanted, question, Breaking change | 2023-10-22 | discussion | low | medium | no | Partially addressed by #832 (`get_metrics()` now S3). The remaining question is whether individual metric functions (e.g., `bias_sample`/`bias_quantile`) should also become S3 generics for standalone use. Practical need is low since `score()` already dispatches correctly via `get_metrics.<class>`. 6 conceptual metrics span 2+ forecast types with type-suffixed names. | #832 (done). Related to #455 (vector returns), #941 (rethink `score()` S3). No blockers. | Consider downgrading from \"Breaking change\" since S3 wrappers could be additive. If pursued, create `bias()`, `overprediction()`, `underprediction()`, `dispersion()`, `ae_median()`, `logs()` as S3 generics dispatching on forecast class, keeping suffixed functions as implementations. Defer to future major release. | N/A | N/A | — | N/A |\n| 329 | Add additional documentation for `crps_sample()`, `logs_sample()` and `dss_sample()` | package improvement | 2023-09-15 | documentation | low | small | partially | Still relevant. Formulas exist in the vignette (`scoring-rules.Rmd`) but have not been ported to the roxygen docs for these three functions. Other functions in the same file (e.g., `bias_sample`, `pit_histogram_sample`) already have formula-rich roxygen documentation via `\\deqn{}`, providing a clear template. `dss_sample()` is worst — almost no description beyond \"wrapper.\" | None. Standalone documentation task. | Add `\\deqn{}` mathematical formulas and expanded descriptions to roxygen docs of `crps_sample()`, `logs_sample()`, and `dss_sample()` in `R/metrics-sample.R`. Adapt formulas from the existing vignette content. `dss_sample()` needs the most work. | N/A | N/A | — | N/A |\n| 289 | Add support for comparing two distributions | | 2023-07-12 | feature | low | large | no | Still relevant. Zero implementation exists. The entire package architecture assumes scoring against scalar observations. No distribution comparison functions in codebase or in `scoringRules` dependency. Requires fundamental design decisions: new forecast class, new metric functions, and integration approach. 2.5+ years old with zero comments. Filed as an aspirational idea. | Related to #852 (custom forecast class workflow), #889/#888 (S3 extensibility). No issues depend on this. | Design and implement a new forecast class for distribution-to-distribution comparison with associated metrics (energy distance, KL divergence, Wasserstein distance). Should be deferred until S3 extensibility infrastructure (#852, #889, #888) is complete. | N/A | N/A | — | N/A |\n| 286 | Extension covering idees in \"Model Diagnostics and Forecast Evaluation for Quantiles\" | enhancement, feature request | 2023-05-03 | feature | medium | large | no | Still relevant. None of the three proposed features (consistency-banded coverage plots, Murphy diagrams, CORP reliability diagrams) are implemented. Basic coverage plots and PIT histograms exist but lack the statistical rigor proposed. Paper's code could serve as starting point. ~3 years old with no progress. Design decisions needed on scope, API, and potential new dependencies (e.g., `reliabilitydiag` R package). | Thematically related to #49 (calibration plotting improvement). No hard blockers. @jbracher is already a scoringutils contributor. | Implement three new diagnostic tools from Bracher et al.: (1) add consistency bands and confidence intervals to existing coverage plots, (2) create `murphy_diagram()` function and plot, (3) create CORP reliability diagram function. Consider addressing #49 simultaneously. | N/A | N/A | — | N/A |\n| 267 | How to score literature/other resources | documentation, help wanted | 2023-02-13 | documentation | low | small | no | Still relevant. Function-level `@references` tags exist across 8 R files, and the scoring-rules vignette has a .bib with 7 papers. However, the agreed-upon deliverable — a standalone, discoverable vignette summarizing scoring methodology and linking to key literature — has not been created. Two vignettes referenced in the README (\"metric-details\", \"scoring-forecasts-directly\") do not exist on disk. ~3 years old. | None blocking. Related to #758 (scoring rules vignette), #789 (customization docs), #666 (update paper). | Create a short standalone vignette summarizing scoring methodology and consolidating all referenced literature into one discoverable place. Link to the full paper and relevant external resources. Also fix the broken vignette links in the README. Requires editorial decisions on content. | N/A | N/A | — | N/A |\n| 193 | Add more metrics to assess sharpness of forecasts in a sample format | feature request, implementation-ready | 2022-02-16 | feature | low | small | yes | Still relevant. `mad_sample()` is the only dedicated sharpness metric. SD and IQR (specifically requested) are not implemented. Both are trivial to add following the `mad_sample()` pattern (data-independent, apply function row-wise over predicted matrix). `dispersion_sample()` (CRPS component) also measures spread but is observation-dependent. `scoringRules` provides no additional sharpness measures. | None. Self-contained addition following existing `mad_sample()` pattern. | Implement `sd_sample()` and `iqr_sample()` in `R/metrics-sample.R` following the `mad_sample()` pattern. Add both to the default metric list in `get_metrics.forecast_sample()`. Add tests, documentation, and NAMESPACE exports. | [ ] | [ ] | — | [ ] |\n| 49 | Improve calibration plotting | enhancement, feature request | 2020-07-25 | feature | low | medium | no | Partially addressed. PIT histograms exist via `get_pit_histogram()` (covering item 1 approximately). Items 2 (ACF of PIT values) and 3 (marginal calibration plot) are completely unimplemented. True Talagrand/verification rank histogram also not implemented as a distinct tool. Maintainer said \"no active plan, but nice to have.\" 5+ years old. | Thematically overlaps with #286 (advanced calibration diagnostics). No hard blockers. | Three remaining items: (1) implement verification rank histogram for ensemble/sample forecasts, (2) implement ACF computation/plot for PIT values, (3) implement marginal calibration plot. Consider consolidating with #286 into a single calibration diagnostics tracking issue. | N/A | N/A | — | N/A |\n| 37 | Add Diebold Mariano test | enhancement, good first issue, help wanted, feature request | 2020-06-20 | feature | low | small | partially | Still relevant. No DM test implementation exists. `compare_forecasts()` in `R/pairwise-comparisons.R` already has a clean `test_type` dispatch pattern (Wilcoxon and permutation). The DM test's main value-add is proper handling of autocorrelated score differences common in time-series forecasting. Open since 2020 with maintainer agreement but explicitly deferred. | Dependency of #1055 (model confidence sets). Related to #750 (p-value refactor), #769 (pairwise comparison arguments), #1022 (two-model case). | Add `\"dm\"` as a new `test_type` option in `compare_forecasts()`. Self-implement the DM test statistic (preferred over adding `forecast` dependency) with Newey-West HAC variance estimator. Add tests and documentation. Coordinate naming and autocorrelation lag handling with maintainers. | N/A | N/A | — | N/A |\n\nTRIAGE_COMPLETE","numLines":27,"startLine":105,"totalLines":131}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01YLMd4RA8ThKFqFyfXsjYSE","type":"message","role":"assistant","content":[{"type":"text","text":"There are still remaining automatable=yes issues with Has Tests = `[ ]`: #445, #412, #406, #382, #363, #360, #193. So I should NOT set the pipeline checkpoint yet. Let me verify this is correct by confirming the instruction: \"are all rows with Automatable `yes` and Has Tests not `N/A` now `[x]`?\" — No, there are 7 more. So I don't set the pipeline checkpoint."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":6663,"cache_read_input_tokens":71664,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6663},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"ade020b4-afae-4cd3-8da2-2b556113b61a"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01YLMd4RA8ThKFqFyfXsjYSE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CBAJmjaQiQTpj92gVS6XwG","name":"TodoWrite","input":{"todos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"completed","activeForm":"Updating triage table"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":6663,"cache_read_input_tokens":71664,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6663},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"5e9626eb-f34e-4b01-972c-679aa7817bea"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CBAJmjaQiQTpj92gVS6XwG","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"6a423cf7-bcbb-40fb-94b8-4f651f5ecd4e","tool_use_result":{"oldTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"in_progress","activeForm":"Updating triage table"}],"newTodos":[{"content":"Scan triage table for next automatable issue without tests","status":"completed","activeForm":"Scanning triage table for next automatable issue without tests"},{"content":"Read full issue context for #454","status":"completed","activeForm":"Reading full issue context for #454"},{"content":"Understand test landscape for issue #454","status":"completed","activeForm":"Understanding test landscape for issue #454"},{"content":"Design test cases for #454","status":"completed","activeForm":"Designing test cases for #454"},{"content":"Write test specs to issue file","status":"completed","activeForm":"Writing test specs to issue file"},{"content":"Update triage table","status":"completed","activeForm":"Updating triage table"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01RyJNs3YpXaFBjpNmkcRmQE","type":"message","role":"assistant","content":[{"type":"text","text":"Done. Here's a summary of what was completed:\n\n**Issue #454: Deduplicate code in `get_coverage()` and `interval_coverage()`**\n\n- **Test specs written** to `.ralph/issues/454-deduplicate-coverage.md` with 7 test cases:\n  1. Cross-validation that `get_coverage()` and `interval_coverage()` agree on the same data\n  2. Known-input regression test for `get_coverage()` bounds check\n  3. Boundary cases (observation exactly on lower/upper bound)\n  4. Multiple interval ranges handled correctly\n  5. Both functions agree when observation is outside all intervals\n  6. Non-symmetric intervals (existing test preserved as regression guard)\n  7. Full-dataset cross-validation between `get_coverage()` and `score()` pathways\n\n- **Triage table updated**: Has Tests set to `[x]` for issue #454\n\n- **Pipeline checkpoint NOT set**: 7 more automatable issues still need test specs (#445, #412, #406, #382, #363, #360, #193)"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":396,"cache_read_input_tokens":78327,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":396},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","uuid":"f7cc549d-6578-4223-b2fe-32d85d1a8878"}
{"type":"result","subtype":"success","is_error":false,"duration_ms":248511,"duration_api_ms":247987,"num_turns":25,"result":"Done. Here's a summary of what was completed:\n\n**Issue #454: Deduplicate code in `get_coverage()` and `interval_coverage()`**\n\n- **Test specs written** to `.ralph/issues/454-deduplicate-coverage.md` with 7 test cases:\n  1. Cross-validation that `get_coverage()` and `interval_coverage()` agree on the same data\n  2. Known-input regression test for `get_coverage()` bounds check\n  3. Boundary cases (observation exactly on lower/upper bound)\n  4. Multiple interval ranges handled correctly\n  5. Both functions agree when observation is outside all intervals\n  6. Non-symmetric intervals (existing test preserved as regression guard)\n  7. Full-dataset cross-validation between `get_coverage()` and `score()` pathways\n\n- **Triage table updated**: Has Tests set to `[x]` for issue #454\n\n- **Pipeline checkpoint NOT set**: 7 more automatable issues still need test specs (#445, #412, #406, #382, #363, #360, #193)","stop_reason":null,"session_id":"33435e0e-495f-4aee-b06e-b0e97407b6ef","total_cost_usd":1.2750208,"usage":{"input_tokens":23,"cache_creation_input_tokens":60719,"cache_read_input_tokens":1104353,"output_tokens":10027,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":60719,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]},"modelUsage":{"claude-opus-4-6":{"inputTokens":23,"outputTokens":10027,"cacheReadInputTokens":1104353,"cacheCreationInputTokens":60719,"webSearchRequests":0,"costUSD":1.18246025,"contextWindow":200000,"maxOutputTokens":32000},"claude-haiku-4-5-20251001":{"inputTokens":6601,"outputTokens":4807,"cacheReadInputTokens":99083,"cacheCreationInputTokens":41613,"webSearchRequests":0,"costUSD":0.09256054999999999,"contextWindow":200000,"maxOutputTokens":64000}},"permission_denials":[],"uuid":"949198e5-cb0e-47d3-859d-809cb28693f1"}
