@article{bracherEvaluatingEpidemicForecasts2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {PLoS computational biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  langid = {english},
  pmcid = {PMC7880475},
  pmid = {33577550},
  keywords = {Communicable Diseases,COVID-19,Forecasting,Humans,Pandemics,Probability,SARS-CoV-2},
  annotation = {note: DOI 10.1371/journal.pcbi.1008618},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EX37R6J8/Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf}
}

@article{funkAssessingPerformanceRealtime2019,
  title = {Assessing the Performance of Real-Time Epidemic Forecasts: {{A}} Case Study of {{Ebola}} in the {{Western Area}} Region of {{Sierra Leone}}, 2014-15},
  shorttitle = {Assessing the Performance of Real-Time Epidemic Forecasts},
  author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Lowe, Rachel and Eggo, Rosalind M. and Edmunds, W. John},
  year = {2019},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {2},
  pages = {e1006785},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006785},
  urldate = {2019-09-16},
  abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013--16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
  langid = {english},
  keywords = {Epidemiology,Forecasting,Infectious disease epidemiology,Infectious diseases,Mathematical models,Probability distribution,Public and occupational health,Sierra Leone},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf;/Users/nikos/github-synced/zotero-nikos/storage/JN28VVKF/article.html}
}

@article{gneitingMakingEvaluatingPoint2011,
  title = {Making and {{Evaluating Point Forecasts}}},
  author = {Gneiting, Tilmann},
  year = {2011},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {106},
  number = {494},
  pages = {746--762},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/jasa.2011.r10138},
  urldate = {2021-10-18},
  abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
  keywords = {Bayes rule,Bregman function,Conditional value-at-risk (CVaR),Decision theory,Elicitability,Expectile,Mean,Median,Mode,Proper scoring rule,Quantile,Statistical functional},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JZGDGGB9/Gneiting - 2011 - Making and Evaluating Point Forecasts.pdf;/Users/nikos/github-synced/zotero-nikos/storage/5CBRZY4U/jasa.2011.html}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  urldate = {2020-03-22},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf}
}

@article{macheteContrastingProbabilisticScoring2012,
  title = {Contrasting {{Probabilistic Scoring Rules}}},
  author = {Machete, Reason Lesego},
  year = {2012},
  month = jul,
  journal = {arXiv:1112.4530 [math, stat]},
  eprint = {1112.4530},
  primaryclass = {math, stat},
  urldate = {2020-03-21},
  abstract = {There are several scoring rules that one can choose from in order to score probabilistic forecasting models or estimate model parameters. Whilst it is generally agreed that proper scoring rules are preferable, there is no clear criterion for preferring one proper scoring rule above another. This manuscript compares and contrasts some commonly used proper scoring rules and provides guidance on scoring rule selection. In particular, it is shown that the logarithmic scoring rule prefers erring with more uncertainty, the spherical scoring rule prefers erring with lower uncertainty, whereas the other scoring rules are indifferent to either option.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62B10 62C05 62G05 62G07 62F99 62P05 62P12 62P20,Mathematics - Statistics Theory},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8FYPC3Y4/Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf}
}

@article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {Winkler, R. L. and Mu{\~n}oz, Javier and Cervera, Jos{\'e} L. and Bernardo, Jos{\'e} M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and {R{\'i}os-Insua}, David},
  year = {1996},
  month = jun,
  journal = {Test},
  volume = {5},
  number = {1},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  urldate = {2021-03-04},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for ``good'' probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of ``goodness'' of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf}
}

@article{zielEnergyDistanceEnsemble2021,
  title = {The Energy Distance for Ensemble and Scenario Reduction},
  author = {Ziel, Florian},
  year = {2021},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2202},
  pages = {20190431},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2019.0431},
  urldate = {2023-12-12},
  abstract = {Scenario reduction techniques are widely applied for solving sophisticated dynamic and stochastic programs, especially in energy and power systems, but are also used in probabilistic forecasting, clustering and estimating generative adversarial networks. We propose a new method for ensemble and scenario reduction based on the energy distance which is a special case of the maximum mean discrepancy. We discuss the choice of energy distance in detail, especially in comparison to the popular Wasserstein distance which is dominating the scenario reduction literature. The energy distance is a metric between probability measures that allows for powerful tests for equality of arbitrary multivariate distributions or independence. Thanks to the latter, it is a suitable candidate for ensemble and scenario reduction problems. The theoretical properties and considered examples indicate clearly that the reduced scenario sets tend to exhibit better statistical properties for the energy distance than a corresponding reduction with respect to the Wasserstein distance. We show applications to a Bernoulli random walk and two real data-based examples for electricity demand profiles and day-ahead electricity prices. This article is part of the theme issue `The mathematics of energy systems'.},
  keywords = {electricity load,energy score,Kontorovic distance,maximum mean discrepancy,scenario reduction,stochastic programming,Wasserstein metric},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZIY7UT87/Ziel - 2021 - The energy distance for ensemble and scenario redu.pdf}
}
