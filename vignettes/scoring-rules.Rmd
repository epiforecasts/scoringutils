---
title: "Scoring rules in `scoringutils`"
author: "Nikos Bosse"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Scoring rules in `scoringutils`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 7,
  collapse = TRUE,
  comment = "#>"
)
library(scoringutils)
library(data.table)
```

# Introduction

This vignette gives an overview of the default scoring rules made available through the `scoringutils` package. If you want to obtain more detailed information about how the pacakge works, have a look at the [revised version](https://drive.google.com/file/d/1URaMsXmHJ1twpLpMl1sl2HW4lPuUycoj/view?usp=drive_link) of our `scoringutils` paper. 

We can distinguish two types of forecasts: point forecasts and probabilistic forecasts. A point forecast is a single number representing a single outcome. A probabilistic forecast is a full predictive probability distribution over multiple possible outcomes. In contrast to point forecasts, probabilistic forecasts incorporate uncertainty about different possible outcomes. 

Scoring rules are functions that take a forecast and an observation as input and return a single numeric value. For point forecasts, they take the form $S(\hat{y}, y)$, where $\hat{y}$ is the forecast and $y$ is the observation. For probabilistic forecasts, they take the form $S(F, y)$, where $F$ is the cumulative density function (CDF) of the predictive distribution and $y$ is the observation. By convention, scoring rules are usually negatively oriented, meaning that smaller values are better (the best possible score is usually zero). In that sense, the score can be understood as a penalty. 

Many scoring rules for probabilistic forecasts are so-called (strictly) proper scoring rules. Essentially, this means that they cannot be "cheated": A forecaster evaluated by a strictly proper scoring rule is always incentivised to report her honest best belief about the future and cannot, in expectation, improve her score by reporting something else. A more formal definition is the following: Let the true, unobserved data-generating distribution be $G$. A scoring rule is said to be proper, if under $G$ and for an ideal forecast $F = G$, there is no forecast $F' \neq F$ that in expectation receives a better score than $F$. A scoring rule is considered strictly proper if, under $G$, no other forecast $F'$ in expectation receives a score that is better than or the same as that of $F$. 

# Metrics for point forecasts

See a list of the default metrics for point forecasts by calling `?metrics_point()`. 

This is an overview of the input and output formats for point forecasts: 

```{r, echo=FALSE, out.width="100%", fig.cap="A nice image."}
knitr::include_graphics("default-scoring-rules/input-point.png")
```


## Absolute error

**Observation**: $y$, a real number

**Forecast**: $\hat{y}$, a real number

The absolute error is the absolute difference between the predicted and the observed values. See `?Metrics::ae`. 

$$\text{ae} = |y - \hat{y}|$$
## Squared error

**Observation**: $y$, a real number

**Forecast**: $\hat{y}$, a real number

The squared error is the squared difference between the predicted and the observed values. See `?Metrics::se`. 

$$\text{se} = (y - \hat{y})^2$$

## Absolute percentage error

**Observation**: $y$, a real number

**Forecast**: $\hat{y}$, a real number

The absolute percentage error is the absolute percent difference between the predicted and the observed values. See `?Metrics::ape`. 

$$\text{ape} = \frac{|y - \hat{y}|}{|y|}$$
## Further considerations

XXXae is good for median, se is good for mean. cite Gneiting point forecast paper. 

# Binary forecasts

See a list of the default metrics for point forecasts by calling `?metrics_binary()`. 

This is an overview of the input and output formats for point forecasts: 

```{r, echo=FALSE, out.width="100%", fig.cap="A nice image."}
knitr::include_graphics("default-scoring-rules/input-binary.png")
```


## Brier score

**Observation**: $y$, either 1 or 0

**Forecast**: $p$, a probability that the observed outcome will be 1. 

The Brier score is a strictly proper scoring rule. It is computed as the mean squared error between the probabilistic prediction and the observed outcome.

\begin{equation}
    \text{BS}(p, y) = (p - y)^2 = 
    \begin{cases}
        p^2,       & \text{if } y = 1\\
        (1 - p)^2,   & \text{if } y = 0
    \end{cases}
\end{equation}

See `?brier_score()` for more information.

## Log score

**Observation**: $y$, either 1 or 0

**Forecast**: $p$, a probability that the observed outcome will be 1. 

The log score is a strictly proper scoring rule. It is computed as the negative logarithm of the probability assigned to the observed outcome. 

\begin{equation}
    \text{Log score}(p, y) = - \log(1 - |y - p|) = 
    \begin{cases}
        -\log (p),       & \text{if } y = 1\\
        -\log (1 - p),   & \text{if } y = 0
    \end{cases}
\end{equation}

See `?logs_binary()` for more information.


# Sample-based forecasts

See a list of the default metrics for sample-based forecasts by calling `?metrics_sample()`. 

This is an overview of the input and output formats for quantile forecasts: 

```{r, echo=FALSE, out.width="100%", fig.cap="A nice image."}
knitr::include_graphics("default-scoring-rules/input-sample.png")
```




# Quantile-based forecasts

See a list of the default metrics for quantile-based forecasts by calling `?metrics_quantile()`. 

This is an overview of the input and output formats for quantile forecasts: 

```{r, echo=FALSE, out.width="100%", fig.cap="A nice image."}
knitr::include_graphics("default-scoring-rules/input-quantile.png")
```

## Weighted interval score (WIS)

**Observation**: $y$, a real number

**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles. These quantiles form the lower ($l$) and upper ($u$) bounds of central prediction intervals. 

The weighted interval score (WIS) is a strictly proper scoring rule and can be understood as an approximation of the CRPS for forecasts in a quantile format (which in turn represents a generalisation of the absolute error). Quantiles are assumed to be the lower and upper bounds of prediction intervals symmetric around the median. For a single interval, the interval score is

$$IS_\alpha(F,y) = \underbrace{(u-l)}_\text{dispersion} + \underbrace{\frac{2}{\alpha} \cdot (l-y) \cdot \mathbf{1}(y \leq l)}_{\text{overprediction}} + \underbrace{\frac{2}{\alpha} \cdot (y-u) \cdot \mathbf{1}(y \geq u)}_{\text{underprediction}}, $$

where $\mathbf{1}()$ is the indicator function, and $l$ and $u$ are the $\frac{\alpha}{2}$ and $1 - \frac{\alpha}{2}$ quantiles of the predictive distribution $F$. $l$ and $u$ together form the prediction interval. The interval score can be understood as the sum of three components: dispersion, overprediction and underprediction. 
  
For a set of $K$ prediction intervals and the median $m$, the score is given as a weighted sum of individual interval scores, i.e.

$$WIS = \frac{1}{K + 0.5} \cdot \left(w_0 \cdot |y - m| + \sum_{k = 1}^{K} w_k \cdot IS_{\alpha_{k}}(F, y)\right),$$
where $m$ is the median forecast and $w_k$ is a weight assigned to every interval. When the weights are set to $w_k = \frac{\alpha_k}{2}$ and $w_0 = 0.5$, then the WIS converges to the CRPS for an increasing number of equally spaced quantiles.

See `?wis()` for more information.

### Overprediction, underprediction and dispersion

These are the individual components of the WIS. See `?overprediction()`, `?underprediction()` and `?dispersion()` for more information.

## Bias

**Observation**: $y$, a real number

**Forecast**: $F$. The CDF of the predictive distribution is represented by a set of quantiles.





## Interval coverage

### Interval coverage deviation

## Absolute error of the median


## Quantile score


# Additional metrics

## Quantile coverage


## Probability integral transform (PIT)









A variety of metrics and scoring rules can also be accessed directly through 
the `scoringutils` package. 

The following gives an overview of (most of) the implemented metrics. 

# Bias

The function `bias` determines bias from predictive Monte-Carlo samples, 
automatically recognising whether forecasts are continuous or
integer valued. 

For continuous forecasts, Bias is measured as
$$B_t (P_t, x_t) = 1 - 2 \cdot (P_t (x_t))$$

where $P_t$ is the empirical cumulative distribution function of the
prediction for the observed value $x_t$. Computationally, $P_t (x_t)$ is
just calculated as the fraction of predictive samples for $x_t$
that are smaller than $x_t$.

For integer valued forecasts, Bias is measured as

$$B_t (P_t, x_t) = 1 - (P_t (x_t) + P_t (x_t + 1))$$

to adjust for the integer nature of the forecasts. In both cases, Bias can 
assume values between -1 and 1 and is 0 ideally.

```{r}
## integer valued forecasts
observed <- rpois(30, lambda = 1:30)
predicted <- replicate(200, rpois(n = 30, lambda = 1:30))
bias_sample(observed, predicted)

## continuous forecasts
observed <- rnorm(30, mean = 1:30)
predicted <- replicate(200, rnorm(30, mean = 1:30))
bias_sample(observed, predicted)
```


# Sharpness
Sharpness is the ability of the model to generate predictions within a
narrow range. It is a data-independent measure, and is purely a feature
of the forecasts themselves.

Sharpness / dispersion of predictive samples corresponding to one single observed value is
measured as the normalised median of the absolute deviation from
the median of the predictive samples. For details, see `?stats::mad`

```{r}
predicted <- replicate(200, rpois(n = 30, lambda = 1:30))
mad_sample(predicted = predicted)
```

# Calibration

Calibration or reliability of forecasts is the ability of a model to
correctly identify its own uncertainty in making predictions. In a model
with perfect calibration, the observed data at each time point look as if
they came from the predictive probability distribution at that time.

Equivalently, one can inspect the probability integral transform of the
predictive distribution at time t,

$$u_t = F_t (x_t)$$

where $x_t$ is the observed data point at time $t \text{ in } t_1, …, t_n$,
n being the number of forecasts, and $F_t$ is the (continuous) predictive
cumulative probability distribution at time t. If the true probability
distribution of outcomes at time t is $G_t$ then the forecasts $F_t$ are
said to be ideal if $F_t = G_t$ at all times $t$. In that case, the
probabilities ut are distributed uniformly.

In the case of discrete outcomes such as incidence counts,
the PIT is no longer uniform even when forecasts are ideal.
In that case a randomised PIT can be used instead:

$$u_t = P_t(k_t) + v \cdot (P_t(k_t) - P_t(k_t - 1) )$$

where $k_t$ is the observed count, $P_t(x)$ is the predictive
cumulative probability of observing incidence $k$ at time $t$,
$P_t (-1) = 0$ by definition and $v$ is standard uniform and independent
of $k$. If $P_t$ is the true cumulative
probability distribution, then $u_t$ is standard uniform.

The function checks whether integer or continuous forecasts were provided.
It then applies the (randomised) probability integral and tests
the values $u_t$ for uniformity using the
Anderson-Darling test.

As a rule of thumb, there is no evidence to suggest a forecasting model is
miscalibrated if the p-value found was greater than a threshold of $p >= 0.1$,
some evidence that it was miscalibrated if $0.01 < p < 0.1$, and good
evidence that it was miscalibrated if $p <= 0.01$.
In this context it should be noted, though, that uniformity of the
PIT is a necessary but not sufficient condition of calibration. It should
also be noted that the test only works given sufficient samples, otherwise the 
Null hypothesis will often be rejected outright. 


# Continuous Ranked Probability Score (CRPS)
Wrapper around the `crps_sample()` function from the
`scoringRules` package. For more information look at the manuals from the
`scoringRules` package. The function can be used for continuous as well as 
integer valued forecasts. Smaller values are better. 

```{r}
observed <- rpois(30, lambda = 1:30)
predicted <- replicate(200, rpois(n = 30, lambda = 1:30))
crps_sample(observed, predicted)
```



# Dawid-Sebastiani Score (DSS)
Wrapper around the `dss_sample()` function from the
`scoringRules` package. For more information look at the manuals from the
`scoringRules` package. The function can be used for continuous as well as 
integer valued forecasts. Smaller values are better. 

```{r}
observed <- rpois(30, lambda = 1:30)
predicted <- replicate(200, rpois(n = 30, lambda = 1:30))
dss_sample(observed, predicted)
```

# Log Score
Wrapper around the `logs_sample()` function from the
`scoringRules` package. For more information look at the manuals from the
`scoringRules` package. The function should not be used for integer valued 
forecasts. While Log Scores are in principle possible for integer valued 
forecasts they require a kernel density estimate which is not well defined 
for discrete values. Smaller values are better. 

```{r}
observed <- rnorm(30, mean = 1:30)
predicted <- replicate(200, rnorm(n = 30, mean = 1:30))
logs_sample(observed, predicted)
```


## Interval Score
The Interval Score is a Proper Scoring Rule to score quantile predictions,
following Gneiting and Raftery (2007). Smaller values are better.

The score is computed as

$$ \text{score} = (\text{upper} - \text{lower}) + \\
\frac{2}{\alpha} \cdot (\text{lower} - \text{observed}) \cdot 1(\text{observed} < \text{lower}) + \\
\frac{2}{\alpha} \cdot (\text{observed} - \text{upper}) \cdot
1(\text{observed} > \text{upper})$$


where $1()$ is the indicator function and $\alpha$ is the decimal value that
indicates how much is outside the prediction interval.
To improve usability, the user is asked to provide an interval range in
percentage terms, i.e. interval_range = 90 (percent) for a 90 percent
prediction interval. Correspondingly, the user would have to provide the
5\% and 95\% quantiles (the corresponding alpha would then be 0.1).
No specific distribution is assumed,
but the range has to be symmetric (i.e you can't use the 0.1 quantile
as the lower bound and the 0.7 quantile as the upper). 
Setting `weigh = TRUE` will weigh the score by $\frac{\alpha}{2}$ such that 
the Interval Score converges to the CRPS for increasing number of quantiles. 


```{r}
observed <- c(1, -15, 22)
predicted <- rbind(
  c(-1, 0, 1, 2, 3),
  c(-2, 1, 2, 2, 4),
  c(-2, 0, 3, 3, 4)
)
quantile <- c(0.1, 0.25, 0.5, 0.75, 0.9)

wis(observed, predicted, quantile)
```
