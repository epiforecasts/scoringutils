% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pairwise-comparisons.R
\name{pairwise_comparison}
\alias{pairwise_comparison}
\title{Do Pairwise Comparisons of Scores}
\usage{
pairwise_comparison(
  scores,
  by = "model",
  metric = intersect(c("wis", "crps", "brier_score"), names(scores)),
  baseline = NULL,
  ...
)
}
\arguments{
\item{scores}{An object of class \code{scores} (a data.table with
scores and an additional attribute \code{metrics} as produced by \code{\link[=score]{score()}})}

\item{by}{character vector with column names that define the grouping level
for the pairwise comparisons. By default (\code{model}), there will be one
relative skill score per model. If, for example,
\code{by = c("model", "location")}. Then you will get a
separate relative skill score for every model in every location. Internally,
the data.table with scores will be split according \code{by} (removing "model"
before splitting) and the pairwise comparisons will be computed separately
for the split data.tables.}

\item{metric}{A string with the name of the metric for which
a relative skill shall be computed. By default this is either "crps",
"wis" or "brier_score" if any of these are available.}

\item{baseline}{A string with the name of a model. If a baseline is
given, then a scaled relative skill with respect to the baseline will be
returned. By default (\code{NULL}), relative skill will not be scaled with
respect to a baseline model.}

\item{...}{additional arguments for the comparison between two models. See
\code{\link[=compare_two_models]{compare_two_models()}} for more information.}
}
\value{
A data.table with pairwise comparisons
}
\description{
Compute relative scores between different models making pairwise
comparisons. Pairwise comparisons are a sort of pairwise tournament where all
combinations of two models are compared against each other based on the
overlapping set of available forecasts common to both models.
Internally, a ratio of the mean scores of both models is computed.
The relative score of a model is then the geometric mean of all mean score
ratios which involve that model. When a baseline is provided, then that
baseline is excluded from the relative scores for individual models
(which therefore differ slightly from relative scores without a baseline)
and all relative scores are scaled by (i.e. divided by) the relative score of
the baseline model.
Usually, the function input should be unsummarised scores as
produced by \code{\link[=score]{score()}}.
Note that the function internally infers the \emph{unit of a single forecast} by
determining all columns in the input that do not correspond to metrics
computed by \code{\link[=score]{score()}}. Adding unrelated columns will change results in an
unpredictable way.

The code for the pairwise comparisons is inspired by an implementation by
Johannes Bracher.
The implementation of the permutation test follows the function
\code{permutationTest} from the \code{surveillance} package by Michael HÃ¶hle,
Andrea Riebler and Michaela Paul.
}
\examples{
\dontshow{
  data.table::setDTthreads(2) # restricts number of cores used on CRAN
}

scores <- score(as_forecast(example_quantile))
pairwise <- pairwise_comparison(scores, by = "target_type")

library(ggplot2)
plot_pairwise_comparison(pairwise, type = "mean_scores_ratio") +
  facet_wrap(~target_type)
}
\author{
Nikos Bosse \email{nikosbosse@gmail.com}

Johannes Bracher, \email{johannes.bracher@kit.edu}
}
\keyword{scoring}
