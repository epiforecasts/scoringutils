<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-GB">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Scoring rules in `scoringutils` • scoringutils</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Scoring rules in `scoringutils`">
<meta name="robots" content="noindex">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">scoringutils</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">2.1.2.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://raw.githubusercontent.com/epiforecasts/scoringutils/main/inst/manuscript/manuscript.pdf">scoringutils paper</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/Deprecated-functions.html">Deprecated functions</a></li>
    <li><a class="dropdown-item" href="../articles/Deprecated-visualisations.html">Deprecated Visualisations</a></li>
    <li><a class="dropdown-item" href="../articles/scoring-multivariate-forecasts.html">Scoring multivariate forecasts</a></li>
    <li><a class="dropdown-item" href="../articles/scoring-rules.html">Scoring rules in `scoringutils`</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/epiforecasts/scoringutils/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Scoring rules in `scoringutils`</h1>
                        <h4 data-toc-skip class="author">Nikos
Bosse</h4>
            
            <h4 data-toc-skip class="date">2026-02-09</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/epiforecasts/scoringutils/blob/main/vignettes/scoring-rules.Rmd" class="external-link"><code>vignettes/scoring-rules.Rmd</code></a></small>
      <div class="d-none name"><code>scoring-rules.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This vignette gives an overview of the default scoring rules made
available through the <code>scoringutils</code> package. You can, of
course, also use your own scoring rules, provided they follow the same
format. If you want to obtain more detailed information about how the
package works, have a look at the <a href="https://drive.google.com/file/d/1URaMsXmHJ1twpLpMl1sl2HW4lPuUycoj/view?usp=drive_link" class="external-link">revised
version</a> of our <code>scoringutils</code> paper.</p>
<p>We can distinguish two types of forecasts: point forecasts and
probabilistic forecasts. A point forecast is a single number
representing a single outcome. A probabilistic forecast is a full
predictive probability distribution over multiple possible outcomes. In
contrast to point forecasts, probabilistic forecasts incorporate
uncertainty about different possible outcomes.</p>
<p>Scoring rules are functions that take a forecast and an observation
as input and return a single numeric value. For point forecasts, they
take the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(\hat{y}, y)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math>
is the forecast and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is the observation. For probabilistic forecasts, they usually take the
form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(F, y)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>
is the cumulative density function (CDF) of the predictive distribution
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is the observation. By convention, scoring rules are usually negatively
oriented, meaning that smaller values are better (the best possible
score is usually zero). In that sense, the score can be understood as a
penalty.</p>
<p>Many scoring rules for probabilistic forecasts are so-called
(strictly) proper scoring rules. Essentially, this means that they
cannot be “cheated”: A forecaster evaluated by a strictly proper scoring
rule is always incentivised to report her honest best belief about the
future and cannot, in expectation, improve her score by reporting
something else. A more formal definition is the following: Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>
be the true, unobserved data-generating distribution. A scoring rule is
said to be proper, if under
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>
and for an ideal forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">F = G</annotation></semantics></math>,
there is no forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>′</mi><mo>≠</mo><mi>F</mi></mrow><annotation encoding="application/x-tex">F' \neq F</annotation></semantics></math>
that in expectation receives a better score than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
A scoring rule is considered strictly proper if, under
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>,
no other forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">F'</annotation></semantics></math>
in expectation receives a score that is better than or the same as that
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.</p>
<hr>
</div>
<div class="section level2">
<h2 id="metrics-for-point-forecasts">Metrics for point forecasts<a class="anchor" aria-label="anchor" href="#metrics-for-point-forecasts"></a>
</h2>
<p>See a list of the default metrics for point forecasts by calling
<code>get_metrics(example_point)</code>.</p>
<p>This is an overview of the input and output formats for point
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-point.png" class="r-plt" alt="Input and output formats: metrics for point." width="100%"><p class="caption">
Input and output formats: metrics for point.
</p>
</div>
<div class="section level3">
<h3 id="a-note-of-caution">A note of caution<a class="anchor" aria-label="anchor" href="#a-note-of-caution"></a>
</h3>
<p>Scoring point forecasts can be tricky business. Depending on the
choice of the scoring rule, a forecaster who is clearly worse than
another, might consistently receive better scores (see <span class="citation">Gneiting (2011)</span> for an illustrative
example).</p>
<p>Every scoring rule for a point forecast is implicitly minimised by a
specific aspect of the predictive distribution. The mean squared error,
for example, is only a meaningful scoring rule if the forecaster
actually reported the mean of their predictive distribution as a point
forecast. If the forecaster reported the median, then the mean absolute
error would be the appropriate scoring rule. If the scoring rule and the
predictive task do not align, misleading results ensue. Consider the
following example:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">observed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">4</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span></span>
<span><span class="va">predicted_mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">observed</span><span class="op">)</span></span>
<span><span class="va">predicted_not_mu</span> <span class="op">&lt;-</span> <span class="va">predicted_mu</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">10</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/ae.html" class="external-link">ae</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 34.45981</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/ae.html" class="external-link">ae</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_not_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 32.54821</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/se.html" class="external-link">se</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2171.089</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/se.html" class="external-link">se</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_not_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2290.155</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="absolute-error">Absolute error<a class="anchor" aria-label="anchor" href="#absolute-error"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math>,
a real number, the median of the forecaster’s predictive
distribution.</p>
<p>The absolute error is the absolute difference between the predicted
and the observed values. See <code><a href="https://rdrr.io/pkg/Metrics/man/ae.html" class="external-link">?Metrics::ae</a></code>.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">ae</mtext><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>y</mi><mo>−</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">\text{ae} = |y - \hat{y}|</annotation></semantics></math></p>
<p>The absolute error is only an appropriate rule if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math>
corresponds to the median of the forecaster’s predictive distribution.
Otherwise, results will be misleading (see <span class="citation">Gneiting (2011)</span>).</p>
</div>
<div class="section level3">
<h3 id="squared-error">Squared error<a class="anchor" aria-label="anchor" href="#squared-error"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math>,
a real number, the mean of the forecaster’s predictive distribution.</p>
<p>The squared error is the squared difference between the predicted and
the observed values. See <code><a href="https://rdrr.io/pkg/Metrics/man/se.html" class="external-link">?Metrics::se</a></code>.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">se</mtext><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\text{se} = (y - \hat{y})^2</annotation></semantics></math>
The squared error is only an appropriate rule if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math>
corresponds to the mean of the forecaster’s predictive distribution.
Otherwise, results will be misleading (see <span class="citation">Gneiting (2011)</span>).</p>
</div>
<div class="section level3">
<h3 id="absolute-percentage-error">Absolute percentage error<a class="anchor" aria-label="anchor" href="#absolute-percentage-error"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math>,
a real number</p>
<p>The absolute percentage error is the absolute percent difference
between the predicted and the observed values. See
<code><a href="https://rdrr.io/pkg/Metrics/man/ape.html" class="external-link">?Metrics::ape</a></code>.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">ape</mtext><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><mi>y</mi><mo>−</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>y</mi><mo stretchy="true" form="postfix">|</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{ape} = \frac{|y - \hat{y}|}{|y|}</annotation></semantics></math></p>
<p>The absolute percentage error is only an appropriate rule if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math>
corresponds to the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>-median
of the forecaster’s predictive distribution with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\beta = -1</annotation></semantics></math>.
The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>-median,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="normal">med</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{med}^{(\beta)}(F)</annotation></semantics></math>,
is the median of a random variable whose density is proportional to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mi>β</mi></msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y^\beta f(y)</annotation></semantics></math>.
The specific
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>-median
that corresponds to the absolute percentage error is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="normal">med</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{med}^{(-1)}(F)</annotation></semantics></math>.
Otherwise, results will be misleading (see <span class="citation">Gneiting (2011)</span>).</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="binary-forecasts">Binary forecasts<a class="anchor" aria-label="anchor" href="#binary-forecasts"></a>
</h2>
<p>See a list of the default metrics for point forecasts by calling
<code>?get_metrics(example_binary)</code>.</p>
<p>This is an overview of the input and output formats for point
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-binary.png" class="r-plt" alt="Input and output formats: metrics for binary forecasts." width="100%"><p class="caption">
Input and output formats: metrics for binary forecasts.
</p>
</div>
<div class="section level3">
<h3 id="brier-score">Brier score<a class="anchor" aria-label="anchor" href="#brier-score"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
either 0 or 1</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>,
a probability that the observed outcome will be 1.</p>
<p>The Brier score is a strictly proper scoring rule. It is computed as
the mean squared error between the probabilistic prediction and the
observed outcome.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">BS</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><msup><mi>p</mi><mn>2</mn></msup><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\begin{equation}
    \text{BS}(p, y) = (p - y)^2 =
    \begin{cases}
        p^2,       &amp; \text{if } y = 1\\
        (1 - p)^2,   &amp; \text{if } y = 0
    \end{cases}
\end{equation}</annotation></semantics></math></p>
<p>The Brier score and the logarithmic score (see below) differ in how
they penalise over- and underconfidence (see <span class="citation">Machete (2012)</span>). The Brier score penalises
overconfidence and underconfidence in probability space the same.
Consider the following example:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1e6</span></span>
<span><span class="va">p_true</span> <span class="op">&lt;-</span> <span class="fl">0.7</span></span>
<span><span class="va">observed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html" class="external-link">rbinom</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, size <span class="op">=</span> <span class="fl">1</span>, prob <span class="op">=</span> <span class="va">p_true</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_over</span> <span class="op">&lt;-</span> <span class="va">p_true</span> <span class="op">+</span> <span class="fl">0.15</span></span>
<span><span class="va">p_under</span> <span class="op">&lt;-</span> <span class="va">p_true</span> <span class="op">-</span> <span class="fl">0.15</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_over</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0223866</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_under</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0226134</span></span></code></pre></div>
<p>See <code>?brier_score()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="logarithmic-score">Logarithmic score<a class="anchor" aria-label="anchor" href="#logarithmic-score"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
either 0 or 1</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>,
a probability that the observed outcome will be 1.</p>
<p>The logarithmic score (or log score) is a strictly proper scoring
rule. It is computed as the negative logarithm of the probability
assigned to the observed outcome.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Log score</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>y</mi><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mo>−</mo><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo>−</mo><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\begin{equation}
    \text{Log score}(p, y) = - \log(1 - |y - p|) =
    \begin{cases}
        -\log (p),       &amp; \text{if } y = 1\\
        -\log (1 - p),   &amp; \text{if } y = 0
    \end{cases}
\end{equation}</annotation></semantics></math></p>
<p>The log score penalises overconfidence more strongly than
underconfidence (in probability space). Consider the following
example:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_over</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.07169954</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_under</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.04741833</span></span></code></pre></div>
<p>See <code>?logs_binary()</code> for more information.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="sample-based-forecasts">Sample-based forecasts<a class="anchor" aria-label="anchor" href="#sample-based-forecasts"></a>
</h2>
<p>See a list of the default metrics for sample-based forecasts by
calling <code>get_metrics(example_sample_continuous)</code>.</p>
<p>This is an overview of the input and output formats for quantile
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-sample.png" class="r-plt" alt="Input and output formats: metrics for sample-based forecasts." width="100%"><p class="caption">
Input and output formats: metrics for sample-based forecasts.
</p>
</div>
<div class="section level3">
<h3 id="crps">CRPS<a class="anchor" aria-label="anchor" href="#crps"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A continuous
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>)
or discrete
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>)
forecast.</p>
<p>The continuous ranked probability score (CRPS) is popular in fields
such as meteorology and epidemiology. The CRPS is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">CRPS</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mrow><mo>−</mo><mi>∞</mi></mrow><mi>∞</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>≥</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>d</mi><mi>x</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\text{CRPS}(F, y) = \int_{-\infty}^\infty \left( F(x) - 1(x \geq y) \right)^2 dx,</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is the observed value and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>
the CDF of predictive distribution.</p>
<p>For discrete forecasts, for example count data, the ranked
probability score (RPS) can be used instead and is commonly defined as:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">RPS</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">∞</mo></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>≥</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex"> \text{RPS}(P, y) = \sum_{x = 0}^\infty (P(x) - 1(x \geq y))^2, </annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
is the cumulative probability mass function (PMF) of the predictive
distribution.</p>
<p>The CRPS can be understood as a generalisation of the absolute error
to predictive distributions <span class="citation">(Gneiting and Raftery
2007)</span>. It can also be understood as the integral over the Brier
score for the binary probability forecasts implied by the CDF for all
possible observed values. The CRPS is also related to the
Cramér-distance between two distributions and equals the special case
where one of the distributions is concentrated in a single point (see
e.g. <span class="citation">Ziel (2021)</span>). The CRPS is a global
scoring rule, meaning that the entire predictive distribution is taken
into account when determining the quality of the forecast.</p>
<p><code>scoringutils</code> re-exports the <code><a href="../reference/crps_sample.html">crps_sample()</a></code>
function from the <code>scoringRules</code> package, which assumes that
the forecast is represented by a set of samples from the predictive
distribution. See <code>?crps_sample()</code> for more information.</p>
<div class="section level4">
<h4 id="overprediction-underprediction-and-dispersion">Overprediction, underprediction and dispersion<a class="anchor" aria-label="anchor" href="#overprediction-underprediction-and-dispersion"></a>
</h4>
<p>The CRPS can be interpreted as a sum of a dispersion, an
overprediction and an underprediction component. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
is the median forecast then the dispersion component is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">CRPS</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\text{CRPS}(F, m),</annotation></semantics></math>
the overprediction component is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mi>m</mi><mo>&gt;</mo><mi>y</mi></mtd><mtd columnalign="left" style="text-align: left"><mi>C</mi><mi>R</mi><mi>P</mi><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>C</mi><mi>R</mi><mi>P</mi><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mi>m</mi><mo>≤</mo><mi>y</mi></mtd><mtd columnalign="left" style="text-align: left"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">
\begin{cases}
m &gt; y &amp; CRPS(F, y) - CRPS(F, m)\\
m \leq y &amp; 0\\
\end{cases}
</annotation></semantics></math> and the underprediction component is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mi>m</mi><mo>&lt;</mo><mi>y</mi></mtd><mtd columnalign="left" style="text-align: left"><mi>C</mi><mi>R</mi><mi>P</mi><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>C</mi><mi>R</mi><mi>P</mi><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mi>m</mi><mo>≥</mo><mi>y</mi></mtd><mtd columnalign="left" style="text-align: left"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">
\begin{cases}
m &lt; y &amp; CRPS(F, y) - CRPS(F, m)\\
m \geq y &amp; 0\\
\end{cases}
</annotation></semantics></math></p>
<p>These can be accessed via the <code><a href="../reference/crps_sample.html">dispersion_sample()</a></code>,
<code><a href="../reference/crps_sample.html">overprediction_sample()</a></code> and
<code><a href="../reference/crps_sample.html">underprediction_sample()</a></code> functions, respectively.</p>
</div>
</div>
<div class="section level3">
<h3 id="log-score">Log score<a class="anchor" aria-label="anchor" href="#log-score"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A continuous
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>)
or discrete
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>)
forecast.</p>
<p>The logarithmic scoring rule is simply the negative logarithm of the
density of the the predictive distribution evaluated at the observed
value:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">log score</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>log</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex"> \text{log score}(F, y) = -\log f(y), </annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
is the predictive probability density function (PDF) corresponding to
the Forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is the observed value.</p>
<p>For discrete forecasts, the log score can be computed as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">log score</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>log</mo><msub><mi>p</mi><mi>y</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex"> \text{log score}(F, y) = -\log p_y, </annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>y</mi></msub><annotation encoding="application/x-tex">p_y</annotation></semantics></math>
is the probability assigned to the observed outcome
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
by the forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.</p>
<p>The logarithmic scoring rule can produce large penalties when the
observed value takes on values for which
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(y)</annotation></semantics></math>
(or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>y</mi></msub><annotation encoding="application/x-tex">p_y</annotation></semantics></math>)
is close to zero. It is therefore considered to be sensitive to outlier
forecasts. This may be desirable in some applications, but it also means
that scores can easily be dominated by a few extreme values. The
logarithmic scoring rule is a local scoring rule, meaning that the score
only depends on the probability that was assigned to the actual outcome.
This is often regarded as a desirable property for example in the
context of Bayesian inference . It implies for example, that the ranking
between forecasters would be invariant under monotone transformations of
the predictive distribution and the target.</p>
<p><code>scoringutils</code> re-exports the <code><a href="../reference/logs_sample.html">logs_sample()</a></code>
function from the <code>scoringRules</code> package, which assumes that
the forecast is represented by a set of samples from the predictive
distribution. One implications of this is that it is currently not
advisable to use the log score for discrete forecasts. The reason for
this is that <code><a href="https://rdrr.io/pkg/scoringRules/man/scores_sample_univ.html" class="external-link">scoringRules::logs_sample()</a></code> estimates a
predictive density from the samples, which can be problematic for
discrete forecasts.</p>
<p>See <code>?logs_sample()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="dawid-sebastiani-score">Dawid-Sebastiani score<a class="anchor" aria-label="anchor" href="#dawid-sebastiani-score"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
The predictive distribution with mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
and standard deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>.</p>
<p>The Dawid-Sebastiani score is a proper scoring rule that only relies
on the first moments of the predictive distribution and is therefore
easy to compute. It is given as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">dss</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>y</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><mn>2</mn><mo>⋅</mo><mo>log</mo><mi>σ</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{dss}(F, y) = \left( \frac{y - \mu}{\sigma} \right)^2 + 2 \cdot \log \sigma.</annotation></semantics></math></p>
<p><code>scoringutils</code> re-exports the implementation of the DSS
from the <code>scoringRules</code> package. It assumes that the forecast
is represented by a set of samples drawn from the predictive
distribution. See <code>?dss_sample()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="dispersion---median-absolute-deviation-mad">Dispersion - Median Absolute Deviation (MAD)<a class="anchor" aria-label="anchor" href="#dispersion---median-absolute-deviation-mad"></a>
</h3>
<p><strong>Observation</strong>: Not required.</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>,
the predictive distribution.</p>
<p>Dispersion (also called sharpness) is the ability to produce narrow
forecasts. It is a feature of the forecasts only and does not depend on
the observations. Dispersion is therefore only of interest conditional
on calibration: a very precise forecast is not useful if it is clearly
wrong.</p>
<p>One way to measure sharpness (as suggested by <span class="citation">Funk et al. (2019)</span>) is the normalised median
absolute deviation about the median (MADN) ). It is computed as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>0.675</mn></mfrac><mo>⋅</mo><mtext mathvariant="normal">median</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>F</mi><mo>−</mo><mtext mathvariant="normal">median(F)</mtext><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex"> S(F) = \frac{1}{0.675} \cdot \text{median}(|F - \text{median(F)}|). </annotation></semantics></math>
If the forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>
follows a normal distribution, then sharpness will equal the standard
deviation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.For
more details, see <code>?mad_sample()</code>.</p>
</div>
<div class="section level3">
<h3 id="bias">Bias<a class="anchor" aria-label="anchor" href="#bias"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A continuous
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>)
or discrete
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>)
forecast.</p>
<p>Bias is a measure of the tendency of a forecaster to over- or
underpredict. For <em>continuous</em> forecasts, the
<code>scoringutils</code> implementation calculates bias as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mn>2</mn><mo>⋅</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">B(F, y) = 1 - 2 \cdot F (y), </annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">F(y)</annotation></semantics></math>
is the empirical cumulative distribution function of the forecast
evaluated at the observed value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>.
To handle ties appropriately (which can occur when predictions equal
observations for example due to rounding), the implementation uses
mid-ranks:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">F(y)</annotation></semantics></math>
is computed as the proportion of predictions strictly less than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
plus half the proportion of predictions equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>.</p>
<p>For <em>discrete</em> forecasts, we calculate bias as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">B(P, y) = 1 - (P(y) + P(y + 1)). </annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(y)</annotation></semantics></math>
is the cumulative probability assigned to all outcomes smaller or equal
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
i.e. the cumulative probability mass function.</p>
<p>Bias is bound between -1 and 1 and represents the tendency of
forecasts to be biased rather than the absolute amount of over- and
underprediction (which is e.g. the case for the weighted interval score
(WIS, see below).</p>
</div>
<div class="section level3">
<h3 id="absolute-error-of-the-median">Absolute error of the median<a class="anchor" aria-label="anchor" href="#absolute-error-of-the-median"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">ae</mtext><mtext mathvariant="normal">median</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mtext mathvariant="normal">median</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{ae}_{\text{median}}(F, y) = |\text{median} (F) - y|.</annotation></semantics></math>
See section <a href="#a-note-of-caution">A note of caution</a> or <span class="citation">Gneiting (2011)</span> for a discussion on the
correspondence between the absolute error and the median.</p>
</div>
<div class="section level3">
<h3 id="squared-error-of-the-mean">Squared error of the mean<a class="anchor" aria-label="anchor" href="#squared-error-of-the-mean"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A forecast
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">se</mtext><mtext mathvariant="normal">medn</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">mean</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{se}_{\text{medn}}(F, y) = (\text{mean} (F) - y)^2.</annotation></semantics></math>
See section <a href="#a-note-of-caution">A note of caution</a> or <span class="citation">Gneiting (2011)</span> for a discussion on the
correspondence between the squared error and the mean.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="quantile-based-forecasts">Quantile-based forecasts<a class="anchor" aria-label="anchor" href="#quantile-based-forecasts"></a>
</h2>
<p>See a list of the default metrics for quantile-based forecasts by
calling <code>get_metrics(example_quantile)</code>.</p>
<p>This is an overview of the input and output formats for quantile
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-quantile.png" class="r-plt" alt="Input and output formats: metrics for quantile-based forecasts." width="100%"><p class="caption">
Input and output formats: metrics for quantile-based forecasts.
</p>
</div>
<div class="section level3">
<h3 id="weighted-interval-score-wis">Weighted interval score (WIS)<a class="anchor" aria-label="anchor" href="#weighted-interval-score-wis"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
The CDF of the predictive distribution is represented by a set of
quantiles. These quantiles form the lower
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>)
and upper
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>)
bounds of central prediction intervals.</p>
<p>The weighted interval score (WIS) is a strictly proper scoring rule
and can be understood as an approximation of the CRPS for forecasts in a
quantile format (which in turn represents a generalisation of the
absolute error). Quantiles are assumed to be the lower and upper bounds
of prediction intervals symmetric around the median. For a single
interval, the interval score is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><msub><mi>S</mi><mi>α</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>−</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">dispersion</mtext></munder><mo>+</mo><munder><munder><mrow><mfrac><mn>2</mn><mi>α</mi></mfrac><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mn>𝟏</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>≤</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">overprediction</mtext></munder><mo>+</mo><munder><munder><mrow><mfrac><mn>2</mn><mi>α</mi></mfrac><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mn>𝟏</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>≥</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">underprediction</mtext></munder><mo>,</mo></mrow><annotation encoding="application/x-tex">IS_\alpha(F,y) = \underbrace{(u-l)}_\text{dispersion} + \underbrace{\frac{2}{\alpha} \cdot (l-y) \cdot \mathbf{1}(y \leq l)}_{\text{overprediction}} + \underbrace{\frac{2}{\alpha} \cdot (y-u) \cdot \mathbf{1}(y \geq u)}_{\text{underprediction}}, </annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>𝟏</mn><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{1}()</annotation></semantics></math>
is the indicator function, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>
are the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mi>α</mi><mn>2</mn></mfrac><annotation encoding="application/x-tex">\frac{\alpha}{2}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mfrac><mi>α</mi><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">1 - \frac{\alpha}{2}</annotation></semantics></math>
quantiles of the predictive distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>
together form the prediction interval. The interval score can be
understood as the sum of three components: dispersion, overprediction
and underprediction.</p>
<p>For a set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
prediction intervals and the median
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>,
the score is given as a weighted sum of individual interval scores,
i.e.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>I</mi><mi>S</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>K</mi><mo>+</mo><mn>0.5</mn></mrow></mfrac><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>w</mi><mn>0</mn></msub><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>y</mi><mo>−</mo><mi>m</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>w</mi><mi>k</mi></msub><mo>⋅</mo><mi>I</mi><msub><mi>S</mi><msub><mi>α</mi><mi>k</mi></msub></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">WIS = \frac{1}{K + 0.5} \cdot \left(w_0 \cdot |y - m| + \sum_{k = 1}^{K} w_k \cdot IS_{\alpha_{k}}(F, y)\right),</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
is the median forecast and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>k</mi></msub><annotation encoding="application/x-tex">w_k</annotation></semantics></math>
is a weight assigned to every interval. When the weights are set to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><mfrac><msub><mi>α</mi><mi>k</mi></msub><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">w_k = \frac{\alpha_k}{2}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">w_0 = 0.5</annotation></semantics></math>,
then the WIS converges to the CRPS for an increasing number of equally
spaced quantiles.</p>
<p>See <code>?wis()</code> for more information.</p>
<div class="section level4">
<h4 id="overprediction-underprediction-and-dispersion-1">Overprediction, underprediction and dispersion<a class="anchor" aria-label="anchor" href="#overprediction-underprediction-and-dispersion-1"></a>
</h4>
<p>These are the individual components of the WIS. See
<code>?overprediction_quantile()</code>,
<code>?underprediction_quantile()</code> and
<code>?dispersion_quantile()</code> for more information.</p>
</div>
</div>
<div class="section level3">
<h3 id="bias-1">Bias<a class="anchor" aria-label="anchor" href="#bias-1"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
The CDF of the predictive distribution is represented by a set of
quantiles,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>.</p>
<p>Bias can be measured as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">B</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>2</mn><mo>⋅</mo><mo>max</mo><mo stretchy="false" form="prefix">{</mo><mi>α</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>q</mi><mi>α</mi></msub><mo>∈</mo><mi>Q</mi><mo>∧</mo><msub><mi>q</mi><mi>α</mi></msub><mo>≤</mo><mi>y</mi><mo stretchy="false" form="postfix">}</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>&lt;</mo><msub><mi>q</mi><mn>0.5</mn></msub><mspace width="1.0em"></mspace><mtext mathvariant="normal">(overprediction)</mtext></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>2</mn><mo>⋅</mo><mo>min</mo><mo stretchy="false" form="prefix">{</mo><mi>α</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>q</mi><mi>α</mi></msub><mo>∈</mo><msub><mi>Q</mi><mi>t</mi></msub><mo>∧</mo><msub><mi>q</mi><mi>α</mi></msub><mo>≥</mo><mi>y</mi><mo stretchy="false" form="postfix">}</mo><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>&gt;</mo><msub><mi>q</mi><mn>0.5</mn></msub><mspace width="1.0em"></mspace><mtext mathvariant="normal">(underprediction)</mtext></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>=</mo><msub><mi>q</mi><mn>0.5</mn></msub><mo>,</mo></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\begin{equation}
    \text{B}(F, y) =
    \begin{cases}
        (1 - 2 \cdot \max \{\alpha | q_\alpha \in Q \land q_\alpha \leq y\}),       &amp; \text{if } y &lt; q_{0.5} \quad \text{(overprediction)}\\
        (1 - 2 \cdot \min \{\alpha | q_\alpha \in Q_t \land q_\alpha \geq y\},   &amp; \text{if } y &gt; q_{0.5} \quad \text{(underprediction)}\\
        0,   &amp; \text{if } y = q_{0.5}, \\
    \end{cases}
\end{equation}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>α</mi></msub><annotation encoding="application/x-tex">q_\alpha</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>-quantile
of the predictive distribution. For consistency, we define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
(the set of quantiles that form the predictive distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>)
such that it always includes the element
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>0</mn></msub><mo>=</mo><mo>−</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">q_0 = -\infty</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub><mo>=</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">q_1 = \infty</annotation></semantics></math>.
In clearer terms, bias is:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1 - (2 \times</annotation></semantics></math>
the maximum percentile rank for which the corresponding quantile is
still below the observed value), <em>if the observed value is smaller
than the median of the predictive distribution.</em>
</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1 - (2 \times</annotation></semantics></math>
the minimum percentile rank for which the corresponding quantile is
still larger than the observed value) <em>if the observed value is
larger than the median of the predictive distribution.</em>.</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math><em>if the observed value is exactly the median</em>.</li>
</ul>
<p>Bias can assume values between -1 (underprediction) and 1
(overprediction) and is 0 ideally (i.e. unbiased).</p>
<p>For an increasing number of quantiles, the percentile rank will equal
the proportion of predictive samples below the observed value, and the
bias metric coincides with the one for continuous forecasts (see
above).</p>
<p>See <code>?bias_quantile()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="interval-coverage">Interval coverage<a class="anchor" aria-label="anchor" href="#interval-coverage"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
The CDF of the predictive distribution is represented by a set of
quantiles. These quantiles form central prediction intervals.</p>
<p>Interval coverage for a given interval range is defined as the
proportion of observations that fall within the corresponding central
prediction intervals. Central prediction intervals are symmetric around
the median and formed by two quantiles that denote the lower and upper
bound. For example, the 50% central prediction interval is the interval
between the 0.25 and 0.75 quantiles of the predictive distribution.</p>
<div class="section level4">
<h4 id="interval-coverage-deviation">Interval coverage deviation<a class="anchor" aria-label="anchor" href="#interval-coverage-deviation"></a>
</h4>
<p>The interval coverage deviation is the difference between the
observed interval coverage and the nominal interval coverage. For
example, if the observed interval coverage for the 50% central
prediction interval is 0.6, then the interval coverage deviation is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.6</mn><mo>−</mo><mn>0.5</mn><mo>=</mo><mn>0.1</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">0.6 - 0.5 = 0.1.</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">interval coverage deviation</mtext><mo>=</mo><mtext mathvariant="normal">observed interval coverage</mtext><mo>−</mo><mtext mathvariant="normal">nominal interval coverage</mtext></mrow><annotation encoding="application/x-tex">\text{interval coverage deviation} = \text{observed interval coverage} - \text{nominal interval coverage}</annotation></semantics></math></p>
</div>
</div>
<div class="section level3">
<h3 id="absolute-error-of-the-median-1">Absolute error of the median<a class="anchor" aria-label="anchor" href="#absolute-error-of-the-median-1"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
The CDF of the predictive distribution is represented by a set of
quantiles.</p>
<p>The absolute error of the median is the absolute difference between
the median of the predictive distribution and the observed value.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">ae</mtext><mtext mathvariant="normal">median</mtext></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mtext mathvariant="normal">median</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">\text{ae}_\text{median} = |\text{median}(F) - y|</annotation></semantics></math>
See section <a href="#a-note-of-caution">A note of caution</a> or <span class="citation">Gneiting (2011)</span> for a discussion on the
correspondence between the absolute error and the median.</p>
</div>
<div class="section level3">
<h3 id="quantile-score">Quantile score<a class="anchor" aria-label="anchor" href="#quantile-score"></a>
</h3>
<p><strong>Observation</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
a real number</p>
<p><strong>Forecast</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>.
The CDF of the predictive distribution is represented by a set of
quantiles.</p>
<p>The quantile score, also called pinball loss, for a single quantile
level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
is defined as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">QS</mtext><mi>τ</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mo>⋅</mo><mo stretchy="false" form="prefix">{</mo><mn>𝟏</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>≤</mo><msub><mi>q</mi><mi>τ</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>τ</mi><mo stretchy="false" form="postfix">}</mo><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>q</mi><mi>τ</mi></msub><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mn>2</mn><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>τ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>*</mo><msub><mi>q</mi><mi>τ</mi></msub><mo>−</mo><mi>y</mi><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>≤</mo><msub><mi>q</mi><mi>τ</mi></msub></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mn>2</mn><mo>⋅</mo><mi>τ</mi><mo>*</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>q</mi><mi>τ</mi></msub><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>,</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>y</mi><mo>&gt;</mo><msub><mi>q</mi><mi>τ</mi></msub><mo>,</mo></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\begin{equation}
    \text{QS}_\tau(F, y) = 2 \cdot \{ \mathbf{1}(y \leq q_\tau) - \tau\} \cdot (q_\tau - y) =
    \begin{cases}
        2 \cdot (1 - \tau) * q_\tau - y,       &amp; \text{if } y \leq q_\tau\\
        2 \cdot \tau * |q_\tau - y|,           &amp; \text{if } y &gt; q_\tau,
    \end{cases}
\end{equation}</annotation></semantics></math></p>
<p>with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>τ</mi></msub><annotation encoding="application/x-tex">q_\tau</annotation></semantics></math>
being the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>-quantile
of the predictive distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>𝟏</mn><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{1}(\cdot)</annotation></semantics></math>
the indicator function.</p>
<p>The (unweighted) interval score (see above) for a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">1 - \alpha</annotation></semantics></math>
prediction interval can be computed from the quantile scores at levels
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\alpha/2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">1 - \alpha/2</annotation></semantics></math>
as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">IS</mtext><mi>α</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msub><mtext mathvariant="normal">QS</mtext><mrow><mi>α</mi><mi>/</mi><mn>2</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mtext mathvariant="normal">QS</mtext><mrow><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mi>α</mi></mfrac></mrow><annotation encoding="application/x-tex">\text{IS}_\alpha(F, y) = \frac{\text{QS}_{\alpha/2}(F, y) + \text{QS}_{1 - \alpha/2}(F, y)}{\alpha}</annotation></semantics></math>.</p>
<p>The weighted interval score can be obtained as a simple average of
the quantile scores:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">WIS</mtext><mi>α</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msub><mtext mathvariant="normal">QS</mtext><mrow><mi>α</mi><mi>/</mi><mn>2</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mtext mathvariant="normal">QS</mtext><mrow><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>F</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\text{WIS}_\alpha(F, y) = \frac{\text{QS}_{\alpha/2}(F, y) + \text{QS}_{1 - \alpha/2}(F, y)}{2}</annotation></semantics></math>.</p>
<p>See <code><a href="../reference/quantile_score.html">?quantile_score</a></code> and <span class="citation">Bracher
et al. (2021)</span> for more details.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="additional-metrics">Additional metrics<a class="anchor" aria-label="anchor" href="#additional-metrics"></a>
</h2>
<div class="section level3">
<h3 id="quantile-coverage">Quantile coverage<a class="anchor" aria-label="anchor" href="#quantile-coverage"></a>
</h3>
<p>Quantile coverage for a given quantile level is defined as the
proportion of observed values that are smaller than the corresponding
predictive quantiles. For example, the 0.5 quantile coverage is the
proportion of observed values that are smaller than the 0.5-quantiles of
the predictive distribution.</p>
<!-- ## Probability integral transform (PIT) -->
<!-- # Calibration -->
<!-- Calibration or reliability of forecasts is the ability of a model to -->
<!-- correctly identify its own uncertainty in making predictions. In a model -->
<!-- with perfect calibration, the observed data at each time point look as if -->
<!-- they came from the predictive probability distribution at that time. -->
<!-- Equivalently, one can inspect the probability integral transform of the -->
<!-- predictive distribution at time t, -->
<!-- $$u_t = F_t (x_t)$$ -->
<!-- where $x_t$ is the observed data point at time $t \text{ in } t_1, …, t_n$, -->
<!-- n being the number of forecasts, and $F_t$ is the (continuous) predictive -->
<!-- cumulative probability distribution at time t. If the true probability -->
<!-- distribution of outcomes at time t is $G_t$ then the forecasts $F_t$ are -->
<!-- said to be ideal if $F_t = G_t$ at all times $t$. In that case, the -->
<!-- probabilities ut are distributed uniformly. -->
<!-- In the case of discrete outcomes such as incidence counts, -->
<!-- the PIT is no longer uniform even when forecasts are ideal. -->
<!-- In that case a randomised PIT can be used instead: -->
<!-- $$u_t = P_t(k_t) + v \cdot (P_t(k_t) - P_t(k_t - 1) )$$ -->
<!-- where $k_t$ is the observed count, $P_t(x)$ is the predictive -->
<!-- cumulative probability of observing incidence $k$ at time $t$, -->
<!-- $P_t (-1) = 0$ by definition and $v$ is standard uniform and independent -->
<!-- of $k$. If $P_t$ is the true cumulative -->
<!-- probability distribution, then $u_t$ is standard uniform. -->
<!-- The function checks whether integer or continuous forecasts were provided. -->
<!-- It then applies the (randomised) probability integral and tests -->
<!-- the values $u_t$ for uniformity using the -->
<!-- Anderson-Darling test. -->
<!-- As a rule of thumb, there is no evidence to suggest a forecasting model is -->
<!-- miscalibrated if the p-value found was greater than a threshold of $p >= 0.1$, -->
<!-- some evidence that it was miscalibrated if $0.01 < p < 0.1$, and good -->
<!-- evidence that it was miscalibrated if $p <= 0.01$. -->
<!-- In this context it should be noted, though, that uniformity of the -->
<!-- PIT is a necessary but not sufficient condition of calibration. It should -->
<!-- also be noted that the test only works given sufficient samples, otherwise the  -->
<!-- Null hypothesis will often be rejected outright.  -->
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bracherEvaluatingEpidemicForecasts2021" class="csl-entry">
Bracher, Johannes, Evan L. Ray, Tilmann Gneiting, and Nicholas G. Reich.
2021. <span>“Evaluating Epidemic Forecasts in an Interval
Format.”</span> <em>PLoS Computational Biology</em> 17 (2): e1008618. <a href="https://doi.org/10.1371/journal.pcbi.1008618" class="external-link">https://doi.org/10.1371/journal.pcbi.1008618</a>.
</div>
<div id="ref-funkAssessingPerformanceRealtime2019" class="csl-entry">
Funk, Sebastian, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind
M. Eggo, and W. John Edmunds. 2019. <span>“Assessing the Performance of
Real-Time Epidemic Forecasts: <span>A</span> Case Study of
<span>Ebola</span> in the <span>Western Area</span> Region of
<span>Sierra Leone</span>, 2014-15.”</span> <em>PLOS Computational
Biology</em> 15 (2): e1006785. <a href="https://doi.org/10.1371/journal.pcbi.1006785" class="external-link">https://doi.org/10.1371/journal.pcbi.1006785</a>.
</div>
<div id="ref-gneitingMakingEvaluatingPoint2011" class="csl-entry">
Gneiting, Tilmann. 2011. <span>“Making and <span>Evaluating Point
Forecasts</span>.”</span> <em>Journal of the American Statistical
Association</em> 106 (494): 746–62. <a href="https://doi.org/10.1198/jasa.2011.r10138" class="external-link">https://doi.org/10.1198/jasa.2011.r10138</a>.
</div>
<div id="ref-gneitingStrictlyProperScoring2007" class="csl-entry">
Gneiting, Tilmann, and Adrian E Raftery. 2007. <span>“Strictly
<span>Proper Scoring Rules</span>, <span>Prediction</span>, and
<span>Estimation</span>.”</span> <em>Journal of the American Statistical
Association</em> 102 (477): 359–78. <a href="https://doi.org/10.1198/016214506000001437" class="external-link">https://doi.org/10.1198/016214506000001437</a>.
</div>
<div id="ref-macheteContrastingProbabilisticScoring2012" class="csl-entry">
Machete, Reason Lesego. 2012. <span>“Contrasting <span>Probabilistic
Scoring Rules</span>.”</span> <em>arXiv:1112.4530 [Math, Stat]</em>,
July. <a href="https://arxiv.org/abs/1112.4530" class="external-link">https://arxiv.org/abs/1112.4530</a>.
</div>
<div id="ref-zielEnergyDistanceEnsemble2021" class="csl-entry">
Ziel, Florian. 2021. <span>“The Energy Distance for Ensemble and
Scenario Reduction.”</span> <em>Philosophical Transactions of the Royal
Society A: Mathematical, Physical and Engineering Sciences</em> 379
(2202): 20190431. <a href="https://doi.org/10.1098/rsta.2019.0431" class="external-link">https://doi.org/10.1098/rsta.2019.0431</a>.
</div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://followtheargument.org/" class="external-link">Nikos Bosse</a>, <a href="https://www.samabbott.co.uk/" class="external-link">Sam Abbott</a>, Hugo Gruson, Sebastian Funk.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.9000.</p>
</div>

    </footer>
</div>





  </body>
</html>
