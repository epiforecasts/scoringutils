<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Do pairwise comparison for one set of forecasts — pairwise_comparison_one_group • scoringutils</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Do pairwise comparison for one set of forecasts — pairwise_comparison_one_group"><meta name="description" content="This function does the pairwise comparison for one set of forecasts, but
multiple models involved. It gets called from get_pairwise_comparisons().
get_pairwise_comparisons() splits the data into arbitrary subgroups
specified by the user (e.g. if pairwise comparison should be done separately
for different forecast targets) and then the actual pairwise comparison for
that subgroup is managed from pairwise_comparison_one_group(). In order to
actually do the comparison between two models over a subset of common
forecasts it calls compare_two_models()."><meta property="og:description" content="This function does the pairwise comparison for one set of forecasts, but
multiple models involved. It gets called from get_pairwise_comparisons().
get_pairwise_comparisons() splits the data into arbitrary subgroups
specified by the user (e.g. if pairwise comparison should be done separately
for different forecast targets) and then the actual pairwise comparison for
that subgroup is managed from pairwise_comparison_one_group(). In order to
actually do the comparison between two models over a subset of common
forecasts it calls compare_two_models()."><meta name="robots" content="noindex"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">
    <a class="navbar-brand me-2" href="../index.html">scoringutils</a>
    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.2.2.9000</small>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://raw.githubusercontent.com/epiforecasts/scoringutils/main/inst/manuscript/manuscript.pdf">scoringutils paper</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/Deprecated-functions.html">Deprecated functions</a></li>
    <li><a class="dropdown-item" href="../articles/Deprecated-visualisations.html">Deprecated Visualisations</a></li>
    <li><a class="dropdown-item" href="../articles/scoring-rules.html">Scoring rules in `scoringutils`</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/epiforecasts/scoringutils/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <h1>Do pairwise comparison for one set of forecasts</h1>
      <small class="dont-index">Source: <a href="https://github.com/epiforecasts/scoringutils/blob/main/R/pairwise-comparisons.R" class="external-link"><code>R/pairwise-comparisons.R</code></a></small>
      <div class="d-none name"><code>pairwise_comparison_one_group.Rd</code></div>
    </div>
    <div class="ref-description section level2">
    <p>This function does the pairwise comparison for one set of forecasts, but
multiple models involved. It gets called from <code><a href="get_pairwise_comparisons.html">get_pairwise_comparisons()</a></code>.
<code><a href="get_pairwise_comparisons.html">get_pairwise_comparisons()</a></code> splits the data into arbitrary subgroups
specified by the user (e.g. if pairwise comparison should be done separately
for different forecast targets) and then the actual pairwise comparison for
that subgroup is managed from <code>pairwise_comparison_one_group()</code>. In order to
actually do the comparison between two models over a subset of common
forecasts it calls <code><a href="compare_two_models.html">compare_two_models()</a></code>.</p>
    </div>
    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">pairwise_comparison_one_group</span><span class="op">(</span><span class="va">scores</span>, <span class="va">metric</span>, <span class="va">baseline</span>, <span class="va">by</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
    </div>
    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
<dl><dt id="arg-scores">scores<a class="anchor" aria-label="anchor" href="#arg-scores"></a></dt>
<dd><p>An object of class <code>scores</code> (a data.table with
scores and an additional attribute <code>metrics</code> as produced by <code><a href="score.html">score()</a></code>).</p></dd>
<dt id="arg-metric">metric<a class="anchor" aria-label="anchor" href="#arg-metric"></a></dt>
<dd><p>A string with the name of the metric for which
a relative skill shall be computed. By default this is either "crps",
"wis" or "brier_score" if any of these are available.</p></dd>
<dt id="arg-baseline">baseline<a class="anchor" aria-label="anchor" href="#arg-baseline"></a></dt>
<dd><p>A string with the name of a model. If a baseline is
given, then a scaled relative skill with respect to the baseline will be
returned. By default (<code>NULL</code>), relative skill will not be scaled with
respect to a baseline model.</p></dd>
<dt id="arg-by">by<a class="anchor" aria-label="anchor" href="#arg-by"></a></dt>
<dd><p>Character vector with column names that define the grouping level
for the pairwise comparisons. By default (<code>model</code>), there will be one
relative skill score per model. If, for example,
<code>by = c("model", "location")</code>. Then you will get a
separate relative skill score for every model in every location. Internally,
the data.table with scores will be split according <code>by</code> (removing "model"
before splitting) and the pairwise comparisons will be computed separately
for the split data.tables.</p></dd>
<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Additional arguments for the comparison between two models. See
<code><a href="compare_two_models.html">compare_two_models()</a></code> for more information.</p></dd>
</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A data.table with the results of pairwise comparisons
containing the mean score ratios (<code>mean_scores_ratio</code>),
unadjusted (<code>pval</code>) and adjusted (<code>adj_pval</code>) p-values, and relative skill
values of each model (<code>..._relative_skill</code>). If a baseline model is given
then the scaled relative skill is reported as well
(<code>..._scaled_relative_skill</code>).</p>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>
    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://followtheargument.org/" class="external-link">Nikos Bosse</a>, <a href="https://www.samabbott.co.uk/" class="external-link">Sam Abbott</a>, Hugo Gruson, Sebastian Funk.</p>
</div>
<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.9000.</p>
</div>
    </footer></div>
  </body></html>

